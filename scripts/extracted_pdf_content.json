[
  {
    "page": 103,
    "content": "2.4. Exceptions 85\n2.4.2 Throwing Exceptions\nExceptions originate when a piece of Java code ﬁnds some sort of problem during\nexecution and throws an exception object. This is done by using the throw keyword\nfollowed by an instance of the exception type to be thrown. It is often convenient\nto instantiate an exception object at the time the exception has to be thrown. Thus,\nathrow statement is typically written as follows:\nthrow new exceptionType (parameters );\nwhere exceptionType is the type of the exception and the parameters are sent to\nthat type’s constructor; most exception types offer a version of a constructor that\naccepts an error message string as a parameter.\nAs an example, the following method takes an integer parameter, which it ex-\npects to be positive. If a negative integer is sent, an IllegalArgumentException is\nthrown.\npublic void ensurePositive( intn){\nif(n<0)\nthrow new IllegalArgumentException( \"That's not positive!\" );\n// ...\n}\nThe execution of a throw statement immediately terminates the body of a method.\nThe Throws Clause\nWhen a method is declared, it is possible to explicitly declare, as part of its sig-\nnature, the possibility that a particular exception type may be thrown during a call\nto that method. It does not matter whether the exception is directly from a throw\nstatement in that method body, or propagated upward from a secondary method call\nmade from within the body.\nThe syntax for declaring possible exceptions in a method signature relies on the\nkeywordthrows (not to be confused with an actual throw statement). For example,\ntheparseInt method of the Integer class has the following formal signature:\npublic static int parseInt(String s) throwsNumberFormatException;\nThe designation “ throwsNumberFormatException ” warns users about the possi-\nbility of an exceptional case, so that they might be better prepared to handle an\nexception that may arise. If one of many exception types may possibly be thrown,\nall such types can be listed, separated with commas. Alternatively, it may be pos-\nsible to list an appropriate superclass that encompasses all speciﬁc exceptions that\nmay be thrown.\nwww.it-ebooks.info"
  },
  {
    "page": 104,
    "content": "86 Chapter 2. Object-Oriented Design\nThe use of a throws clause in a method signature does not take away the re-\nsponsibility of properly documenting all possible exceptions through the use of the\n@throws tag within a javadoc comment (see Section 1.9.4). The type and reasons\nfor any potential exceptions should always be properly declared in the documenta-\ntion for a method.\nIn contrast, the use of the throws clause in a method signature is optional\nfor many types of exceptions. For example, the documentation for the nextInt()\nmethod of the Scanner class makes clear that three different exception types may\narise:\n•AnIllegalStateException , if the scanner has been closed\n•ANoSuchElementException , if the scanner is active, but there is currently\nno token available for input\n•AnInputMismatchException , if the next available token does not represent\nan integer\nHowever, no potential exceptions are formally declared within the method signa-\nture; they are only noted in the documentation.\nTo better understand the functional purpose of the throws declaration in a\nmethod signature, it is helpful to know more about the way Java organizes its hier-\narchy of exception types.\n2.4.3 Java’s Exception Hierarchy\nJava deﬁnes a rich inheritance hierarchy of all objects that are deemed Throwable .\nWe show a small portion of this hierarchy in Figure 2.7. The hierarchy is intention-\nally divided into two subclasses: Error andException .Errors are typically thrown\nonly by the Java Virtual Machine and designate the most serious situations that are\nunlikely to be recoverable, such as when the virtual machine is asked to execute\na corrupt class ﬁle, or when the system runs out of memory. In contrast, excep-\ntions designate situations in which a running program might reasonably be able to\nrecover, for example, when unable to open a data ﬁle.\nChecked and Unchecked Exceptions\nJava provides further reﬁnement by declaring the RuntimeException class as an\nimportant subclass of Exception . All subtypes of RuntimeException in Java are\nofﬁcially treated as unchecked exceptions , and any exception type that is not part\nof theRuntimeException is achecked exception .\nThe intent of the design is that runtime exceptions occur entirely due to mis-\ntakes in programming logic, such as using a bad index with an array, or sending an\ninappropriate value as a parameter to a method. While such programming errors\nwww.it-ebooks.info"
  },
  {
    "page": 105,
    "content": "2.4. Exceptions 87\nOutofMemoryErrorRuntimeException IOError VirtualMachineError\nNullPointerExceptionThrowable\nIOException\nIllegalArgumentException\nNoSuchElementException. . .\nEOFException\nIndexOutOfBoundsExceptionException\nArrayIndexOutOfBoundsExceptionFileNotFoundException. . .\nNumberFormatExceptionError\nClassCastException\nFigure 2.7: A small portion of Java’s hierarchy of Throwable types.\nwill certainly occur as part of the software development process, they should pre-\nsumably be resolved before software reaches production quality. Therefore, it is\nnot in the interest of efﬁciency to explicitly check for each such mistake at runtime,and thus these are designated as “unchecked” exceptions.\nIn contrast, other exceptions occur because of conditions that cannot easily be\ndetected until a program is executing, such as an unavailable ﬁle or a failed networkconnection. Those are typically designated as “checked” exceptions in Java (andthus, not a subtype of RuntimeException ).\nThe designation between checked and unchecked exceptions plays a signiﬁcant\nrole in the syntax of the language. In particular, all checked exceptions that might\npropagate upward from a method must be explicitly declared in its signature.\nA consequence is that if one method calls a second method declaring checked\nexceptions, then the call to that second method must either be guarded within atry-catch statement, or else the calling method must itself declare the checked ex-\nceptions in its signature, since there is risk that such an exception might propagate\nupward from the calling method.\nDeﬁning New Exception Types\nIn this book, we will rely entirely on existing RuntimeException types to designate\nvarious requirements on the use of our data structures. However, some librariesdeﬁne new classes of exceptions to describe more speciﬁc conditions. Specializedexceptions should inherit either from the Exception class (if checked), from the\nRuntimeException class (if unchecked), or from an existing Exception subtype\nthat is more relevant.\nwww.it-ebooks.info"
  },
  {
    "page": 106,
    "content": "88 Chapter 2. Object-Oriented Design\n2.5 Casting and Generics\nIn this section, we discuss casting among reference variables, as well as a technique,\ncalled generics, that allows us to deﬁne methods and classes that work with a variety\nof data types without the need for explicit casting.\n2.5.1 Casting\nWe begin our discussion with methods for type conversions for objects.\nWidening Conversions\nAwidening conversion occurs when a type Tis converted into a “wider” type U.\nThe following are common cases of widening conversions:\n•TandUare class types and Uis a superclass of T.\n•TandUare interface types and Uis a superinterface of T.\n•Tis a class that implements interface U.\nWidening conversions are automatically performed to store the result of an ex-\npression into a variable, without the need for an explicit cast. Thus, we can directly\nassign the result of an expression of type Tinto a variable vof type Uwhen the\nconversion from TtoUis a widening conversion. When discussing polymorphism\non page 68, we gave the following example of an implicit widening cast, assigning\nan instance of the narrower PredatoryCreditCard class to a variable of the wider\nCreditCard type:\nCreditCard card = newPredatoryCreditCard(...); // parameters omitted\nThe correctness of a widening conversion can be checked by the compiler and its\nvalidity does not require testing by the Java runtime environment during program\nexecution.\nNarrowing Conversions\nAnarrowing conversion occurs when a type Tis converted into a “narrower”\ntype S. The following are common cases of narrowing conversions:\n•TandSare class types and Sis a subclass of T.\n•TandSare interface types and Sis a subinterface of T.\n•Tis an interface implemented by class S.\nIn general, a narrowing conversion of reference types requires an explicit cast.\nAlso, the correctness of a narrowing conversion may not be veriﬁable by the com-\npiler. Thus, its validity should be tested by the Java runtime environment during\nprogram execution.\nwww.it-ebooks.info"
  },
  {
    "page": 107,
    "content": "2.5. Casting and Generics 89\nThe example code fragment below shows how to use a cast to perform a nar-\nrowing conversion from type PredatoryCreditCard to typeCreditCard .\nCreditCard card = newPredatoryCreditCard(...); // widening\nPredatoryCreditCard pc = (PredatoryCreditCard) card; // narrowing\nAlthough variable card happens to reference an instance of a PredatoryCreditCard ,\nthe variable has declared type, CreditCard . Therefore, the assignment pc = card\nis a narrowing conversion and requires an explicit cast that will be evaluated at\nruntime (as not all cards are predatory).\nCasting Exceptions\nIn Java, we can cast an object reference oof type Tinto a type S, provided the\nobject ois referring to is actually of type S. If, on the other hand, object ois not\nalso of type S, then attempting to cast oto type Swill throw an exception called\nClassCastException . We illustrate this rule in the following code fragment, using\nJava’sNumber abstract class, which is a superclass of both Integer andDouble .\nNumber n;Integer i;n =newInteger(3);\ni = (Integer) n; // This is legal\nn =newDouble(3.1415);\ni = (Integer) n; // This is illegal\nTo avoid problems such as this and to avoid peppering our code with try-catch\nblocks every time we perform a cast, Java provides a way to make sure an objectcast will be correct. Namely, it provides an operator, instanceof , that allows us to\ntest whether an object variable is referring to an object that belongs to a particulartype. The syntax for this operator is objectReference instanceof referenceType ,\nwhere objectReference is an expression that evaluates to an object reference and\nreferenceType is the name of some existing class, interface, or enum (Section 1.3).\nIfobjectReference is indeed an instance satisfying referenceType , then the operator\nreturnstrue; otherwise, it returns false . Thus, we can avoid a ClassCastException\nfrom being thrown in the code fragment above by modifying it as follows:\nNumber n;Integer i;n =newInteger(3);\nif(ninstanceof Integer)\ni = (Integer) n; // This is legal\nn =newDouble(3.1415);\nif(ninstanceof Integer)\ni = (Integer) n; // This will not be attempted\nwww.it-ebooks.info"
  },
  {
    "page": 108,
    "content": "90 Chapter 2. Object-Oriented Design\nCasting with Interfaces\nInterfaces allow us to enforce that objects implement certain methods, but using\ninterface variables with concrete objects sometimes requires casting. Suppose wedeclare a Person interface as shown in Code Fragment 2.14. Note that method\nequals of the Person interface takes one parameter of type Person . Thus, we can\npass an object of any class implementing the Person interface to this method.\n1public interface Person{\n2public boolean equals(Person other); // is this the same person?\n3publicString getName(); // get this person’s name\n4public int getAge(); // get this person’s age\n5}\nCode Fragment 2.14: Interface Person .\nIn Code Fragment 2.15, we show a class, Student , that implements Person .\nBecause the parameter to equals is aPerson , the implementation must not assume\nthat it is necessarily of type Student . Instead, it ﬁrst uses the instanceof operator\nat line 15, returning false if the argument is not a student (since it surely is not\nthe student in question). Only after verifying that the parameter is a student, is itexplicitly cast to a Student , at which point its idﬁeld can be accessed.\n1public class Studentimplements Person{\n2String id;\n3String name;\n4intage;\n5publicStudent(String i, String n, inta){ // simple constructor\n6id = i;\n7name = n;\n8age = a;\n9}\n10protected int studyHours(){returnage/2;}// just a guess\n11publicString getID(){returnid;} // ID of the student\n12publicString getName() {returnname;}// from Person interface\n13public int getAge(){returnage;} // from Person interface\n14public boolean equals(Person other) { // from Person interface\n15if(!(otherinstanceof Student)) return false ;// cannot possibly be equal\n16Student s = (Student) other; // explicit cast now safe\n17returnid.equals(s.id); // compare IDs\n18}\n19publicString toString(){ // for printing\n20return\"Student(ID:\" + id +\", Name:\" + name + \", Age:\" + age + \")\";\n21}\n22}\nCode Fragment 2.15: ClassStudent implementing interface Person .\nwww.it-ebooks.info"
  },
  {
    "page": 109,
    "content": "2.5. Casting and Generics 91\n2.5.2 Generics\nJava includes support for writing generic classes and methods that can operate on a\nvariety of data types while often avoiding the need for explicit casts. The generics\nframework allows us to deﬁne a class in terms of a set of formal type parameters ,\nwhich can then be used as the declared type for variables, parameters, and return\nvalues within the class deﬁnition. Those formal type parameters are later speciﬁed\nwhen using the generic class as a type elsewhere in a program.\nTo better motivate the use of generics, we consider a simple case study. Often,\nwe wish to treat a pair of related values as a single object, for example, so that\nthe pair can be returned from a method. A solution is to deﬁne a new class whose\ninstances store both values. This is our ﬁrst example of an object-oriented design\npattern known as the composition design pattern . If we know, for example, that we\nwant a pair to store a string and a ﬂoating-point number, perhaps to store a stock\nticker label and a price, we could easily design a custom class for that purpose.\nHowever, for another purpose, we might want to store a pair that consists of a Book\nobject and an integer that represents a quantity. The goal of generic programming\nis to be able to write a single class that can represent all such pairs.\nThe generics framework was not a part of the original Java language; it was\nadded as part of Java SE 5. Prior to that, generic programming was implemented\nby relying heavily on Java’s Object class, which is the universal supertype of all\nobjects (including the wrapper types corresponding to primitives). In that “classic”\nstyle, a generic pair might be implemented as shown in Code Fragment 2.16.\n1public class ObjectPair{\n2Object ﬁrst;\n3Object second;\n4publicObjectPair(Object a, Object b) { // constructor\n5ﬁrst = a;\n6second = b;\n7}\n8publicObject getFirst() {returnﬁrst;}\n9publicObject getSecond() {returnsecond;}\n10}\nCode Fragment 2.16: Representing a generic pair of objects using a classic style.\nAnObjectPair instance stores the two objects that are sent to the constructor,\nand provides individual accessors for each component of the pair. With this deﬁni-\ntion, a pair can be declared and instantiated with the following command:\nObjectPair bid = newObjectPair( \"ORCL\", 32.07);\nThis instantiation is legal because the parameters to the constructor undergo widen-\ning conversions. The ﬁrst parameter, \"ORCL\" , is aString , and thus also an Object .\nwww.it-ebooks.info"
  },
  {
    "page": 110,
    "content": "92 Chapter 2. Object-Oriented Design\nThe second parameter is a double , but it is automatically boxed into a Double ,\nwhich then qualiﬁes as an Object . (For the record, this is not quite the “classic”\nstyle, as automatic boxing was not introduced until Java SE 5.)\nThe drawback of the classic approach involves use of the accessors, both of\nwhich formally return an Object reference. Even if we know that the ﬁrst object is\na string in our application, we cannot legally make the following assignment:\nString stock = bid.getFirst(); // illegal; compile error\nThis represents a narrowing conversion from the declared return type of Object to\nthe variable of type String . Instead, an explicit cast is required, as follows:\nString stock = (String) bid.getFirst(); // narrowing cast: Object to String\nWith the classic style for generics, code became rampant with such explicit casts.\nUsing Java’s Generics Framework\nWith Java’s generics framework, we can implement a pair class using formal type\nparameters to represent the two relevant types in our composition. An implemen-tation using this framework is given in Code Fragment 2.17.\n1public class Pair<A,B>{\n2A ﬁrst;\n3B second;\n4publicPair(A a, B b){ // constructor\n5ﬁrst = a;\n6second = b;\n7}\n8publicA getFirst(){returnﬁrst;}\n9publicB getSecond(){returnsecond;}\n10}\nCode Fragment 2.17: Representing a pair of objects with generic type parameters.\nAngle brackets are used at line 1 to enclose the sequence of formal type parameters.Although any valid identiﬁer can be used for a formal type parameter, single-letter\nuppercase names are conventionally used (in this example, AandB). We may then\nuse these type parameters within the body of the class deﬁnition. For example, we\ndeclare instance variable, ﬁrst, to have type A; we similarly use Aas the declared\ntype for the ﬁrst constructor parameter and for the return type of method, getFirst .\nWhen subsequently declaring a variable with such a parameterize type, we must\nexplicitly specify actual type parameters that will take the place of the generic\nformal type parameters. For example, to declare a variable that is a pair holding astock-ticker string and a price, we write the following:\nPair<String,Double >bid;\nwww.it-ebooks.info"
  },
  {
    "page": 111,
    "content": "2.5. Casting and Generics 93\nEffectively, we have stated that we wish to have String serve in place of type A,\nandDouble serve in place of type Bfor the pair known as bid. The actual types for\ngeneric programming must be object types, which is why we use the wrapper class\nDouble instead of the primitive type double . (Fortunately, the automatic boxing\nand unboxing will work in our favor.)\nWe can subsequently instantiate the generic class using the following syntax:\nbid =newPair<>(\"ORCL\", 32.07); // rely on type inference\nAfter the new operator, we provide the name of the generic class, then an empty\nset of angle brackets (known as the “diamond”), and ﬁnally the parameters to theconstructor. An instance of the generic class is created, with the actual types forthe formal type parameters determined based upon the original declaration of thevariable to which it is assigned ( bidin this example). This process is known as type\ninference , and was introduced to the generics framework in Java SE 7.\nIt is also possible to use a style that existed prior to Java SE 7, in which the\ngeneric type parameters are explicitly speciﬁed between angle brackets during in-\nstantiation. Using that style, our previous example would be implemented as:\nbid =newPair<String,Double >(\"ORCL\", 32.07); // give explicit types\nHowever, it is important that one of the two above styles be used. If angle\nbrackets are entirely omitted, as in the following example,\nbid =newPair(\"ORCL\", 32.07); // classic style\nthis reverts to the classic style, with Object automatically used for all generic type\nparameters, and resulting in a compiler warning when assigning to a variable withmore speciﬁc types.\nAlthough the syntax for the declaration and instantiation of objects using the\ngenerics framework is slightly more cluttered than the classic style, the advantageis that there is no longer any need for explicit narrowing casts from Object to a\nmore speciﬁc type. Continuing with our example, since bidwas declared with\nactual type parameters <String,Double >, the return type of the getFirst() method\nisString , and the return type of the getSecond() method is Double . Unlike the\nclassic style, we can make the following assignments without any explicit casting\n(although there is still an automatic unboxing of the Double ):\nString stock = bid.getFirst();doubleprice = bid.getSecond();\nwww.it-ebooks.info"
  },
  {
    "page": 112,
    "content": "94 Chapter 2. Object-Oriented Design\nGenerics and Arrays\nThere is an important caveat related to generic types and the use of arrays. Although\nJava allows the declaration of an array storing a parameterized type, it does nottechnically allow the instantiation of new arrays involving those types. Fortunately,\nit allows an array deﬁned with a parameterized type to be initialized with a newly\ncreated, nonparametric array, which can then be cast to the parameterized type.Even so, this latter mechanism causes the Java compiler to issue a warning, becauseit is not 100% type-safe.\nWe will see this issue arise in two ways:\n•Code outside a generic class may wish to declare an array storing instances\nof the generic class with actual type parameters.\n•A generic class may wish to declare an array storing objects that belong to\none of the formal parameter types.\nAs an example of the ﬁrst use case, we continue with our stock market example\nand presume that we would like to keep an array of Pair<String,Double >objects.\nSuch an array can be declared with a parameterized type, but it must be instantiated\nwith an unparameterized type and then cast back to the parameterized type. We\ndemonstrate this usage in the following:\nPair<String,Double >[ ] holdings;\nholdings = newPair<String,Double >[25];// illegal; compile error\nholdings = newPair[25]; // correct, but warning about unchecked cast\nholdings[0] = newPair<>(\"ORCL\", 32.07); // valid element assignment\nAs an example of the second use case, assume that we want to create a generic\nPortfolio class that can store a ﬁxed number of generic entries in an array. If the\nclass uses <T>as a parameterized type, it can declare an array of type T[ ], but\nit cannot directly instantiate such an array. Instead, a common approach is to in-\nstantiate an array of type Object[ ] , and then make a narrowing cast to type T[ ], as\nshown in the following:\npublic class Portfolio<T>{\nT[ ] data;publicPortfolio( intcapacity){\ndata =newT[capacity]; // illegal; compiler error\ndata = (T[ ]) newObject[capacity]; // legal, but compiler warning\n}publicT get(intindex){returndata[index];}\npublic void set(intindex, T element) {data[index] = element; }\n}\nwww.it-ebooks.info"
  },
  {
    "page": 113,
    "content": "2.5. Casting and Generics 95\nGeneric Methods\nThe generics framework allows us to deﬁne generic versions of individual methods\n(as opposed to generic versions of entire classes). To do so, we include a generic\nformal type declaration among the method modiﬁers.\nFor example, we show below a nonparametric GenericDemo class with a pa-\nrameterized static method that can reverse an array containing elements of any\nobject type.\npublic class GenericDemo{\npublic static <T>voidreverse(T[ ] data) {\nintlow = 0, high = data.length −1;\nwhile(low<high){ // swap data[low] and data[high]\nT temp = data[low];data[low++] = data[high]; // post-increment of low\ndata[high−−] = temp; // post-decrement of high\n}\n}\n}\nNote the use of the <T>modiﬁer to declare the method to be generic, and the use\nof the type Twithin the method body, when declaring the local variable, temp .\nThe method can be called using the syntax, GenericDemo.reverse(books) , with\ntype inference determining the generic type, assuming books is an array of some\nobject type. (This generic method cannot be applied to primitive arrays, becauseautoboxing does not apply to entire arrays.)\nAs an aside, we note that we could have implemented a reverse method equally\nwell using a classic style, acting upon an Object[ ] array.\nBounded Generic Types\nBy default, when using a type name such as Tin a generic class or method, a\nuser can specify any object type as the actual type of the generic. A formal pa-\nrameter type can be restricted by using the extends keyword followed by a class\nor interface. In that case, only a type that satisﬁes the stated condition is allowed\nto substitute for the parameter. The advantage of such a bounded type is that itbecomes possible to call any methods that are guaranteed by the stated bound.\nAs an example, we might declare a generic ShoppingCart that could only be\ninstantiated with a type that satisﬁed the Sellable interface (from Code Fragment 2.8\non page 77). Such a class would be declared beginning with the line:\npublic class ShoppingCart <Textends Sellable>{\nWithin that class deﬁnition, we would then be allowed to call methods such as\ndescription() andlowestPrice() on any instances of type T.\nwww.it-ebooks.info"
  },
  {
    "page": 114,
    "content": "96 Chapter 2. Object-Oriented Design\n2.6 Nested Classes\nJava allows a class deﬁnition to be nested inside the deﬁnition of another class.\nThe main use for nesting classes is when deﬁning a class that is strongly afﬁli-\nated with another class. This can help increase encapsulation and reduce undesired\nname conﬂicts. Nested classes are a valuable technique when implementing data\nstructures, as an instance of a nested use can be used to represent a small portion\nof a larger data structure, or an auxiliary class that helps navigate a primary data\nstructure. We will use nested classes in many implementations within this book.\nTo demonstrate the mechanics of a nested class, we consider a new Transaction\nclass to support logging of transactions associated with a credit card. That new class\ndeﬁnition can be nested within the CreditCard class using a style as follows:\npublic class CreditCard{\nprivate static class Transaction{/∗details omitted ∗/}\n// instance variable for a CreditCard\nTransaction[ ] history; // keep log of all transactions for this card\n}\nThe containing class is known as the outer class . The nested class is formally a\nmember of the outer class, and its fully qualiﬁed name is OuterName.NestedName .\nFor example, with the above deﬁnition the nested class is CreditCard.Transaction ,\nalthough we may refer to it simply as Transaction from within the CreditCard class.\nMuch like packages (see Section 1.8), the use of nested classes can help re-\nduce name collisions, as it is perfectly acceptable to have another class named\nTransaction nested within some other class (or as a self-standing class).\nA nested class has an independent set of modiﬁers from the outer class. Visi-\nbility modiﬁers (e.g., public ,private ) effect whether the nested class deﬁnition is\naccessible beyond the outer class deﬁnition. For example, a private nested class\ncan be used by the outer class, but by no other classes.\nA nested class can also be designated as either static or (by default) nonstatic,\nwith signiﬁcant consequences. A static nested class is most like a traditional class;\nits instances have no association with any speciﬁc instance of the outer class.\nA nonstatic nested class is more commonly known as an inner class in Java.\nAn instance of an inner class can only be created from within a nonstatic method of\nthe outer class, and that inner instance becomes associated with the outer instance\nthat creates it. Each instance of an inner class implicitly stores a reference to its\nassociated outer instance, accessible from within the inner class methods using the\nsyntax OuterName .this (as opposed to this, which refers to the inner instance).\nThe inner instance also has private access to all members of its associated outer\ninstance, and can rely on the formal type parameters of the outer class, if generic.\nwww.it-ebooks.info"
  },
  {
    "page": 115,
    "content": "2.7. Exercises 97\n2.7 Exercises\nReinforcement\nR-2.1 Give three examples of life-critical software applications.\nR-2.2 Give an example of a software application in which adaptability can mean the\ndifference between a prolonged lifetime of sales and bankruptcy.\nR-2.3 Describe a component from a text-editor GUI and the methods that it encapsu-\nlates.\nR-2.4 Assume that we change the CreditCard class (see Code Fragment 1.5) so that\ninstance variable balance hasprivate visibility. Why is the following implemen-\ntation of the PredatoryCreditCard.charge method ﬂawed?\npublic boolean charge(doubleprice){\nboolean isSuccess = super.charge(price);\nif(!isSuccess)\ncharge(5); // the penalty\nreturnisSuccess;\n}\nR-2.5 Assume that we change the CreditCard class (see Code Fragment 1.5) so that\ninstance variable balance hasprivate visibility. Why is the following implemen-\ntation of the PredatoryCreditCard.charge method ﬂawed?\npublic boolean charge(doubleprice){\nboolean isSuccess = super.charge(price);\nif(!isSuccess)\nsuper.charge(5); // the penalty\nreturnisSuccess;\n}\nR-2.6 Give a short fragment of Java code that uses the progression classes from Sec-\ntion 2.2.3 to ﬁnd the eighth value of a Fibonacci progression that starts with 2\nand 2 as its ﬁrst two values.\nR-2.7 If we choose an increment of 128, how many calls to the nextValue method from\ntheArithmeticProgression class of Section 2.2.3 can we make before we cause a\nlong-integer overﬂow?\nR-2.8 Can two interfaces mutually extend each other? Why or why not?\nR-2.9 What are some potential efﬁciency disadvantages of having very deep inheritance\ntrees, that is, a large set of classes, A,B,C, and so on, such that BextendsA,C\nextendsB,DextendsC, etc.?\nR-2.10 What are some potential efﬁciency disadvantages of having very shallow inheri-\ntance trees, that is, a large set of classes, A,B,C, and so on, such that all of these\nclasses extend a single class, Z?\nwww.it-ebooks.info"
  },
  {
    "page": 116,
    "content": "98 Chapter 2. Object-Oriented Design\nR-2.11 Consider the following code fragment, taken from some package:\npublic class Maryland extends State{\nMaryland(){/∗null constructor ∗/}\npublic void printMe(){System.out.println( \"Read it.\" );}\npublic static void main(String[ ] args) {\nRegion east = newState();\nState md = newMaryland();\nObject obj = newPlace();\nPlace usa = newRegion();\nmd.printMe();\neast.printMe();\n((Place) obj).printMe();\nobj = md;((Maryland) obj).printMe();\nobj = usa;\n((Place) obj).printMe();usa = md;\n((Place) usa).printMe();\n}\n}\nclassStateextends Region{\nState(){/∗null constructor ∗/}\npublic void printMe(){System.out.println( \"Ship it.\" );}\n}classRegionextends Place{\nRegion(){/∗null constructor ∗/}\npublic void printMe(){System.out.println( \"Box it.\" );}\n}\nclassPlaceextends Object{\nPlace(){/∗null constructor ∗/}\npublic void printMe(){System.out.println( \"Buy it.\" );}\n}\nWhat is the output from calling the main() method of the Maryland class?\nR-2.12 Draw a class inheritance diagram for the following set of classes:\n•ClassGoat extendsObject and adds an instance variable tailand methods\nmilk() andjump() .\n•ClassPigextendsObject and adds an instance variable nose and methods\neat(food) andwallow() .\n•ClassHorse extendsObject and adds instance variables height andcolor ,\nand methods run() andjump() .\n•ClassRacer extendsHorse and adds a method race() .\n•ClassEquestrian extendsHorse and adds instance variable weight andis-\nTrained , and methods trot() andisTrained() .\nwww.it-ebooks.info"
  },
  {
    "page": 117,
    "content": "2.7. Exercises 99\nR-2.13 Consider the inheritance of classes from Exercise R-2.12, and let dbe an object\nvariable of type Horse . Ifdrefers to an actual object of type Equestrian , can it\nbe cast to the class Racer ? Why or why not?\nR-2.14 Give an example of a Java code fragment that performs an array reference that\nis possibly out of bounds, and if it is out of bounds, the program catches that\nexception and prints the following error message:\n“Don’t try buffer overflow attacks in Java! ”\nR-2.15 If the parameter to the makePayment method of the CreditCard class (see Code\nFragment 1.5) were a negative number, that would have the effect of raising\nthe balance on the account. Revise the implementation so that it throws an\nIllegalArgumentException if a negative amount is sent as a parameter.\nCreativity\nC-2.16 Suppose you are on the design team for a new e-book reader. What are the\nprimary classes and methods that the Java software for your reader will need?\nYou should include an inheritance diagram for this code, but you don’t need to\nwrite any actual code. Your software architecture should at least include ways for\ncustomers to buy new books, view their list of purchased books, and read their\npurchased books.\nC-2.17 Most modern Java compilers have optimizers that can detect simple cases when\nit is logically impossible for certain statements in a program to ever be executed.\nIn such cases, the compiler warns the programmer about the useless code. Write\na short Java method that contains code for which it is provably impossible for\nthat code to ever be executed, yet the Java compiler does not detect this fact.\nC-2.18 ThePredatoryCreditCard class provides a processMonth() method that models\nthe completion of a monthly cycle. Modify the class so that once a customer has\nmade ten calls to charge during a month, each additional call to that method in\nthe current month results in an additional $1 surcharge.\nC-2.19 Modify the PredatoryCreditCard class so that a customer is assigned a minimum\nmonthly payment, as a percentage of the balance, and so that a late fee is assessed\nif the customer does not subsequently pay that minimum amount before the next\nmonthly cycle.\nC-2.20 Assume that we change the CreditCard class (see Code Fragment 1.5) so that\ninstance variable balance hasprivate visibility, but a new protected method is\nadded, with signature setBalance(newBalance) . Show how to properly imple-\nment the method PredatoryCreditCard.processMonth() in this setting.\nC-2.21 Write a program that consists of three classes, A,B, and C, such that Bextends\nAand that Cextends B. Each class should deﬁne an instance variable named “ x”\n(that is, each has its own variable named x). Describe a way for a method in C\nto access and set A’s version of xto a given value, without changing BorC’s\nversion.\nwww.it-ebooks.info"
  },
  {
    "page": 118,
    "content": "100 Chapter 2. Object-Oriented Design\nC-2.22 Explain why the Java dynamic dispatch algorithm, which looks for the method\nto invoke for a call obj.foo() , will never get into an inﬁnite loop.\nC-2.23 Modify the advance method of the FibonacciProgression class so as to avoid use\nof any temporary variable.\nC-2.24 Write a Java class that extends the Progression class so that each value in the pro-\ngression is the absolute value of the difference between the previous two values.\nYou should include a default constructor that starts with 2 and 200 as the ﬁrst two\nvalues and a parametric constructor that starts with a speciﬁed pair of numbers\nas the ﬁrst two values.\nC-2.25 Redesign the Progression class to be abstract and generic, producing a sequence\nof values of generic type T, and supporting a single constructor that accepts an\ninitial value. Make all corresponding modiﬁcations to the rest of the classes in\nour hierarchy so that they remain as nongeneric classes, while inheriting from the\nnew generic Progression class.\nC-2.26 Use a solution to Exercise C-2.25 to create a new progression class for which\neach value is the square root of the previous value, represented as a Double .\nYou should include a default constructor that has 65 ,536 as the ﬁrst value and a\nparametric constructor that starts with a speciﬁed number as the ﬁrst value.\nC-2.27 Use a solution to Exercise C-2.25 to reimplement the FibonacciProgression sub-\nclass to rely on the BigInteger class, in order to avoid overﬂows all together.\nC-2.28 Write a set of Java classes that can simulate an Internet application in which one\nparty, Alice, is periodically creating a set of packets that she wants to send to\nBob. An Internet process is continually checking if Alice has any packets to\nsend, and if so, it delivers them to Bob’s computer; Bob is periodically checking\nif his computer has a packet from Alice, and if so, he reads and deletes it.\nC-2.29 Write a Java program that inputs a polynomial in standard algebraic notation and\noutputs the ﬁrst derivative of that polynomial.\nProjects\nP-2.30 Write a Java program that inputs a document and then outputs a bar-chart plot of\nthe frequencies of each alphabet character that appears within that document.\nP-2.31 Write a Java program to simulate an ecosystem containing two types of creatures,\nbears andﬁsh. The ecosystem consists of a river, which is modeled as a relatively\nlarge array. Each cell of the array should contain an Animal object, which can\nbe aBear object, a Fish object, or null. In each time step, based on a random\nprocess, each animal either attempts to move into an adjacent array cell or stay\nwhere it is. If two animals of the same type are about to collide in the same\ncell, then they stay where they are, but they create a new instance of that type\nof animal, which is placed in a random empty (i.e., previously null) cell in the\narray. If a bear and a ﬁsh collide, however, then the ﬁsh dies (i.e., it disappears).\nUse actual object creation, via the new operator, to model the creation of new\nobjects, and provide a visualization of the array after each time step.\nwww.it-ebooks.info"
  },
  {
    "page": 119,
    "content": "Chapter Notes 101\nP-2.32 Write a simulator as in the previous project, but add a boolean gender ﬁeld and\na ﬂoating-point strength ﬁeld to each Animal object. Now, if two animals of\nthe same type try to collide, then they only create a new instance of that type of\nanimal if they are of different genders. Otherwise, if two animals of the same\ntype and gender try to collide, then only the one of larger strength survives.\nP-2.33 Write a Java program that simulates a system that supports the functions of an e-\nbook reader. You should include methods for users of your system to “buy” new\nbooks, view their list of purchased books, and read their purchased books. Your\nsystem should use actual books, which have expired copyrights and are available\non the Internet, to populate your set of available books for users of your system\nto “purchase” and read.\nP-2.34 Deﬁne aPolygon interface that has methods area() andperimeter() . Then im-\nplement classes for Triangle ,Quadrilateral ,Pentagon ,Hexagon , andOctagon ,\nwhich implement this interface, with the obvious meanings for the area() and\nperimeter() methods. Also implement classes, IsoscelesTriangle ,Equilateral-\nTriangle ,Rectangle , andSquare , which have the appropriate inheritance rela-\ntionships. Finally, write a simple user interface, which allows users to create\npolygons of the various types, input their geometric dimensions, and then out-\nput their area and perimeter. For extra effort, allow users to input polygons by\nspecifying their vertex coordinates and be able to test if two such polygons are\nsimilar.\nP-2.35 Write a Java program that inputs a list of words, separated by whitespace, and\noutputs how many times each word appears in the list. You need not worry about\nefﬁciency at this point, however, as this topic is something that will be addressed\nlater in this book.\nP-2.36 Write a Java program that can “make change.” Your program should take two\nnumbers as input, one that is a monetary amount charged and the other that is\na monetary amount given. It should then return the number of each kind of bill\nand coin to give back as change for the difference between the amount given and\nthe amount charged. The values assigned to the bills and coins can be based on\nthe monetary system of any current or former government. Try to design your\nprogram so that it returns the fewest number of bills and coins as possible.\nChapter Notes\nFor a broad overview of developments in computer science and engineering, we refer the\nreader to The Computer Science and Engineering Handbook [89]. For more information\nabout the Therac-25 incident, please see the paper by Leveson and Turner [65].\nThe reader interested in studying object-oriented programming further is referred to the\nbooks by Booch [16], Budd [19], and Liskov and Guttag [67]. Liskov and Guttag also pro-\nvide a nice discussion of abstract data types, as does the book chapter by Demurjian [28] in\ntheThe Computer Science and Engineering Handbook [89]. Design patterns are described\nin the book by Gamma et al. [37].\nwww.it-ebooks.info"
  },
  {
    "page": 120,
    "content": "www.it-ebooks.info"
  },
  {
    "page": 121,
    "content": "Chapter\n3Fundamental Data Structures\nContents\n3.1 Using Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n3.1.1 Storing Game Entries in an Array . . . . . . . . . . . . . . 104\n3.1.2 Sorting an Array . . . . . . . . . . . . . . . . . . . . . . . 110\n3.1.3 java.util Methods for Arrays and Random Numbers . . . . 112\n3.1.4 Simple Cryptography with Character Arrays . . . . . . . . 115\n3.1.5 Two-Dimensional Arrays and Positional Games . . . . . . 118\n3.2 Singly Linked Lists . . . . . . . . . . . . . . . . . . . . . . . 122\n3.2.1 Implementing a Singly Linked List Class . . . . . . . . . . 126\n3.3 Circularly Linked Lists . . . . . . . . . . . . . . . . . . . . . 128\n3.3.1 Round-Robin Scheduling . . . . . . . . . . . . . . . . . . 128\n3.3.2 Designing and Implementing a Circularly Linked List . . . 129\n3.4 Doubly Linked Lists . . . . . . . . . . . . . . . . . . . . . . 132\n3.4.1 Implementing a Doubly Linked List Class . . . . . . . . . 135\n3.5 Equivalence Testing . . . . . . . . . . . . . . . . . . . . . . 138\n3.5.1 Equivalence Testing with Arrays . . . . . . . . . . . . . . 139\n3.5.2 Equivalence Testing with Linked Lists . . . . . . . . . . . 140\n3.6 Cloning Data Structures . . . . . . . . . . . . . . . . . . . 141\n3.6.1 Cloning Arrays . . . . . . . . . . . . . . . . . . . . . . . . 142\n3.6.2 Cloning Linked Lists . . . . . . . . . . . . . . . . . . . . . 144\n3.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\nwww.it-ebooks.info"
  },
  {
    "page": 122,
    "content": "104 Chapter 3. Fundamental Data Structures\n3.1 Using Arrays\nIn this section, we explore a few applications of arrays—the concrete data structures\nintroduced in Section 1.3 that access their entries using integer indices.\n3.1.1 Storing Game Entries in an Array\nThe ﬁrst application we study is storing a sequence of high score entries for a video\ngame in an array. This is representative of many applications in which a sequence\nof objects must be stored. We could just as easily have chosen to store records for\npatients in a hospital or the names of players on a football team. Nevertheless, let\nus focus on storing high score entries, which is a simple application that is already\nrich enough to present some important data-structuring concepts.\nTo begin, we consider what information to include in an object representing a\nhigh score entry. Obviously, one component to include is an integer representing\nthe score itself, which we identify as score . Another useful thing to include is the\nname of the person earning this score, which we identify as name . We could go\non from here, adding ﬁelds representing the date the score was earned or game\nstatistics that led to that score. However, we omit such details to keep our example\nsimple. A Java class, GameEntry , representing a game entry, is given in Code\nFragment 3.1.\n1public class GameEntry{\n2privateString name; // name of the person earning this score\n3private int score; // the score value\n4/∗∗Constructs a game entry with given parameters.. ∗/\n5publicGameEntry(String n, ints){\n6name = n;\n7score = s;\n8}\n9/∗∗Returns the name ﬁeld. ∗/\n10publicString getName() {returnname;}\n11/∗∗Returns the score ﬁeld. ∗/\n12public int getScore(){returnscore;}\n13/∗∗Returns a string representation of this entry. ∗/\n14publicString toString(){\n15return\"(\"+ name + \", \"+ score + \")\";\n16}\n17}\nCode Fragment 3.1: Java code for a simple GameEntry class. Note that we include\nmethods for returning the name and score for a game entry object, as well as a\nmethod for returning a string representation of this entry.\nwww.it-ebooks.info"
  },
  {
    "page": 123,
    "content": "3.1. Using Arrays 105\nA Class for High Scores\nTo maintain a sequence of high scores, we develop a class named Scoreboard . A\nscoreboard is limited to a certain number of high scores that can be saved; once that\nlimit is reached, a new score only qualiﬁes for the scoreboard if it is strictly higher\nthan the lowest “high score” on the board. The length of the desired scoreboard\nmay depend on the game, perhaps 10, 50, or 500. Since that limit may vary, we\nallow it to be speciﬁed as a parameter to our Scoreboard constructor.\nInternally, we will use an array named board to manage the GameEntry in-\nstances that represent the high scores. The array is allocated with the speciﬁed\nmaximum capacity, but all entries are initially null. As entries are added, we will\nmaintain them from highest to lowest score, starting at index 0 of the array. We\nillustrate a typical state of the data structure in Figure 3.1, and give Java code to\nconstruct such a data structure in Code Fragment 3.2.\nFigure 3.1: An illustration of an array of length ten storing references to six\nGameEntry objects in the cells with indices 0 to 5; the rest are null references.\n1/∗∗Class for storing high scores in an array in nondecreasing order. ∗/\n2public class Scoreboard{\n3private int numEntries = 0; // number of actual entries\n4privateGameEntry[ ] board; // array of game entries (names & scores)\n5/∗∗Constructs an empty scoreboard with the given capacity for storing entries. ∗/\n6publicScoreboard( intcapacity){\n7board = newGameEntry[capacity];\n8}\n...// more methods will go here\n36}\nCode Fragment 3.2: The beginning of a Scoreboard class for maintaining a set of\nscores as GameEntry objects. (Completed in Code Fragments 3.3 and 3.4.)\nwww.it-ebooks.info"
  },
  {
    "page": 124,
    "content": "106 Chapter 3. Fundamental Data Structures\nAdding an Entry\nOne of the most common updates we might want to make to a Scoreboard is to add\na new entry. Keep in mind that not every entry will necessarily qualify as a high\nscore. If the board is not yet full, any new entry will be retained. Once the board isfull, a new entry is only retained if it is strictly better than one of the other scores,in particular, the last entry of the scoreboard, which is the lowest of the high scores.\nCode Fragment 3.3 provides an implementation of an update method for the\nScoreboard class that considers the addition of a new game entry.\n9/∗∗Attempt to add a new score to the collection (if it is high enough) ∗/\n10public void add(GameEntry e) {\n11intnewScore = e.getScore();\n12// is the new entry e really a high score?\n13if(numEntries <board.length||newScore >board[numEntries−1].getScore()){\n14 if(numEntries <board.length) // no score drops from the board\n15 numEntries++; // so overall number increases\n16 // shift any lower scores rightward to make room for the new entry\n17 intj = numEntries−1;\n18 while(j>0 && board[j−1].getScore() <newScore){\n19 board[j] = board[j −1]; // shift entry from j-1 to j\n20 j−−; // and decrement j\n21}\n22 board[j] = e; // when done, add new entry\n23}\n24}\nCode Fragment 3.3: Java code for inserting a GameEntry object into a Scoreboard .\nWhen a new score is considered, the ﬁrst goal is to determine whether it quali-\nﬁes as a high score. This will be the case (see line 13) if the scoreboard is below itscapacity, or if the new score is strictly higher than the lowest score on the board.\nOnce it has been determined that a new entry should be kept, there are two\nremaining tasks: (1) properly update the number of entries, and (2) place the newentry in the appropriate location, shifting entries with inferior scores as needed.\nThe ﬁrst of these tasks is easily handled at lines 14 and 15, as the total number\nof entries can only be increased if the board is not yet at full capacity. (When full,the addition of a new entry will be counteracted by the removal of the entry with\nlowest score.)\nThe placement of the new entry is implemented by lines 17–22. Index jis\ninitially set to numEntries−1, which is the index at which the last GameEntry will\nreside after completing the operation. Either jis the correct index for the newest\nentry, or one or more immediately before it will have lesser scores. The while loop\nchecks the compound condition, shifting entries rightward and decrementing j, as\nlong as there is another entry at index j−1 with a score less than the new score.\nwww.it-ebooks.info"
  },
  {
    "page": 125,
    "content": "3.1. Using Arrays 107\nFigure 3.2: Preparing to add Jill’s GameEntry object to the board array. In order to\nmake room for the new reference, we have to shift any references to game entries\nwith smaller scores than the new one to the right by one cell.\nFigure 3.2 shows an example of the process, just after the shifting of existing\nentries, but before adding the new entry. When the loop completes, jwill be the\ncorrect index for the new entry. Figure 3.3 shows the result of a complete operation,\nafter the assignment of board[j] = e , accomplished by line 22 of the code.\nIn Exercise C-3.19, we explore how game entry addition might be simpliﬁed\nfor the case when we don’t need to preserve relative orders.\nFigure 3.3: Adding a reference to Jill’s GameEntry object to the board array. The\nreference can now be inserted at index 2, since we have shifted all references to\nGameEntry objects with scores less than the new one to the right.\nwww.it-ebooks.info"
  },
  {
    "page": 126,
    "content": "108 Chapter 3. Fundamental Data Structures\nRemoving an Entry\nSuppose some hot shot plays our video game and gets his or her name on our high\nscore list, but we later learn that cheating occurred. In this case, we might want\nto have a method that lets us remove a game entry from the list of high scores.\nTherefore, let us consider how we might remove a reference to a GameEntry object\nfrom aScoreboard .\nWe choose to add a method to the Scoreboard class, with signature remove( i),\nwhere idesignates the current index of the entry that should be removed and re-\nturned. When a score is removed, any lower scores will be shifted upward, to ﬁll in\nfor the removed entry. If index iis outside the range of current entries, the method\nwill throw an IndexOutOfBoundsException .\nOur implementation for remove will involve a loop for shifting entries, much\nlike our algorithm for addition, but in reverse. To remove the reference to the object\nat index i, we start at index iand move all the references at indices higher than i\none cell to the left. (See Figure 3.4.)\nFigure 3.4: An illustration of the removal of Paul’s score from index 3 of an array\nstoring references to GameEntry objects.\nOur implementation of the remove method for the Scoreboard class is given\nin Code Fragment 3.4. The details for doing the remove operation contain a few\nsubtle points. The ﬁrst is that, in order to remove and return the game entry (let’s\ncall it e) at index iin our array, we must ﬁrst save ein a temporary variable. We\nwill use this variable to return ewhen we are done removing it.\nwww.it-ebooks.info"
  },
  {
    "page": 127,
    "content": "3.1. Using Arrays 109\n25/∗∗Remove and return the high score at index i. ∗/\n26publicGameEntry remove( inti)throwsIndexOutOfBoundsException {\n27if(i<0||i>= numEntries)\n28 throw new IndexOutOfBoundsException( \"Invalid index: \" + i);\n29GameEntry temp = board[i]; // save the object to be removed\n30for(intj = i; j<numEntries−1; j++) // count up from i (not down)\n31 board[j] = board[j+1]; // move one cell to the left\n32board[numEntries −1 ] =null; // null out the old last score\n33numEntries−−;\n34returntemp; // return the removed object\n35}\nCode Fragment 3.4: Java code for performing the Scoreboard.remove operation.\nThe second subtle point is that, in moving references higher than ione cell to\nthe left, we don’t go all the way to the end of the array. First, we base our loop\non the number of current entries, not the capacity of the array, because there isno reason for “shifting” a series of null references that may be at the end of the\narray. We also carefully deﬁne the loop condition, j<numEntries−1, so that the\nlast iteration of the loop assigns board[numEntries−2] = board[numEntries −1].\nThere is no entry to shift into cell board[numEntries−1], so we return that cell to\nnull just after the loop. We conclude by returning a reference to the removed entry\n(which no longer has any reference pointing to it within the board array).\nConclusions\nIn the version of the Scoreboard class that is available online, we include an im-\nplementation of the toString() method, which allows us to display the contents of\nthe current scoreboard, separated by commas. We also include a main method that\nperforms a basic test of the class.\nThe methods for adding and removing objects in an array of high scores are\nsimple. Nevertheless, they form the basis of techniques that are used repeatedlyto build more sophisticated data structures. These other structures may be moregeneral than the array structure above, of course, and often they will have a lotmore operations that they can perform than just addandremove . But studying the\nconcrete array data structure, as we are doing now, is a great starting point to un-\nderstanding these other structures, since every data structure has to be implemented\nusing concrete means.\nIn fact, later in this book, we will study a Java collections class, ArrayList ,\nwhich is more general than the array structure we are studying here. The ArrayList\nhas methods to operate on an underlying array; yet it also eliminates the error thatoccurs when adding an object to a full array by automatically copying the objectsinto a larger array when necessary. We will discuss the ArrayList class in far more\ndetail in Section 7.2.\nwww.it-ebooks.info"
  },
  {
    "page": 128,
    "content": "110 Chapter 3. Fundamental Data Structures\n3.1.2 Sorting an Array\nIn the previous subsection, we considered an application for which we added an\nobject to an array at a given position while shifting other elements so as to keep the\nprevious order intact. In this section, we use a similar technique to solve the sorting\nproblem, that is, starting with an unordered array of elements and rearranging them\ninto nondecreasing order.\nThe Insertion-Sort Algorithm\nWe study several sorting algorithms in this book, most of which are described in\nChapter 12. As a warm-up, in this section we describe a simple sorting algorithm\nknown as insertion-sort . The algorithm proceeds by considering one element at\na time, placing the element in the correct order relative to those before it. We\nstart with the ﬁrst element in the array, which is trivially sorted by itself. When\nconsidering the next element in the array, if it is smaller than the ﬁrst, we swap\nthem. Next we consider the third element in the array, swapping it leftward until it\nis in its proper order relative to the ﬁrst two elements. We continue in this manner\nwith the fourth element, the ﬁfth, and so on, until the whole array is sorted. We can\nexpress the insertion-sort algorithm in pseudocode, as shown in Code Fragment 3.5.\nAlgorithm InsertionSort( A):\nInput: An array Aofncomparable elements\nOutput: The array Awith elements rearranged in nondecreasing order\nforkfrom 1 to n−1do\nInsert A[k]at its proper location within A[0],A[1],...,A[k].\nCode Fragment 3.5: High-level description of the insertion-sort algorithm.\nThis is a simple, high-level description of insertion-sort. If we look back to\nCode Fragment 3.3 in Section 3.1.1, we see that the task of inserting a new entry\ninto the list of high scores is almost identical to the task of inserting a newly con-\nsidered element in insertion-sort (except that game scores were ordered from high\nto low). We provide a Java implementation of insertion-sort in Code Fragment 3.6,\nusing an outer loop to consider each element in turn, and an inner loop that moves\na newly considered element to its proper location relative to the (sorted) subarray\nof elements that are to its left. We illustrate an example run of the insertion-sort\nalgorithm in Figure 3.5.\nWe note that if an array is already sorted, the inner loop of insertion-sort does\nonly one comparison, determines that there is no swap needed, and returns back\nto the outer loop. Of course, we might have to do a lot more work than this if the\ninput array is extremely out of order. In fact, we will have to do the most work if\nthe input array is in decreasing order.\nwww.it-ebooks.info"
  },
  {
    "page": 129,
    "content": "3.1. Using Arrays 111\n1/∗∗Insertion-sort of an array of characters into nondecreasing order ∗/\n2public static void insertionSort( char[ ] data){\n3intn = data.length;\n4for(intk = 1; k <n; k++){ // begin with second character\n5 charcur = data[k]; // time to insert cur=data[k]\n6 intj = k; // ﬁnd correct index j for cur\n7 while(j>0 && data[j−1]>cur){// thus, data[j-1] must go after cur\n8 data[j] = data[j−1]; // slide data[j-1] rightward\n9 j−−; // and consider previous j for cur\n10}\n11 data[j] = cur; // this is the proper place for cur\n12}\n13}\nCode Fragment 3.6: Java code for performing insertion-sort on a character array.\nFigure 3.5: Execution of the insertion-sort algorithm on an array of eight charac-\nters. Each row corresponds to an iteration of the outer loop, and each copy of the\nsequence in a row corresponds to an iteration of the inner loop. The current element\nthat is being inserted is highlighted in the array, and shown as the curvalue.insertinsertinsert\n0\n00\n0\n0\n0\n000 0\n0 0\nDone!0C E H G F\nB C A E H G F D\nB E H G F C D\nA H G F B C D E\nA F B C D E HA G F B C D E HE H G FD C B B EC\nFFGH G F D C\nA F B C D E H\nC D E H G\nB AD\nA B C D E G H A B CA\nGHEAD\nD E H G\nH G F E DA\nCBB A\n2 3 4 5 6 71 2 3 4 5 6 7\n1 2 3 4 5 6 7\n1 2 3 4 5 6 7\n1 2 3 4 5 6 7\n1 2 3 4 5 6 71 2 3 4 5 6 71 2 3 4 5 6 7 1 2 3 4 5 6 7\n1\n12 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7cur\n1 2 3 4 5 6 7move move moveno moveno move\nno move\nno move\nmove no move\nmove move no move\nwww.it-ebooks.info"
  },
  {
    "page": 130,
    "content": "112 Chapter 3. Fundamental Data Structures\n3.1.3 java.util Methods for Arrays and Random Numbers\nBecause arrays are so important, Java provides a class, java.util.Arrays , with a\nnumber of built-in static methods for performing common tasks on arrays. Later in\nthis book, we will describe the algorithms that several of these methods are based\nupon. For now, we provide an overview of the most commonly used methods of\nthat class, as follows (more discussion is in Section 3.5.1):\nequals( A,B):Returns true if and only if the array Aand the array Bare\nequal. Two arrays are considered equal if they have the\nsame number of elements and every corresponding pair\nof elements in the two arrays are equal. That is, AandB\nhave the same values in the same order.\nﬁll(A,x):Stores value xin every cell of array A, provided the type\nof array Ais deﬁned so that it is allowed to store the\nvalue x.\ncopyOf( A,n):Returns an array of size nsuch that the ﬁrst kelements of\nthis array are copied from A, where k=min{n,A.length}.\nIfn>A.length , then the last n−A.length elements in\nthis array will be padded with default values, e.g., 0 for\nan array of intandnull for an array of objects.\ncopyOfRange( A,s,t):Returns an array of size t−ssuch that the elements of\nthis array are copied in order from A[s]toA[t−1], where\ns<t, padded as with copyOf() ift>A.length .\ntoString( A):Returns a String representation of the array A, beginning\nwith[, ending with ], and with elements of Adisplayed\nseparated by string \", \" . The string representation of\nan element A[i]is obtained using String.valueOf( A[i]),\nwhich returns the string \"null\" for anull reference and\notherwise calls A[i].toString() .\nsort( A):Sorts the array Abased on a natural ordering of its el-\nements, which must be comparable. Sorting algorithms\nare the focus of Chapter 12.\nbinarySearch( A,x):Searches the sorted array Afor value x, returning the\nindex where it is found, or else the index of where it\ncould be inserted while maintaining the sorted order. The\nbinary-search algorithm is described in Section 5.1.3.\nAs static methods, these are invoked directly on the java.util.Arrays class, not\non a particular instance of the class. For example, if data were an array, we\ncould sort it with syntax, java.util.Arrays.sort(data) , or with the shorter syntax\nArrays.sort(data) if we ﬁrst import the Arrays class (see Section 1.8).\nwww.it-ebooks.info"
  },
  {
    "page": 131,
    "content": "3.1. Using Arrays 113\nPseudoRandom Number Generation\nAnother feature built into Java, which is often useful when testing programs dealing\nwith arrays, is the ability to generate pseudorandom numbers, that is, numbers thatappear to be random (but are not necessarily truly random). In particular, Javahas a built-in class, java.util.Random , whose instances are pseudorandom number\ngenerators , that is, objects that compute a sequence of numbers that are statistically\nrandom. These sequences are not actually random, however, in that it is possibleto predict the next number in the sequence given the past list of numbers. Indeed,a popular pseudorandom number generator is to generate the next number, next,\nfrom the current number, cur, according to the formula (in Java syntax):\nnext = (a ∗cur + b) % n;\nwherea,b, andnare appropriately chosen integers, and %is the modulus opera-\ntor. Something along these lines is, in fact, the method used by java.util.Random\nobjects, with n=2\n48. It turns out that such a sequence can be proven to be statis-\ntically uniform, which is usually good enough for most applications requiring ran-dom numbers, such as games. For applications, such as computer security settings,\nwhere unpredictable random sequences are needed, this kind of formula should not\nbe used. Instead, ideally a sample from a source that is actually random should beused, such as radio static coming from outer space.\nSince the next number in a pseudorandom generator is determined by the pre-\nvious number(s), such a generator always needs a place to start, which is called itsseed. The sequence of numbers generated for a given seed will always be the same.\nThe seed for an instance of the java.util.Random class can be set in its constructor\nor with its setSeed() method.\nOne common trick to get a different sequence each time a program is run is\nto use a seed that will be different for each run. For example, we could use sometimed input from a user or we could set the seed to the current time in millisecondssince January 1, 1970 (provided by method System.currentTimeMillis ).\nMethods of the java.util.Random class include the following:\nnextBoolean() :Returns the next pseudorandom boolean value.\nnextDouble() :Returns the next pseudorandom double value, between\n0.0 and 1.0.\nnextInt() :Returns the next pseudorandom intvalue.\nnextInt( n):Returns the next pseudorandom intvalue in the range\nfrom 0 up to but not including n.\nsetSeed( s):Sets the seed of this pseudorandom number generator tothelong s.\nwww.it-ebooks.info"
  },
  {
    "page": 132,
    "content": "114 Chapter 3. Fundamental Data Structures\nAn Illustrative Example\nWe provide a short (but complete) illustrative program in Code Fragment 3.7.\n1importjava.util.Arrays;\n2importjava.util.Random;\n3/∗∗Program showing some array uses. ∗/\n4public class ArrayTest{\n5public static void main(String[ ] args) {\n6intdata[ ] = new int[10];\n7Random rand = newRandom(); // a pseudo-random number generator\n8rand.setSeed(System.currentTimeMillis()); // use current time as a seed\n9// ﬁll the data array with pseudo-random numbers from 0 to 99, inclusive\n10for(inti = 0; i<data.length; i++)\n11 data[i] = rand.nextInt(100); // the next pseudo-random number\n12int[ ] orig = Arrays.copyOf(data, data.length); // make a copy of the data array\n13System.out.println( \"arrays equal before sort: \" +Arrays.equals(data, orig));\n14Arrays.sort(data); // sorting the data array (orig is unchanged)\n15System.out.println( \"arrays equal after sort: \" + Arrays.equals(data, orig));\n16System.out.println( \"orig = \" + Arrays.toString(orig));\n17System.out.println( \"data = \" + Arrays.toString(data));\n18}\n19}\nCode Fragment 3.7: A simple test of some built-in methods in java.util.Arrays .\nWe show a sample output of this program below:\narrays equal before sort: true\narrays equal after sort: falseorig = [41, 38, 48, 12, 28, 46, 33, 19, 10, 58]data = [10, 12, 19, 28, 33, 38, 41, 46, 48, 58]\nIn another run, we got the following output:\narrays equal before sort: truearrays equal after sort: falseorig = [87, 49, 70, 2, 59, 37, 63, 37, 95, 1]\ndata = [1, 2, 37, 37, 49, 59, 63, 70, 87, 95]\nBy using a pseudorandom number generator to determine program values, we\nget a different input to our program each time we run it. This feature is, in fact, what\nmakes pseudorandom number generators useful for testing code, particularly when\ndealing with arrays. Even so, we should not use random test runs as a replacement\nfor reasoning about our code, as we might miss important special cases in test runs.Note, for example, that there is a slight chance that the origanddata arrays will be\nequal even after data is sorted, namely, if origis already ordered. The odds of this\noccurring are less than 1 in 3 million, so it’s unlikely to happen during even a few\nthousand test runs; however, we need to reason that this is possible.\nwww.it-ebooks.info"
  },
  {
    "page": 133,
    "content": "3.1. Using Arrays 115\n3.1.4 Simple Cryptography with Character Arrays\nAn important application of character arrays and strings is cryptography , which\nis the science of secret messages. This ﬁeld involves the process of encryption ,\nin which a message, called the plaintext , is converted into a scrambled message,\ncalled the ciphertext . Likewise, cryptography studies corresponding ways of per-\nforming decryption , turning a ciphertext back into its original plaintext.\nArguably the earliest encryption scheme is the Caesar cipher , which is named\nafter Julius Caesar, who used this scheme to protect important military messages.\n(All of Caesar’s messages were written in Latin, of course, which already makes\nthem unreadable for most of us!) The Caesar cipher is a simple way to obscure a\nmessage written in a language that forms words with an alphabet.\nThe Caesar cipher involves replacing each letter in a message with the letter that\nis a certain number of letters after it in the alphabet. So, in an English message, we\nmight replace each A with D, each B with E, each C with F, and so on, if shifting by\nthree characters. We continue this approach all the way up to W, which is replaced\nwith Z. Then, we let the substitution pattern wrap around , so that we replace X\nwith A, Y with B, and Z with C.\nConverting Between Strings and Character Arrays\nGiven that strings are immutable, we cannot directly edit an instance to encrypt\nit. Instead, our goal will be to generate a new string. A convenient technique for\nperforming string transformations is to create an equivalent array of characters, edit\nthe array, and then reassemble a (new) string based on the array.\nJava has support for conversions from strings to character arrays and vice versa.\nGiven a string S, we can create a new character array matching Sby using the\nmethod, S.toCharArray() . For example, if s=\"bird\" , the method returns the char-\nacter array A={'b','i','r','d'}. Conversely, there is a form of the String\nconstructor that accepts a character array as a parameter. For example, with char-\nacter array A={'b','i','r','d'}, the syntax newString( A)produces\"bird\" .\nUsing Character Arrays as Replacement Codes\nIf we were to number our letters like array indices, so that A is 0, B is 1, C is 2, then\nwe can represent the replacement rule as a character array, encoder , such that A is\nmapped to encoder[0] , B is mapped to encoder[1] , and so on. Then, in order to ﬁnd\na replacement for a character in our Caesar cipher, we need to map the characters\nfrom A to Z to the respective numbers from 0 to 25. Fortunately, we can rely on the\nfact that characters are represented in Unicode by integer code points, and the code\npoints for the uppercase letters of the Latin alphabet are consecutive (for simplicity,\nwe restrict our encryption to uppercase letters).\nwww.it-ebooks.info"
  },
  {
    "page": 134,
    "content": "116 Chapter 3. Fundamental Data Structures\nJava allows us to “subtract” two characters from each other, with an integer\nresult equal to their separation distance in the encoding. Given a variable cthat is\nknown to be an uppercase letter, the Java computation, j= c−'A'produces the\ndesired index j. As a sanity check, if character cis'A', then j=0. When cis'B',\nthe difference is 1. In general, the integer jthat results from such a calculation can\nbe used as an index into our precomputed encoder array, as illustrated in Figure 3.6.\n10 24 23 22 21 20 19 18 17 16 15 14 13 12 1125 9 8 7 6 5 4 3 2 1 0M O P Q R S T U V W X Y Z A B C D E F G H I J K L N\nHere is the\nreplacement for 'T'=In UnicodeUsing'T'as an indexencoder array\n= 19'T'−'A'\n65 84−\nFigure 3.6: Illustrating the use of uppercase characters as indices, in this case to\nperform the replacement rule for Caesar cipher encryption.\nThe process of decrypting the message can be implemented by simply using\na different character array to represent the replacement rule—one that effectively\nshifts characters in the opposite direction.\nIn Code Fragment 3.8, we present a Java class that performs the Caesar cipher\nwith an arbitrary rotational shift. The constructor for the class builds the encoder\nand decoder translation arrays for the given rotation. We rely heavily on modular\narithmetic, as a Caesar cipher with a rotation of rencodes the letter having index k\nwith the letter having index (k+r)mod 26, where mod is the modulo operator,\nwhich returns the remainder after performing an integer division. This operator isdenoted with %in Java, and it is exactly the operator we need to easily perform\nthe wraparound at the end of the alphabet, for 26 mod 26 is 0, 27 mod 26 is 1,and 28 mod 26 is 2. The decoder array for the Caesar cipher is just the opposite—we replace each letter with the one rplaces before it, with wraparound; to avoid\nsubtleties involving negative numbers and the modulus operator, we will replace\nthe letter having code kwith the letter having code (k−r+26)mod 26.\nWith the encoder and decoder arrays in hand, the encryption and decryption\nalgorithms are essentially the same, and so we perform both by means of a private\nutility method named transform . This method converts a string to a character ar-\nray, performs the translation diagrammed in Figure 3.6 for any uppercase alphabetsymbols, and ﬁnally returns a new string, constructed from the updated array.\nThemain method of the class, as a simple test, produces the following output:\nEncryption code = DEFGHIJKLMNOPQRSTUVWXYZABCDecryption code = XYZABCDEFGHIJKLMNOPQRSTUVWSecret: WKH HDJOH LV LQ SODB; PHHW DW MRH’V.\nMessage: THE EAGLE IS IN PLAY; MEET AT JOE’S.\nwww.it-ebooks.info"
  },
  {
    "page": 135,
    "content": "3.1. Using Arrays 117\n1/∗∗Class for doing encryption and decryption using the Caesar Cipher. ∗/\n2public class CaesarCipher{\n3protected char [ ] encoder = new char [26]; // Encryption array\n4protected char [ ] decoder = new char [26]; // Decryption array\n5/∗∗Constructor that initializes the encryption and decryption arrays ∗/\n6publicCaesarCipher( introtation){\n7for(intk=0; k<26; k++){\n8 encoder[k] = ( char) ('A'+ (k + rotation) % 26);\n9 decoder[k] = ( char) ('A'+ (k−rotation + 26) % 26);\n10}\n11}\n12/∗∗Returns String representing encrypted message. ∗/\n13publicString encrypt(String message) {\n14returntransform(message, encoder); // use encoder array\n15}\n16/∗∗Returns decrypted message given encrypted secret. ∗/\n17publicString decrypt(String secret) {\n18returntransform(secret, decoder); // use decoder array\n19}\n20/∗∗Returns transformation of original String using given code. ∗/\n21privateString transform(String original, char[ ] code){\n22char[ ] msg = original.toCharArray();\n23for(intk=0; k<msg.length; k++)\n24 if(Character.isUpperCase(msg[k])) { // we have a letter to change\n25 intj = msg[k]−'A'; // will be value from 0 to 25\n26 msg[k] = code[j]; // replace the character\n27}\n28return new String(msg);\n29}\n30/∗∗Simple main method for testing the Caesar cipher ∗/\n31public static void main(String[ ] args) {\n32CaesarCipher cipher = newCaesarCipher(3);\n33System.out.println( \"Encryption code = \" +newString(cipher.encoder));\n34System.out.println( \"Decryption code = \" +newString(cipher.decoder));\n35String message = \"THE EAGLE IS IN PLAY; MEET AT JOE 'S.\";\n36String coded = cipher.encrypt(message);\n37System.out.println( \"Secret: \" + coded);\n38String answer = cipher.decrypt(coded);\n39System.out.println( \"Message: \" + answer); // should be plaintext again\n40}\n41}\nCode Fragment 3.8: A complete Java class for performing the Caesar cipher.\nwww.it-ebooks.info"
  },
  {
    "page": 136,
    "content": "118 Chapter 3. Fundamental Data Structures\n3.1.5 Two-Dimensional Arrays and Positional Games\nMany computer games, be they strategy games, simulation games, or ﬁrst-person\nconﬂict games, involve objects that reside in a two-dimensional space. Software for\nsuch positional games needs a way of representing objects in a two-dimensional\nspace. A natural way to do this is with a two-dimensional array , where we use two\nindices, say iand j, to refer to the cells in the array. The ﬁrst index usually refers\nto a row number and the second to a column number. Given such an array, we can\nmaintain two-dimensional game boards and perform other kinds of computations\ninvolving data stored in rows and columns.\nArrays in Java are one-dimensional; we use a single index to access each cell\nof an array. Nevertheless, there is a way we can deﬁne two-dimensional arrays in\nJava—we can create a two-dimensional array as an array of arrays. That is, we can\ndeﬁne a two-dimensional array to be an array with each of its cells being another\narray. Such a two-dimensional array is sometimes also called a matrix . In Java, we\nmay declare a two-dimensional array as follows:\nint[ ][ ] data = new int[8][10];\nThis statement creates a two-dimensional “array of arrays,” data, which is 8×10,\nhaving 8 rows and 10 columns. That is, data is an array of length 8 such that each\nelement of data is an array of length 10 of integers. (See Figure 3.7.) The following\nwould then be valid uses of array data andintvariables i,j, andk:\ndata[i][i+1] = data[i][i] + 3;\nj = data.length; // j is 8\nk = data[4].length; // k is 10\nTwo-dimensional arrays have many applications to numerical analysis. Rather\nthan going into the details of such applications, however, we explore an application\nof two-dimensional arrays for implementing a simple positional game.\n22 18 709 5 33 10 4 56 82 440\n45 32 830 120 750 660 13 77 20 105\n4 880 45 66 61 28 650 7 510 67\n940 12 36 3 20 100 306 590 0 500\n50 65 42 49 88 25 70 126 83 288\n398 233 5 83 59 232 49 8 365 90\n33 58 632 87 94 5 59 204 120 829\n62 394 3 4 102 140 183 390 16 268\n0\n1\n2\n3\n4\n5\n6\n70 1 2 3 4 5 6 7 9\nFigure 3.7: Illustration of a two-dimensional integer array, data, which has 8 rows\nand 10 columns. The value of data[3][5] is 100 and the value of data[6][2] is 632.\nwww.it-ebooks.info"
  },
  {
    "page": 137,
    "content": "3.1. Using Arrays 119\nTic-Tac-Toe\nAs most school children know, Tic-Tac-Toe is a game played in a three-by-three\nboard. Two players—X and O—alternate in placing their respective marks in the\ncells of this board, starting with player X. If either player succeeds in getting threeof his or her marks in a row, column, or diagonal, then that player wins.\nThis is admittedly not a sophisticated positional game, and it’s not even that\nmuch fun to play, since a good player O can always force a tie. Tic-Tac-Toe’s savinggrace is that it is a nice, simple example showing how two-dimensional arrays canbe used for positional games. Software for more sophisticated positional games,\nsuch as checkers, chess, or the popular simulation games, are all based on the same\napproach we illustrate here for using a two-dimensional array for Tic-Tac-Toe.\nThe basic idea is to use a two-dimensional array, board , to maintain the game\nboard. Cells in this array store values that indicate if that cell is empty or stores an\nX or O. That is, board is a three-by-three matrix, whose middle row consists of the\ncellsboard[1][0],board[1][1], andboard[1][2]. In our case, we choose to make the\ncells in the board array be integers, with a 0 indicating an empty cell, a 1 indicating\nan X, and a−1 indicating an O. This encoding allows us to have a simple way of\ntesting if a given board conﬁguration is a win for X or O, namely, if the values\nof a row, column, or diagonal add up to 3 or −3, respectively. We illustrate this\napproach in Figure 3.8.\nFigure 3.8: An illustration of a Tic-Tac-Toe board and the two-dimensional integer\narray,board , representing it.\nWe give a complete Java class for maintaining a Tic-Tac-Toe board for two\nplayers in Code Fragments 3.9 and 3.10. We show a sample output in Figure 3.9.Note that this code is just for maintaining the Tic-Tac-Toe board and register-ing moves; it doesn’t perform any strategy or allow someone to play Tic-Tac-Toeagainst the computer. The details of such a program are beyond the scope of this\nchapter, but it might nonetheless make a good course project (see Exercise P-8.67).\nwww.it-ebooks.info"
  },
  {
    "page": 138,
    "content": "120 Chapter 3. Fundamental Data Structures\n1/∗∗Simulation of a Tic-Tac-Toe game (does not do strategy). ∗/\n2public class TicTacToe{\n3public static ﬁnal int X = 1, O =−1; // players\n4public static ﬁnal int EMPTY = 0; // empty cell\n5private int board[ ][ ] = new int[3][3]; // game board\n6private int player; // current player\n7/∗∗Constructor ∗/\n8publicTicTacToe(){clearBoard();}\n9/∗∗Clears the board ∗/\n10public void clearBoard(){\n11for(inti = 0; i<3; i++)\n12 for(intj = 0; j<3; j++)\n13 board[i][j] = EMPTY; // every cell should be empty\n14player = X; // the ﬁrst player is 'X'\n15}\n16/∗∗Puts an X or O mark at position i,j. ∗/\n17public void putMark( inti,intj)throwsIllegalArgumentException {\n18if((i<0)||(i>2)||(j<0)||(j>2))\n19 throw new IllegalArgumentException( \"Invalid board position\" );\n20if(board[i][j] != EMPTY)\n21 throw new IllegalArgumentException( \"Board position occupied\" );\n22board[i][j] = player; // place the mark for the current player\n23player =−player; // switch players (uses fact that O = - X)\n24}\n25/∗∗Checks whether the board conﬁguration is a win for the given player. ∗/\n26public boolean isWin(intmark){\n27return((board[0][0] + board[0][1] + board[0][2] == mark ∗3)// row 0\n28||(board[1][0] + board[1][1] + board[1][2] == mark ∗3)// row 1\n29||(board[2][0] + board[2][1] + board[2][2] == mark ∗3)// row 2\n30||(board[0][0] + board[1][0] + board[2][0] == mark ∗3)// column 0\n31||(board[0][1] + board[1][1] + board[2][1] == mark ∗3)// column 1\n32||(board[0][2] + board[1][2] + board[2][2] == mark ∗3)// column 2\n33||(board[0][0] + board[1][1] + board[2][2] == mark ∗3)// diagonal\n34||(board[2][0] + board[1][1] + board[0][2] == mark ∗3));// rev diag\n35}\n36/∗∗Returns the winning player 's code, or 0 to indicate a tie (or unﬁnished game). ∗/\n37public int winner(){\n38if(isWin(X))\n39 return(X);\n40else if(isWin(O))\n41 return(O);\n42else\n43 return(0);\n44}\nCode Fragment 3.9: A simple, complete Java class for playing Tic-Tac-Toe between\ntwo players. (Continues in Code Fragment 3.10.)\nwww.it-ebooks.info"
  },
  {
    "page": 139,
    "content": "3.1. Using Arrays 121\n45/∗∗Returns a simple character string showing the current board. ∗/\n46publicString toString(){\n47StringBuilder sb = newStringBuilder();\n48for(inti=0; i<3; i++){\n49 for(intj=0; j<3; j++){\n50 switch(board[i][j]){\n51 caseX: sb.append( \"X\");break;\n52 caseO: sb.append( \"O\");break;\n53 caseEMPTY: sb.append( \" \");break;\n54}\n55 if(j<2) sb.append( \"|\"); // column boundary\n56}\n57 if(i<2) sb.append( \"\\n-----\\n\" ); // row boundary\n58}\n59returnsb.toString();\n60}\n61/∗∗Test run of a simple game ∗/\n62public static void main(String[ ] args) {\n63TicTacToe game = newTicTacToe();\n64/∗X moves: ∗/ /∗O moves: ∗/\n65game.putMark(1,1); game.putMark(0,2);\n66game.putMark(2,2); game.putMark(0,0);\n67game.putMark(0,1); game.putMark(2,1);\n68game.putMark(1,2); game.putMark(1,0);\n69game.putMark(2,0);\n70System.out.println(game);\n71intwinningPlayer = game.winner();\n72String[ ] outcome = {\"O wins\" ,\"Tie\",\"X wins\"};// rely on ordering\n73System.out.println(outcome[1 + winningPlayer]);\n74}\n75}\nCode Fragment 3.10: A simple, complete Java class for playing Tic-Tac-Toe be-\ntween two players. (Continued from Code Fragment 3.9.)\nO|X|O\n-----\nO|X|X\n-----X|O|XTie\nFigure 3.9: Sample output of a Tic-Tac-Toe game.\nwww.it-ebooks.info"
  },
  {
    "page": 140,
    "content": "122 Chapter 3. Fundamental Data Structures\n3.2 Singly Linked Lists\nIn the previous section, we presented the array data structure and discussed some\nof its applications. Arrays are great for storing things in a certain order, but they\nhave drawbacks. The capacity of the array must be ﬁxed when it is created, and\ninsertions and deletions at interior positions of an array can be time consuming if\nmany elements must be shifted.\nIn this section, we introduce a data structure known as a linked list , which pro-\nvides an alternative to an array-based structure. A linked list, in its simplest form,\nis a collection of nodes that collectively form a linear sequence. In a singly linked\nlist, each node stores a reference to an object that is an element of the sequence, as\nwell as a reference to the next node of the list (see Figure 3.10).\nMSP\nelement next\nFigure 3.10: Example of a node instance that forms part of a singly linked list.\nThe node’s element ﬁeld refers to an object that is an element of the sequence (the\nairport code MSP , in this example), while the next ﬁeld refers to the subsequent\nnode of the linked list (or nullif there is no further node).\nA linked list’s representation relies on the collaboration of many objects (see\nFigure 3.11). Minimally, the linked list instance must keep a reference to the ﬁrst\nnode of the list, known as the head . Without an explicit reference to the head,\nthere would be no way to locate that node (or indirectly, any others). The last\nnode of the list is known as the tail. The tail of a list can be found by traversing the\nlinked list— starting at the head and moving from one node to another by following\neach node’s next reference. We can identify the tail as the node having null as its\nnext reference. This process is also known as link hopping orpointer hopping .\nHowever, storing an explicit reference to the tail node is a common efﬁciency to\navoid such a traversal. In similar regard, it is common for a linked list instance to\nkeep a count of the total number of nodes that comprise the list (also known as the\nsizeof the list), to avoid traversing the list to count the nodes.\nLAX MSP BOS ATL\nhead tail\nFigure 3.11: Example of a singly linked list whose elements are strings indicating\nairport codes. The list instance maintains a member named head that refers to the\nﬁrst node of the list, and another member named tailthat refers to the last node of\nthe list. The null value is denoted as Ø.\nwww.it-ebooks.info"
  },
  {
    "page": 141,
    "content": "3.2. Singly Linked Lists 123\nInserting an Element at the Head of a Singly Linked List\nAn important property of a linked list is that it does not have a predetermined ﬁxed\nsize; it uses space proportional to its current number of elements. When using a\nsingly linked list, we can easily insert an element at the head of the list, as shown\nin Figure 3.12, and described with pseudocode in Code Fragment 3.11. The mainidea is that we create a new node, set its element to the new element, set its next\nlink to refer to the current head , and set the list’s head to point to the new node.\nATL BOS MSPhead\n(a)\nBOSnewest\nMSP ATLhead\nLAX\n(b)\nLAX MSP ATL BOShead newest\n(c)\nFigure 3.12: Insertion of an element at the head of a singly linked list: (a) before the\ninsertion; (b) after a new node is created and linked to the existing head; (c) afterreassignment of the head reference to the newest node.\nAlgorithm addFirst( e):\nnewest=Node (e){create new node instance storing reference to element e}\nnewest.next=head{set new node’s next to reference the old head node }\nhead=newest {set variable head to reference the new node }\nsize=size+1 {increment the node count }\nCode Fragment 3.11: Inserting a new element at the beginning of a singly linked list.\nNote that we set the next pointer of the new node before we reassign variable head\nto it. If the list were initially empty (i.e., head isnull), then a natural consequence\nis that the new node has its next reference set to null.\nwww.it-ebooks.info"
  },
  {
    "page": 142,
    "content": "124 Chapter 3. Fundamental Data Structures\nInserting an Element at the Tail of a Singly Linked List\nWe can also easily insert an element at the tail of the list, provided we keep a\nreference to the tail node, as shown in Figure 3.13. In this case, we create a new\nnode, assign its next reference to null, set thenext reference of the tail to point to\nthis new node, and then update the tailreference itself to this new node. We give\npseudocode for the process in Code Fragment 3.12.\nATL BOS MSPtail\n(a)\nMIA ATL BOS MSPtail newest\n(b)\nMSP MIAtail newest\nATL BOS\n(c)\nFigure 3.13: Insertion at the tail of a singly linked list: (a) before the insertion;\n(b) after creation of a new node; (c) after reassignment of the tail reference. Notethat we must set the next link of the tailnode in (b) before we assign the tailvariable\nto point to the new node in (c).\nAlgorithm addLast( e):\nnewest=Node (e){create new node instance storing reference to element e}\nnewest.next=null{set new node’s next to reference the nullobject}\ntail.next=newest {make old tail node point to new node }\ntail=newest {set variable tailto reference the new node }\nsize=size+1 {increment the node count }\nCode Fragment 3.12: Inserting a new node at the end of a singly linked list. Note\nthat we set the next pointer for the old tail node before we make variable tailpoint\nto the new node. This code would need to be adjusted for inserting onto an empty\nlist, since there would not be an existing tail node.\nwww.it-ebooks.info"
  },
  {
    "page": 143,
    "content": "3.2. Singly Linked Lists 125\nRemoving an Element from a Singly Linked List\nRemoving an element from the head of a singly linked list is essentially the reverse\noperation of inserting a new element at the head. This operation is illustrated in\nFigure 3.14 and described in detail in Code Fragment 3.13.\nhead\nMSP ATL BOS LAX\n(a)\nBOShead\nMSP ATL LAX\n(b)\nATL BOS MSPhead\n(c)\nFigure 3.14: Removal of an element at the head of a singly linked list: (a) before\nthe removal; (b) after “linking out” the old head; (c) ﬁnal conﬁguration.\nAlgorithm removeFirst() :\nifhead ==nullthen\nthe list is empty.\nhead=head .next {makehead point to next node (or null) }\nsize=size−1 {decrement the node count }\nCode Fragment 3.13: Removing the node at the beginning of a singly linked list.\nUnfortunately, we cannot easily delete the last node of a singly linked list. Even\nif we maintain a tailreference directly to the last node of the list, we must be able\nto access the node before the last node in order to remove the last node. But we\ncannot reach the node before the tail by following next links from the tail. The only\nway to access this node is to start from the head of the list and search all the way\nthrough the list. But such a sequence of link-hopping operations could take a longtime. If we want to support such an operation efﬁciently, we will need to make ourlistdoubly linked (as we do in Section 3.4).\nwww.it-ebooks.info"
  },
  {
    "page": 144,
    "content": "126 Chapter 3. Fundamental Data Structures\n3.2.1 Implementing a Singly Linked List Class\nIn this section, we present a complete implementation of a SinglyLinkedList class,\nsupporting the following methods:\nsize() :Returns the number of elements in the list.\nisEmpty() :Returnstrue if the list is empty, and false otherwise.\nﬁrst() :Returns (but does not remove) the ﬁrst element in the list.\nlast() :Returns (but does not remove) the last element in the list.\naddFirst( e):Adds a new element to the front of the list.\naddLast( e):Adds a new element to the end of the list.\nremoveFirst() :Removes and returns the ﬁrst element of the list.\nIfﬁrst() ,last() , orremoveFirst() are called on a list that is empty, we will simply\nreturn anull reference and leave the list unchanged.\nBecause it does not matter to us what type of elements are stored in the list, we\nuse Java’s generics framework (see Section 2.5.2) to deﬁne our class with a formal\ntype parameter Ethat represents the user’s desired element type.\nOur implementation also takes advantage of Java’s support for nested classes\n(see Section 2.6), as we deﬁne a private Node class within the scope of the pub-\nlicSinglyLinkedList class. Code Fragment 3.14 presents the Node class deﬁnition,\nand Code Fragment 3.15 the rest of the SinglyLinkedList class. Having Node as a\nnested class provides strong encapsulation, shielding users of our class from the un-\nderlying details about nodes and links. This design also allows Java to differentiate\nthis node type from forms of nodes we may deﬁne for use in other structures.\n1public class SinglyLinkedList <E>{\n2//---------------- nested Node class ----------------\n3private static class Node<E>{\n4privateE element; // reference to the element stored at this node\n5privateNode<E>next; // reference to the subsequent node in the list\n6publicNode(E e, Node <E>n){\n7 element = e;\n8 next = n;\n9}\n10publicE getElement(){returnelement;}\n11publicNode<E>getNext(){returnnext;}\n12public void setNext(Node <E>n){next = n;}\n13}//----------- end of nested Node class -----------\n... rest of SinglyLinkedList class will follow ...\nCode Fragment 3.14: A nested Node class within the SinglyLinkedList class. (The\nremainder of the SinglyLinkedList class will be given in Code Fragment 3.15.)\nwww.it-ebooks.info"
  },
  {
    "page": 145,
    "content": "3.2. Singly Linked Lists 127\n1public class SinglyLinkedList <E>{\n...(nested Node class goes here)\n14// instance variables of the SinglyLinkedList\n15privateNode<E>head =null;// head node of the list (or null if empty)\n16privateNode<E>tail =null; // last node of the list (or null if empty)\n17private int size = 0; // number of nodes in the list\n18publicSinglyLinkedList() {} // constructs an initially empty list\n19// access methods\n20public int size(){returnsize;}\n21public boolean isEmpty(){returnsize == 0;}\n22publicE ﬁrst(){ // returns (but does not remove) the ﬁrst element\n23if(isEmpty()) return null;\n24returnhead.getElement();\n25}\n26publicE last(){ // returns (but does not remove) the last element\n27if(isEmpty()) return null;\n28returntail.getElement();\n29}\n30// update methods\n31public void addFirst(E e){ // adds element e to the front of the list\n32head =newNode<>(e, head); // create and link a new node\n33if(size == 0)\n34 tail = head; // special case: new node becomes tail also\n35size++;\n36}\n37public void addLast(E e){ // adds element e to the end of the list\n38Node<E>newest = newNode<>(e,null);// node will eventually be the tail\n39if(isEmpty())\n40 head = newest; // special case: previously empty list\n41else\n42 tail.setNext(newest); // new node after existing tail\n43tail = newest; // new node becomes the tail\n44size++;\n45}\n46publicE removeFirst(){ // removes and returns the ﬁrst element\n47if(isEmpty()) return null; // nothing to remove\n48E answer = head.getElement();\n49head = head.getNext(); // will become null if list had only one node\n50size−−;\n51if(size == 0)\n52 tail =null; // special case as list is now empty\n53returnanswer;\n54}\n55}\nCode Fragment 3.15: TheSinglyLinkedList class deﬁnition (when combined with\nthe nested Node class of Code Fragment 3.14).\nwww.it-ebooks.info"
  },
  {
    "page": 189,
    "content": "4.3. Asymptotic Analysis 171\n1/∗∗Returns the maximum value of a nonempty array of numbers. ∗/\n2public static double arrayMax( double[ ] data){\n3intn = data.length;\n4doublecurrentMax = data[0]; // assume ﬁrst entry is biggest (for now)\n5for(intj=1; j<n; j++) // consider all other entries\n6if(data[j]>currentMax) // if data[j] is biggest thus far...\n7 currentMax = data[j]; // record it as the current max\n8returncurrentMax;\n9}\nCode Fragment 4.3: A method that returns the maximum value of an array.\nUsing the big-Oh notation, we can write the following mathematically precise\nstatement on the running time of algorithm arrayMax foranycomputer.\nProposition 4.16: The algorithm, arrayMax , for computing the maximum ele-\nment of an array of nnumbers, runs in O(n)time.\nJustiﬁcation: The initialization at lines 3 and 4 and the return statement at line 8\nrequire only a constant number of primitive operations. Each iteration of the loop\nalso requires only a constant number of primitive operations, and the loop executes\nn−1 times. Therefore, we account for the number of primitive operations being\nc′·(n−1)+c′′for appropriate constants c′andc′′that reﬂect, respectively, the work\nperformed inside and outside the loop body. Because each primitive operation runs\nin constant time, we have that the running time of algorithm arrayMax on an input\nof size nis at most c′·(n−1)+c′′=c′·n+(c′′−c′)≤c′·nif we assume, without\nloss of generality, that c′′≤c′. We conclude that the running time of algorithm\narrayMax isO(n).\nFurther Analysis of the Maximum-Finding Algorithm\nA more interesting question about arrayMax is how many times we might update\nthe current “biggest” value. In the worst case, if the data is given to us in increasing\norder, the biggest value is reassigned n−1 times. But what if the input is given\nto us in random order, with all orders equally likely; what would be the expected\nnumber of times we update the biggest value in this case? To answer this question,\nnote that we update the current biggest in an iteration of the loop only if the current\nelement is bigger than all the elements that precede it. If the sequence is given to\nus in random order, the probability that the jthelement is the largest of the ﬁrst j\nelements is 1 /j(assuming uniqueness). Hence, the expected number of times we\nupdate the biggest (including initialization) is Hn=∑n\nj=11/j, which is known as\nthenthHarmonic number . It can be shown that HnisO(logn). Therefore, the\nexpected number of times the biggest value is updated by arrayMax on a randomly\nordered sequence is O(logn).\nwww.it-ebooks.info"
  },
  {
    "page": 190,
    "content": "172 Chapter 4. Algorithm Analysis\nComposing Long Strings\nAs our next example, we revisit the experimental study from Section 4.1, in which\nwe examined two different implementations for composing a long string (see CodeFragment 4.2). Our ﬁrst algorithm was based on repeated use of the string concate-\nnation operator; for convenience, that method is also given in Code Fragment 4.4.\n1/∗∗Uses repeated concatenation to compose a String with n copies of character c. ∗/\n2public static String repeat1( charc,intn){\n3String answer = \"\";\n4for(intj=0; j<n; j++)\n5answer += c;\n6returnanswer;\n7}\nCode Fragment 4.4: Composing a string using repeated concatenation.\nThe most important aspect of this implementation is that strings in Java are\nimmutable objects. Once created, an instance cannot be modiﬁed. The command,\nanswer += c , is shorthand for answer = (answer + c) . This command does not\ncause a new character to be added to the existing String instance; instead it produces\na newString with the desired sequence of characters, and then it reassigns the\nvariable,answer , to refer to that new string.\nIn terms of efﬁciency, the problem with this interpretation is that the creation\nof a new string as a result of a concatenation, requires time that is proportional\nto the length of the resulting string. The ﬁrst time through this loop, the result\nhas length 1, the second time through the loop the result has length 2, and so on,\nuntil we reach the ﬁnal string of length n. Therefore, the overall time taken by this\nalgorithm is proportional to\n1+2+···+n,\nwhich we recognize as the familiar O(n2)summation from Proposition 4.3. There-\nfore, the total time complexity of the repeat1 algorithm is O(n2).\nWe see this theoretical analysis reﬂected in the experimental results. The run-\nning time of a quadratic algorithm should theoretically quadruple if the size of theproblem doubles, as (2n)\n2=4·n2. (We say “theoretically,” because this does not\naccount for lower-order terms that are hidden by the asymptotic notation.) We seesuch an approximate fourfold increase in the running time of repeat1 in Table 4.1\non page 152.\nIn contrast, the running times in that table for the repeat2 algorithm, which uses\nJava’sStringBuilder class, demonstrate a trend of approximately doubling each\ntime the problem size doubles. The StringBuilder class relies on an advanced tech-\nnique with a worst-case running time of O(n)for composing a string of length n;\nwe will later explore that technique as the focus of Section 7.2.1.\nwww.it-ebooks.info"
  },
  {
    "page": 191,
    "content": "4.3. Asymptotic Analysis 173\nThree-Way Set Disjointness\nSuppose we are given three sets, A,B, and C, stored in three different integer arrays.\nWe will assume that no individual set contains duplicate values, but that there may\nbe some numbers that are in two or three of the sets. The three-way set disjointness\nproblem is to determine if the intersection of the three sets is empty, namely, thatthere is no element xsuch that x∈A,x∈B, and x∈C. A simple Java method to\ndetermine this property is given in Code Fragment 4.5.\n1/∗∗Returns true if there is no element common to all three arrays. ∗/\n2public static boolean disjoint1( int[ ] groupA, int[ ] groupB, int[ ] groupC){\n3for(inta : groupA)\n4for(intb : groupB)\n5 for(intc : groupC)\n6 if((a == b) && (b == c))\n7 return false ; // we found a common value\n8return true ; // if we reach this, sets are disjoint\n9}\nCode Fragment 4.5: Algorithm disjoint1 for testing three-way set disjointness.\nThis simple algorithm loops through each possible triple of values from the\nthree sets to see if those values are equivalent. If each of the original sets has sizen, then the worst-case running time of this method is O(n\n3).\nWe can improve upon the asymptotic performance with a simple observation.\nOnce inside the body of the loop over B, if selected elements aandbdo not match\neach other, it is a waste of time to iterate through all values of Clooking for a\nmatching triple. An improved solution to this problem, taking advantage of this\nobservation, is presented in Code Fragment 4.6.\n1/∗∗Returns true if there is no element common to all three arrays. ∗/\n2public static boolean disjoint2( int[ ] groupA, int[ ] groupB, int[ ] groupC){\n3for(inta : groupA)\n4for(intb : groupB)\n5 if(a == b) // only check C when we ﬁnd match from A and B\n6 for(intc : groupC)\n7 if(a == c) // and thus b == c as well\n8 return false ; // we found a common value\n9return true ; // if we reach this, sets are disjoint\n10}\nCode Fragment 4.6: Algorithm disjoint2 for testing three-way set disjointness.\nIn the improved version, it is not simply that we save time if we get lucky. We\nclaim that the worst-case running time for disjoint2 isO(n2). There are quadrat-\nically many pairs (a,b)to consider. However, if AandBare each sets of distinct\nwww.it-ebooks.info"
  },
  {
    "page": 192,
    "content": "174 Chapter 4. Algorithm Analysis\nelements, there can be at most O(n)such pairs with aequal to b. Therefore, the\ninnermost loop, over C, executes at most ntimes.\nTo account for the overall running time, we examine the time spent executing\neach line of code. The management of the forloop over Arequires O(n)time. The\nmanagement of the forloop over Baccounts for a total of O(n2)time, since that\nloop is executed ndifferent times. The test a == b is evaluated O(n2)times. The\nrest of the time spent depends upon how many matching (a,b)pairs exist. As we\nhave noted, there are at most nsuch pairs; therefore, the management of the loop\nover Cand the commands within the body of that loop use at most O(n2)time. By\nour standard application of Proposition 4.8, the total time spent is O(n2).\nElement Uniqueness\nA problem that is closely related to the three-way set disjointness problem is the\nelement uniqueness problem . In the former, we are given three sets and we pre-\nsumed that there were no duplicates within a single set. In the element uniquenessproblem, we are given an array with nelements and asked whether all elements of\nthat collection are distinct from each other.\nOur ﬁrst solution to this problem uses a straightforward iterative algorithm.\nTheunique1 method, given in Code Fragment 4.7, solves the element uniqueness\nproblem by looping through all distinct pairs of indices j<k, checking if any of\nthose pairs refer to elements that are equivalent to each other. It does this using twonestedforloops, such that the ﬁrst iteration of the outer loop causes n−1 iterations\nof the inner loop, the second iteration of the outer loop causes n−2 iterations of\nthe inner loop, and so on. Thus, the worst-case running time of this method isproportional to\n(n−1)+(n−2)+···+2+1,\nwhich we recognize as the familiar O(n\n2)summation from Proposition 4.3.\n1/∗∗Returns true if there are no duplicate elements in the array. ∗/\n2public static boolean unique1(int[ ] data){\n3intn = data.length;\n4for(intj=0; j<n−1; j++)\n5for(intk=j+1; k <n; k++)\n6 if(data[j] == data[k])\n7 return false ; // found duplicate pair\n8return true ; // if we reach this, elements are unique\n9}\nCode Fragment 4.7: Algorithm unique1 for testing element uniqueness.\nwww.it-ebooks.info"
  },
  {
    "page": 193,
    "content": "4.3. Asymptotic Analysis 175\nUsing Sorting as a Problem-Solving Tool\nAn even better algorithm for the element uniqueness problem is based on using\nsorting as a problem-solving tool. In this case, by sorting the array of elements, we\nare guaranteed that any duplicate elements will be placed next to each other. Thus,\nto determine if there are any duplicates, all we need to do is perform a single pass\nover the sorted array, looking for consecutive duplicates.\nA Java implementation of this algorithm is given in Code Fragment 4.8. (See\nSection 3.1.3 for discussion of the java.util.Arrays class.)\n1/∗∗Returns true if there are no duplicate elements in the array. ∗/\n2public static boolean unique2(int[ ] data){\n3intn = data.length;\n4int[ ] temp = Arrays.copyOf(data, n); // make copy of data\n5Arrays.sort(temp); // and sort the copy\n6for(intj=0; j<n−1; j++)\n7if(temp[j] == temp[j+1]) // check neighboring entries\n8 return false ; // found duplicate pair\n9return true ; // if we reach this, elements are unique\n10}\nCode Fragment 4.8: Algorithm unique2 for testing element uniqueness.\nSorting algorithms will be the focus of Chapter 12. The best sorting algorithms\n(including those used by Array.sort in Java) guarantee a worst-case running time of\nO(nlogn). Once the data is sorted, the subsequent loop runs in O(n)time, and so\nthe entire unique2 algorithm runs in O(nlogn)time. Exercise C-4.35 explores the\nuse of sorting to solve the three-way set disjointness problem in O(nlogn)time.\nPreﬁx Averages\nThe next problem we consider is computing what are known as preﬁx averages of\na sequence of numbers. Namely, given a sequence xconsisting of nnumbers, we\nwant to compute a sequence asuch that ajis the average of elements x0,..., xj, for\nj=0,..., n−1, that is,\naj=∑j\ni=0xi\nj+1.\nPreﬁx averages have many applications in economics and statistics. For example,\ngiven the year-by-year returns of a mutual fund, ordered from recent to past, an\ninvestor will typically want to see the fund’s average annual returns for the most\nrecent year, the most recent three years, the most recent ﬁve years, and so on. Like-\nwise, given a stream of daily Web usage logs, a website manager may wish to track\naverage usage trends over various time periods. We present two implementation\nfor computing preﬁx averages, yet with signiﬁcantly different running times.\nwww.it-ebooks.info"
  },
  {
    "page": 194,
    "content": "176 Chapter 4. Algorithm Analysis\nA Quadratic-Time Algorithm\nOur ﬁrst algorithm for computing preﬁx averages, denoted as preﬁxAverage1 , is\nshown in Code Fragment 4.9. It computes each element ajindependently, using an\ninner loop to compute that partial sum.\n1/∗∗Returns an array a such that, for all j, a[j] equals the average of x[0], ..., x[j]. ∗/\n2public static double [ ] preﬁxAverage1( double[ ] x){\n3intn = x.length;\n4double[ ] a =new double [n]; // ﬁlled with zeros by default\n5for(intj=0; j<n; j++){\n6doubletotal = 0; // begin computing x[0] + ... + x[j]\n7for(inti=0; i<= j; i++)\n8 total += x[i];\n9a[j] = total / (j+1); // record the average\n10}\n11returna;\n12}\nCode Fragment 4.9: Algorithm preﬁxAverage1 .\nLet us analyze the preﬁxAverage1 algorithm.\n•The initialization of n = x.length at line 3 and the eventual return of a refer-\nence to array aat line 11 both execute in O(1)time.\n•Creating and initializing the new array, a, at line 4 can be done with in O(n)\ntime, using a constant number of primitive operations per element.\n•There are two nested forloops, which are controlled, respectively, by coun-\nters jandi. The body of the outer loop, controlled by counter j, is ex-\necuted ntimes, for j=0,..., n−1. Therefore, statements total = 0 and\na[j] = total / (j+1) are executed ntimes each. This implies that these two\nstatements, plus the management of counter jin the loop, contribute a num-\nber of primitive operations proportional to n, that is, O(n)time.\n•The body of the inner loop, which is controlled by counter i, is executed j+1\ntimes, depending on the current value of the outer loop counter j. Thus, state-\nmenttotal += x[i] , in the inner loop, is executed 1 +2+3+···+ntimes.\nBy recalling Proposition 4.3, we know that 1 +2+3+···+n=n(n+1)/2,\nwhich implies that the statement in the inner loop contributes O(n2)time.\nA similar argument can be done for the primitive operations associated with\nmaintaining counter i, which also take O(n2)time.\nThe running time of implementation preﬁxAverage1 is given by the sum of these\nterms. The ﬁrst term is O(1), the second and third terms are O(n), and the fourth\nterm is O(n2). By a simple application of Proposition 4.8, the running time of\npreﬁxAverage1 isO(n2).\nwww.it-ebooks.info"
  },
  {
    "page": 195,
    "content": "4.3. Asymptotic Analysis 177\nA Linear-Time Algorithm\nAn intermediate value in the computation of the preﬁx average is the preﬁx sum\nx0+x1+···+xj, denoted as total in our ﬁrst implementation; this allows us to\ncompute the preﬁx average a[j] = total / (j + 1) . In our ﬁrst algorithm, the preﬁx\nsum is computed anew for each value of j. That contributed O(j)time for each j,\nleading to the quadratic behavior.\nFor greater efﬁciency, we can maintain the current preﬁx sum dynamically,\neffectively computing x0+x1+···+xjastotal +xj, where value total is equal to\nthe sum x0+x1+···+xj−1, when computed by the previous pass of the loop over j.\nCode Fragment 4.10 provides a new implementation, denoted as preﬁxAverage2 ,\nusing this approach.\n1/∗∗Returns an array a such that, for all j, a[j] equals the average of x[0], ..., x[j]. ∗/\n2public static double [ ] preﬁxAverage2( double[ ] x){\n3intn = x.length;\n4double[ ] a =new double [n]; // ﬁlled with zeros by default\n5doubletotal = 0; // compute preﬁx sum as x[0] + x[1] + ...\n6for(intj=0; j<n; j++){\n7total += x[j]; // update preﬁx sum to include x[j]\n8a[j] = total / (j+1); // compute average based on current sum\n9}\n10returna;\n11}\nCode Fragment 4.10: Algorithm preﬁxAverage2 .\nThe analysis of the running time of algorithm preﬁxAverage2 follows:\n•Initializing variables nandtotal uses O(1)time.\n•Initializing the array auses O(n)time.\n•There is a single forloop, which is controlled by counter j. The maintenance\nof that loop contributes a total of O(n)time.\n•The body of the loop is executed ntimes, for j=0,..., n−1. Thus, state-\nmentstotal += x[j] anda[j] = total / (j+1) are executed ntimes each.\nSince each of these statements uses O(1)time per iteration, their overall\ncontribution is O(n)time.\n•The eventual return of a reference to array Auses O(1)time.\nThe running time of algorithm preﬁxAverage2 is given by the sum of the ﬁve terms.\nThe ﬁrst and last are O(1)and the remaining three are O(n). By a simple applica-\ntion of Proposition 4.8, the running time of preﬁxAverage2 isO(n), which is much\nbetter than the quadratic time of algorithm preﬁxAverage1 .\nwww.it-ebooks.info"
  },
  {
    "page": 196,
    "content": "178 Chapter 4. Algorithm Analysis\n4.4 Simple Justiﬁcation Techniques\nSometimes, we will want to make claims about an algorithm, such as showing that\nit is correct or that it runs fast. In order to rigorously make such claims, we must\nuse mathematical language, and in order to back up such claims, we must justify or\nprove our statements. Fortunately, there are several simple ways to do this.\n4.4.1 By Example\nSome claims are of the generic form, “There is an element xin a set Sthat has\nproperty P.” To justify such a claim, we only need to produce a particular xinS\nthat has property P. Likewise, some hard-to-believe claims are of the generic form,\n“Every element xin a set Shas property P.” To justify that such a claim is false, we\nonly need to produce a particular xfrom Sthat does not have property P. Such an\ninstance is called a counterexample .\nExample 4.17: Professor Amongus claims that every number of the form 2i−1\nis a prime, when iis an integer greater than 1. Professor Amongus is wrong.\nJustiﬁcation: To prove Professor Amongus is wrong, we ﬁnd a counterexample.\nFortunately, we need not look too far, for 24−1=15=3·5.\n4.4.2 The “Contra” Attack\nAnother set of justiﬁcation techniques involves the use of the negative. The two\nprimary such methods are the use of the contrapositive and the contradiction . To\njustify the statement “if pis true, then qis true,” we establish that “if qis not true,\nthen pis not true” instead. Logically, these two statements are the same, but the\nlatter, which is called the contrapositive of the ﬁrst, may be easier to think about.\nExample 4.18: Letaandbbe integers. If abis even, then ais even or bis even.\nJustiﬁcation: To justify this claim, consider the contrapositive, “If ais odd and\nbis odd, then abis odd.” So, suppose a=2j+1 and b=2k+1, for some integers\njandk. Then ab=4jk+2j+2k+1=2(2jk+j+k)+1; hence, abis odd.\nBesides showing a use of the contrapositive justiﬁcation technique, the previous\nexample also contains an application of de Morgan’s law . This law helps us deal\nwith negations, for it states that the negation of a statement of the form “ porq” is\n“not pand not q.” Likewise, it states that the negation of a statement of the form\n“pandq” is “not por not q.”\nwww.it-ebooks.info"
  },
  {
    "page": 197,
    "content": "4.4. Simple Justiﬁcation Techniques 179\nContradiction\nAnother negative justiﬁcation technique is justiﬁcation by contradiction , which\nalso often involves using de Morgan’s law. In applying the justiﬁcation by con-\ntradiction technique, we establish that a statement qis true by ﬁrst supposing that\nqis false and then showing that this assumption leads to a contradiction (such as\n2/n⌉}ationslash=2 or 1>3). By reaching such a contradiction, we show that no consistent sit-\nuation exists with qbeing false, so qmust be true. Of course, in order to reach this\nconclusion, we must be sure our situation is consistent before we assume qis false.\nExample 4.19: Letaandbbe integers. If abis odd, then ais odd and bis odd.\nJustiﬁcation: Letabbe odd. We wish to show that ais odd and bis odd. So,\nwith the hope of leading to a contradiction, let us assume the opposite, namely,\nsuppose ais even or bis even. In fact, without loss of generality, we can assume\nthatais even (since the case for bis symmetric). Then a=2jfor some integer\nj. Hence, ab= (2j)b=2(jb), that is, abis even. But this is a contradiction: ab\ncannot simultaneously be odd and even. Therefore, ais odd and bis odd.\n4.4.3 Induction and Loop Invariants\nMost of the claims we make about a running time or a space bound involve an inte-\nger parameter n(usually denoting an intuitive notion of the “size” of the problem).\nMoreover, most of these claims are equivalent to saying some statement q(n)is true\n“for all n≥1.” Since this is making a claim about an inﬁnite set of numbers, we\ncannot justify this exhaustively in a direct fashion.\nInduction\nWe can often justify claims such as those above as true, however, by using the\ntechnique of induction . This technique amounts to showing that, for any particular\nn≥1, there is a ﬁnite sequence of implications that starts with something known\nto be true and ultimately leads to showing that q(n)is true. Speciﬁcally, we begin a\njustiﬁcation by induction by showing that q(n)is true for n=1 (and possibly some\nother values n=2,3,..., k, for some constant k). Then we justify that the inductive\n“step” is true for n>k, namely, we show “if q(j)is true for all j<n, then q(n)is\ntrue.” The combination of these two pieces completes the justiﬁcation by induction.\nwww.it-ebooks.info"
  },
  {
    "page": 198,
    "content": "180 Chapter 4. Algorithm Analysis\nProposition 4.20: Consider the Fibonacci function F(n), which is deﬁned such\nthatF(1) =1,F(2) =2, and F(n) =F(n−2)+F(n−1)forn>2. (See Sec-\ntion 2.2.3.) We claim that F(n)<2n.\nJustiﬁcation: We will show our claim is correct by induction.\nBase cases: (n≤2).F(1)=1<2=21andF(2)=2<4=22.\nInduction step: (n>2). Suppose our claim is true for all j<n. Since both n−2\nandn−1 are less than n, we can apply the inductive assumption (sometimes called\nthe “inductive hypothesis”) to imply that\nF(n)=F(n−2)+F(n−1)<2n−2+2n−1.\nSince\n2n−2+2n−1<2n−1+2n−1=2·2n−1=2n,\nwe have that F(n)<2n, thus showing the inductive hypothesis for n.\nLet us do another inductive argument, this time for a fact we have seen before.\nProposition 4.21: (which is the same as Proposition 4.3)\nn\n∑\ni=1i=n(n+1)\n2.\nJustiﬁcation: We will justify this equality by induction.\nBase case: n=1. Trivial, for 1 =n(n+1)/2, ifn=1.\nInduction step: n≥2. Assume the inductive hypothesis is true for any j<n.\nTherefore, for j=n−1, we have\nn−1\n∑\ni=1i=(n−1)(n−1+1)\n2=(n−1)n\n2.\nHence, we obtain\nn\n∑\ni=1i=n+n−1\n∑\ni=1i=n+(n−1)n\n2=2n+n2−n\n2=n2+n\n2=n(n+1)\n2,\nthereby proving the inductive hypothesis for n.\nWe may sometimes feel overwhelmed by the task of justifying something true\nforall n≥1. We should remember, however, the concreteness of the inductive tech-\nnique. It shows that, for any particular n, there is a ﬁnite step-by-step sequence of\nimplications that starts with something true and leads to the truth about n. In short,\nthe inductive argument is a template for building a sequence of direct justiﬁcations.\nwww.it-ebooks.info"
  },
  {
    "page": 199,
    "content": "4.4. Simple Justiﬁcation Techniques 181\nLoop Invariants\nThe ﬁnal justiﬁcation technique we discuss in this section is the loop invariant . To\nprove some statement Labout a loop is correct, deﬁne Lin terms of a series of\nsmaller statements L0,L1,...,Lk, where:\n1.Theinitial claim,L0, is true before the loop begins.\n2.IfLj−1is true before iteration j, thenLjwill be true after iteration j.\n3.The ﬁnal statement, Lk, implies the desired statement Lto be true.\nLet us give a simple example of using a loop-invariant argument to justify the\ncorrectness of an algorithm. In particular, we use a loop invariant to justify that\nthe method arrayFind (see Code Fragment 4.11) ﬁnds the smallest index at which\nelementvaloccurs in array A.\n1/∗∗Returns index j such that data[j] == val, or −1 if no such element. ∗/\n2public static int arrayFind( int[ ] data,intval){\n3intn = data.length;\n4intj = 0;\n5while(j<n){// val is not equal to any of the ﬁrst j elements of data\n6if(data[j] == val)\n7 returnj; // a match was found at index j\n8j++; // continue to next index\n9// val is not equal to any of the ﬁrst j elements of data\n10}\n11return−1; // if we reach this, no match found\n12}\nCode Fragment 4.11: Algorithm arrayFind for ﬁnding the ﬁrst index at which a\ngiven element occurs in an array.\nTo show that arrayFind is correct, we inductively deﬁne a series of statements,\nLj, that lead to the correctness of our algorithm. Speciﬁcally, we claim the follow-\ning is true at the beginning of iteration jof the while loop:\nLj:valis not equal to any of the ﬁrst jelements of data.\nThis claim is true at the beginning of the ﬁrst iteration of the loop, because jis\n0 and there are no elements among the ﬁrst 0 in data (this kind of a trivially true\nclaim is said to hold vacuously ). In iteration j, we compare element valto element\ndata[j]; if these two elements are equivalent, we return the index j, which is clearly\ncorrect since no earlier elements equal val. If the two elements valanddata[j]are\nnot equal, then we have found one more element not equal to valand we increment\nthe index j. Thus, the claim Ljwill be true for this new value of j; hence, it is\ntrue at the beginning of the next iteration. If the while loop terminates without ever\nreturning an index in data, then we have j=n. That is,Lnis true—there are no\nelements of data equal to val. Therefore, the algorithm correctly returns −1 to\nindicate that valis not indata.\nwww.it-ebooks.info"
  },
  {
    "page": 200,
    "content": "182 Chapter 4. Algorithm Analysis\n4.5 Exercises\nReinforcement\nR-4.1 Graph the functions 8 n, 4nlogn, 2n2,n3, and 2nusing a logarithmic scale for\nthex- and y-axes; that is, if the function value f(n)isy, plot this as a point with\nx-coordinate at log nandy-coordinate at log y.\nR-4.2 The number of operations executed by algorithms AandBis 8nlognand 2 n2,\nrespectively. Determine n0such that Ais better than Bforn≥n0.\nR-4.3 The number of operations executed by algorithms AandBis 40 n2and 2 n3, re-\nspectively. Determine n0such that Ais better than Bforn≥n0.\nR-4.4 Give an example of a function that is plotted the same on a log-log scale as it is\non a standard scale.\nR-4.5 Explain why the plot of the function ncis a straight line with slope con a log-log\nscale.\nR-4.6 What is the sum of all the even numbers from 0 to 2 n, for any integer n≥1?\nR-4.7 Show that the following two statements are equivalent:\n(a) The running time of algorithm Ais always O(f(n)).\n(b) In the worst case, the running time of algorithm AisO(f(n)).\nR-4.8 Order the following functions by asymptotic growth rate.\n4nlogn+2n2102logn\n3n+100log n4n 2n\nn2+10n n3nlogn\nR-4.9 Give a big-Oh characterization, in terms of n, of the running time of the example1\nmethod shown in Code Fragment 4.12.\nR-4.10 Give a big-Oh characterization, in terms of n, of the running time of the example2\nmethod shown in Code Fragment 4.12.\nR-4.11 Give a big-Oh characterization, in terms of n, of the running time of the example3\nmethod shown in Code Fragment 4.12.\nR-4.12 Give a big-Oh characterization, in terms of n, of the running time of the example4\nmethod shown in Code Fragment 4.12.\nR-4.13 Give a big-Oh characterization, in terms of n, of the running time of the example5\nmethod shown in Code Fragment 4.12.\nR-4.14 Show that if d(n)isO(f(n)), then ad(n)isO(f(n)), for any constant a>0.\nR-4.15 Show that if d(n)isO(f(n))ande(n)isO(g(n)), then the product d(n)e(n)is\nO(f(n)g(n)).\nR-4.16 Show that if d(n)isO(f(n))ande(n)isO(g(n)), then d(n)+e(n)isO(f(n)+\ng(n)).\nwww.it-ebooks.info"
  },
  {
    "page": 201,
    "content": "4.5. Exercises 183\n1/∗∗Returns the sum of the integers in given array. ∗/\n2public static int example1( int[ ] arr){\n3intn = arr.length, total = 0;\n4for(intj=0; j<n; j++) // loop from 0 to n-1\n5total += arr[j];\n6returntotal;\n7}\n8\n9/∗∗Returns the sum of the integers with even index in given array. ∗/\n10public static int example2( int[ ] arr){\n11intn = arr.length, total = 0;\n12for(intj=0; j<n; j += 2) // note the increment of 2\n13total += arr[j];\n14returntotal;\n15}\n1617/∗∗Returns the sum of the preﬁx sums of given array. ∗/\n18public static int example3( int[ ] arr){\n19intn = arr.length, total = 0;\n20for(intj=0; j<n; j++) // loop from 0 to n-1\n21for(intk=0; k<= j; k++) // loop from 0 to j\n22 total += arr[j];\n23returntotal;\n24}\n2526/∗∗Returns the sum of the preﬁx sums of given array. ∗/\n27public static int example4( int[ ] arr){\n28intn = arr.length, preﬁx = 0, total = 0;\n29for(intj=0; j<n; j++){ // loop from 0 to n-1\n30preﬁx += arr[j];\n31total += preﬁx;\n32}\n33returntotal;\n34}\n3536/∗∗Returns the number of times second array stores sum of preﬁx sums from ﬁrst. ∗/\n37public static int example5( int[ ] ﬁrst,int[ ] second){// assume equal-length arrays\n38intn = ﬁrst.length, count = 0;\n39for(inti=0; i<n; i++){ // loop from 0 to n-1\n40inttotal = 0;\n41for(intj=0; j<n; j++) // loop from 0 to n-1\n42 for(intk=0; k<= j; k++) // loop from 0 to j\n43 total += ﬁrst[k];\n44if(second[i] == total) count++;\n45}\n46returncount;\n47}\nCode Fragment 4.12:\nSome sample algorithms for analysis.\nwww.it-ebooks.info"
  },
  {
    "page": 202,
    "content": "184 Chapter 4. Algorithm Analysis\nR-4.17 Show that if d(n)isO(f(n))ande(n)isO(g(n)), then d(n)−e(n)isnot neces-\nsarily O(f(n)−g(n)).\nR-4.18 Show that if d(n)isO(f(n))andf(n)isO(g(n)), then d(n)isO(g(n)).\nR-4.19 Show that O(max{f(n),g(n)})=O(f(n)+g(n)).\nR-4.20 Show that f(n)isO(g(n))if and only if g(n)isΩ(f(n)).\nR-4.21 Show that if p(n)is a polynomial in n, then log p(n)isO(logn).\nR-4.22 Show that (n+1)5isO(n5).\nR-4.23 Show that 2n+1isO(2n).\nR-4.24 Show that nisO(nlogn).\nR-4.25 Show that n2isΩ(nlogn).\nR-4.26 Show that nlognisΩ(n).\nR-4.27 Show that⌈f(n)⌉isO(f(n)), iff(n)is a positive nondecreasing function that is\nalways greater than 1.\nR-4.28 For each function f(n)and time tin the following table, determine the largest\nsizenof a problem Pthat can be solved in time tif the algorithm for solving P\ntakes f(n)microseconds (one entry is already completed).\n1 Second 1 Hour 1 Month 1 Century\nlogn≈10300000\nn\nnlogn\nn2\n2n\nR-4.29 Algorithm Aexecutes an O(logn)-time computation for each entry of an array\nstoring nelements. What is its worst-case running time?\nR-4.30 Given an n-element array X, Algorithm B chooses log nelements in Xat random\nand executes an O(n)-time calculation for each. What is the worst-case running\ntime of Algorithm B?\nR-4.31 Given an n-element array Xof integers, Algorithm C executes an O(n)-time com-\nputation for each even number in X, and an O(logn)-time computation for each\nodd number in X. What are the best-case and worst-case running times of Algo-\nrithm C?\nR-4.32 Given an n-element array X, Algorithm D calls Algorithm E on each element\nX[i]. Algorithm E runs in O(i)time when it is called on element X[i]. What is\nthe worst-case running time of Algorithm D?\nwww.it-ebooks.info"
  },
  {
    "page": 203,
    "content": "4.5. Exercises 185\nR-4.33 Al and Bob are arguing about their algorithms. Al claims his O(nlogn)-time\nmethod is always faster than Bob’s O(n2)-time method. To settle the issue, they\nperform a set of experiments. To Al’s dismay, they ﬁnd that if n<100, the\nO(n2)-time algorithm runs faster, and only when n≥100 is the O(nlogn)-time\none better. Explain how this is possible.\nR-4.34 There is a well-known city (which will go nameless here) whose inhabitants have\nthe reputation of enjoying a meal only if that meal is the best they have ever\nexperienced in their life. Otherwise, they hate it. Assuming meal quality is\ndistributed uniformly across a person’s life, describe the expected number of\ntimes inhabitants of this city are happy with their meals?\nCreativity\nC-4.35 Assuming it is possible to sort nnumbers in O(nlogn)time, show that it is pos-\nsible to solve the three-way set disjointness problem in O(nlogn)time.\nC-4.36 Describe an efﬁcient algorithm for ﬁnding the ten largest elements in an array of\nsizen. What is the running time of your algorithm?\nC-4.37 Give an example of a positive function f(n)such that f(n)is neither O(n)nor\nΩ(n).\nC-4.38 Show that ∑n\ni=1i2isO(n3).\nC-4.39 Show that ∑n\ni=1i/2i<2.\nC-4.40 Determine the total number of grains of rice requested by the inventor of chess.\nC-4.41 Show that logbf(n)isΘ(logf(n))ifb>1 is a constant.\nC-4.42 Describe an algorithm for ﬁnding both the minimum and maximum of nnumbers\nusing fewer than 3 n/2 comparisons.\nC-4.43 Bob built a website and gave the URL only to his nfriends, which he numbered\nfrom 1 to n. He told friend number ithat he/she can visit the website at most\nitimes. Now Bob has a counter, C, keeping track of the total number of visits\nto the site (but not the identities of who visits). What is the minimum value for\nCsuch that Bob can know that one of his friends has visited his/her maximum\nallowed number of times?\nC-4.44 Draw a visual justiﬁcation of Proposition 4.3 analogous to that of Figure 4.3(b)\nfor the case when nis odd.\nC-4.45 An array Acontains n−1 unique integers in the range [0,n−1], that is, there is\none number from this range that is not in A. Design an O(n)-time algorithm for\nﬁnding that number. You are only allowed to use O(1)additional space besides\nthe array Aitself.\nC-4.46 Perform an asymptotic analysis of the insertion-sort algorithm given in Sec-\ntion 3.1.2. What are the worst-case and best-case running times?\nwww.it-ebooks.info"
  },
  {
    "page": 204,
    "content": "186 Chapter 4. Algorithm Analysis\nC-4.47 Communication security is extremely important in computer networks, and one\nway many network protocols achieve security is to encrypt messages. Typical\ncryptographic schemes for the secure transmission of messages over such net-\nworks are based on the fact that no efﬁcient algorithms are known for factoring\nlarge integers. Hence, if we can represent a secret message by a large prime\nnumber p, we can transmit, over the network, the number r=p·q, where q>p\nis another large prime number that acts as the encryption key. An eavesdropper\nwho obtains the transmitted number ron the network would have to factor rin\norder to ﬁgure out the secret message p.\nUsing factoring to ﬁgure out a message is hard without knowing the encryptionkeyq. To understand why, consider the following naive factoring algorithm:\nfor(intp=2; p<r; p++)\nif(r % p == 0)\nreturn\"The secret message is p!\" ;\na.Suppose the eavesdropper’s computer can divide two 100-bit integers inµs(1 millionth of a second). Estimate the worst-case time to decipher the\nsecret message pif the transmitted message rhas 100 bits.\nb.What is the worst-case time complexity of the above algorithm? Since the\ninput to the algorithm is just one large number r, assume that the input size\nnis the number of bytes needed to store r, that is, n=⌊(log\n2r)/8⌋+1, and\nthat each division takes time O(n).\nC-4.48 Al says he can prove that all sheep in a ﬂock are the same color:\nBase case: One sheep. It is clearly the same color as itself.\nInduction step: A ﬂock of nsheep. Take a sheep, a, out. The remaining n−1\nare all the same color by induction. Now put sheep aback in and take out a\ndifferent sheep, b. By induction, the n−1 sheep (now with a) are all the same\ncolor. Therefore, all the sheep in the ﬂock are the same color. What is wrong\nwith Al’s “justiﬁcation”?\nC-4.49 Consider the following “justiﬁcation” that the Fibonacci function, F(n)isO(n):\nBase case (n≤2):F(1)=1 and F(2)=2.\nInduction step (n>2): Assume claim true for n′<n. Consider n.F(n) =\nF(n−2)+F(n−1). By induction, F(n−2)isO(n−2)andF(n−1)isO(n−1).\nThen, F(n)isO((n−2)+(n−1)), by the identity presented in Exercise R-4.16.\nTherefore, F(n)isO(n).\nWhat is wrong with this “justiﬁcation”?\nC-4.50 Consider the Fibonacci function, F(n)(see Proposition 4.20). Show by induction\nthatF(n)isΩ((3/2)n).\nC-4.51 LetSbe a set of nlines in the plane such that no two are parallel and no three\nmeet in the same point. Show, by induction, that the lines in Sdetermine Θ(n2)\nintersection points.\nC-4.52 Show that the summation ∑n\ni=1logiisO(nlogn).\nC-4.53 Show that the summation ∑ni=1logiisΩ(nlogn).\nwww.it-ebooks.info"
  },
  {
    "page": 205,
    "content": "4.5. Exercises 187\nC-4.54 Letp(x)be a polynomial of degree n, that is, p(x)=∑n\ni=0aixi.\na.Describe a simple O(n2)-time algorithm for computing p(x).\nb.Describe an O(nlogn)-time algorithm for computing p(x), based upon a\nmore efﬁcient calculation of xi.\nc.Now consider a rewriting of p(x)as\np(x)=a0+x(a1+x(a2+x(a3+···+x(an−1+xan)···))),\nwhich is known as Horner’s method . Using the big-Oh notation, charac-\nterize the number of arithmetic operations this method executes.\nC-4.55 An evil king has nbottles of wine, and a spy has just poisoned one of them.\nUnfortunately, they do not know which one it is. The poison is very deadly; just\none drop diluted even a billion to one will still kill. Even so, it takes a full month\nfor the poison to take effect. Design a scheme for determining exactly which\none of the wine bottles was poisoned in just one month’s time while expending\nO(logn)taste testers.\nC-4.56 An array Acontains nintegers taken from the interval [0,4n], with repetitions\nallowed. Describe an efﬁcient algorithm for determining an integer value kthat\noccurs the most often in A. What is the running time of your algorithm?\nC-4.57 Given an array Aofnpositive integers, each represented with k=⌈logn⌉+1\nbits, describe an O(n)-time method for ﬁnding a k-bit integer not in A.\nC-4.58 Argue why any solution to the previous problem must run in Ω(n)time.\nC-4.59 Given an array Aofnarbitrary integers, design an O(n)-time method for ﬁnding\nan integer that cannot be formed as the sum of two integers in A.\nProjects\nP-4.60 Perform an experimental analysis of the two algorithms preﬁxAverage1 andpre-\nﬁxAverage2 , from Section 4.3.3. Visualize their running times as a function of\nthe input size with a log-log chart.\nP-4.61 Perform an experimental analysis that compares the relative running times of the\nmethods shown in Code Fragment 4.12.\nP-4.62 Perform an experimental analysis to test the hypothesis that Java’s Array.sort\nmethod runs in O(nlogn)time on average.\nP-4.63 For each of the algorithms unique1 andunique2 , which solve the element unique-\nness problem, perform an experimental analysis to determine the largest value of\nnsuch that the given algorithm runs in one minute or less.\nwww.it-ebooks.info"
  },
  {
    "page": 206,
    "content": "188 Chapter 4. Algorithm Analysis\nChapter Notes\nThe big-Oh notation has prompted several comments about its proper use [18, 43, 59].\nKnuth [60, 59] deﬁnes it using the notation f(n)=O(g(n)), but says this “equality” is only\n“one way.” We have chosen to take a more standard view of equality and view the big-Oh\nnotation as a set, following Brassard [18]. The reader interested in studying average-case\nanalysis is referred to the book chapter by Vitter and Flajolet [93].\nwww.it-ebooks.info"
  },
  {
    "page": 207,
    "content": "Chapter\n5Recursion\nContents\n5.1 Illustrative Examples . . . . . . . . . . . . . . . . . . . . . . 191\n5.1.1 The Factorial Function . . . . . . . . . . . . . . . . . . . 191\n5.1.2 Drawing an English Ruler . . . . . . . . . . . . . . . . . . 193\n5.1.3 Binary Search . . . . . . . . . . . . . . . . . . . . . . . . 196\n5.1.4 File Systems . . . . . . . . . . . . . . . . . . . . . . . . . 198\n5.2 Analyzing Recursive Algorithms . . . . . . . . . . . . . . . 202\n5.3 Further Examples of Recursion . . . . . . . . . . . . . . . . 206\n5.3.1 Linear Recursion . . . . . . . . . . . . . . . . . . . . . . . 206\n5.3.2 Binary Recursion . . . . . . . . . . . . . . . . . . . . . . 211\n5.3.3 Multiple Recursion . . . . . . . . . . . . . . . . . . . . . 212\n5.4 Designing Recursive Algorithms . . . . . . . . . . . . . . . 214\n5.5 Recursion Run Amok . . . . . . . . . . . . . . . . . . . . . 215\n5.5.1 Maximum Recursive Depth in Java . . . . . . . . . . . . . 218\n5.6 Eliminating Tail Recursion . . . . . . . . . . . . . . . . . . 219\n5.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\nwww.it-ebooks.info"
  },
  {
    "page": 208,
    "content": "190 Chapter 5. Recursion\nOne way to describe repetition within a computer program is the use of loops,\nsuch as Java’s while -loop and for-loop constructs described in Section 1.5.2. An\nentirely different way to achieve repetition is through a process known as recursion .\nRecursion is a technique by which a method makes one or more calls to itself\nduring execution, or by which a data structure relies upon smaller instances of\nthe very same type of structure in its representation. There are many examples ofrecursion in art and nature. For example, fractal patterns are naturally recursive. Aphysical example of recursion used in art is in the Russian Matryoshka dolls. Each\ndoll is either made of solid wood, or is hollow and contains another Matryoshka\ndoll inside it.\nIn computing, recursion provides an elegant and powerful alternative for per-\nforming repetitive tasks. In fact, a few programming languages (e.g., Scheme,Smalltalk) do not explicitly support looping constructs and instead rely directlyon recursion to express repetition. Most modern programming languages supportfunctional recursion using the identical mechanism that is used to support tradi-\ntional forms of method calls. When one invocation of the method makes a recursive\ncall, that invocation is suspended until the recursive call completes.\nRecursion is an important technique in the study of data structures and algo-\nrithms. We will use it prominently in several later chapters of this book (mostnotably, Chapters 8 and 12). In this chapter, we begin with the following four illus-trative examples of the use of recursion, providing a Java implementation for each.\n•Thefactorial function (commonly denoted as n!) is a classic mathematical\nfunction that has a natural recursive deﬁnition.\n•AnEnglish ruler has a recursive pattern that is a simple example of a fractal\nstructure.\n•Binary search is among the most important computer algorithms. It allows\nus to efﬁciently locate a desired value in a data set with upwards of billionsof entries.\n•Theﬁle system for a computer has a recursive structure in which directories\ncan be nested arbitrarily deeply within other directories. Recursive algo-\nrithms are widely used to explore and manage these ﬁle systems.\nWe then describe how to perform a formal analysis of the running time of a\nrecursive algorithm, and we discuss some potential pitfalls when deﬁning recur-\nsions. In the balance of the chapter, we provide many more examples of recursive\nalgorithms, organized to highlight some common forms of design.\nwww.it-ebooks.info"
  },
  {
    "page": 209,
    "content": "5.1. Illustrative Examples 191\n5.1 Illustrative Examples\n5.1.1 The Factorial Function\nTo demonstrate the mechanics of recursion, we begin with a simple mathematical\nexample of computing the value of the factorial function . The factorial of a posi-\ntive integer n, denoted n!, is deﬁned as the product of the integers from 1 to n. If\nn=0, then n! is deﬁned as 1 by convention. More formally, for any integer n≥0,\nn!=/braceleftbigg1 ifn=0\nn·(n−1)·(n−2)···3·2·1 if n≥1.\nFor example, 5! =5·4·3·2·1=120. The factorial function is important because\nit is known to equal the number of ways in which ndistinct items can be arranged\ninto a sequence, that is, the number of permutations ofnitems. For example, the\nthree characters a,b, andccan be arranged in 3! =3·2·1=6 ways:abc,acb,\nbac,bca,cab, andcba.\nThere is a natural recursive deﬁnition for the factorial function. To see this,\nobserve that 5! =5·(4·3·2·1) =5·4!. More generally, for a positive integer n,\nwe can deﬁne n! to be n·(n−1)!. This recursive deﬁnition can be formalized as\nn!=/braceleftbigg1 ifn=0\nn·(n−1)! ifn≥1.\nThis deﬁnition is typical of many recursive deﬁnitions of functions. First, we\nhave one or more base cases , which refer to ﬁxed values of the function. The above\ndeﬁnition has one base case stating that n!=1 for n=0. Second, we have one\nor more recursive cases , which deﬁne the function in terms of itself. In the above\ndeﬁnition, there is one recursive case, which indicates that n!=n·(n−1)! for n≥1.\nA Recursive Implementation of the Factorial Function\nRecursion is not just a mathematical notation; we can use recursion to design a Java\nimplementation of the factorial function, as shown in Code Fragment 5.1.\n1public static int factorial(intn)throwsIllegalArgumentException {\n2if(n<0)\n3throw new IllegalArgumentException(); // argument must be nonnegative\n4else if(n == 0)\n5return1; // base case\n6else\n7returnn∗factorial(n−1); // recursive case\n8}\nCode Fragment 5.1: A recursive implementation of the factorial function.\nwww.it-ebooks.info"
  },
  {
    "page": 210,
    "content": "192 Chapter 5. Recursion\nThis method does not use any explicit loops. Repetition is achieved through\nrepeated recursive invocations of the method. The process is ﬁnite because each\ntime the method is invoked, its argument is smaller by one, and when a base caseis reached, no further recursive calls are made.\nWe illustrate the execution of a recursive method using a recursion trace . Each\nentry of the trace corresponds to a recursive call. Each new recursive method callis indicated by a downward arrow to a new invocation. When the method returns,an arrow showing this return is drawn and the return value may be indicated along-side this arrow. An example of such a trace for the factorial function is shown in\nFigure 5.1.\nreturn4∗6 = 24\nfactorial(1)\nfactorial(0)factorial(3)\nfactorial(2)factorial(5)\nfactorial(4)\nreturn1return1∗1 = 1return2∗1 = 2return3∗2 = 6return5∗24 = 120\nFigure 5.1: A recursion trace for the call factorial(5) .\nA recursion trace closely mirrors a programming language’s execution of the\nrecursion. In Java, each time a method (recursive or otherwise) is called, a structure\nknown as an activation record oractivation frame is created to store information\nabout the progress of that invocation of the method. This frame stores the parame-\nters and local variables speciﬁc to a given call of the method, and information about\nwhich command in the body of the method is currently executing.\nWhen the execution of a method leads to a nested method call, the execution\nof the former call is suspended and its frame stores the place in the source code atwhich the ﬂow of control should continue upon return of the nested call. A newframe is then created for the nested method call. This process is used both in thestandard case of one method calling a different method, or in the recursive casewhere a method invokes itself. The key point is to have a separate frame for each\nactive call.\nwww.it-ebooks.info"
  },
  {
    "page": 211,
    "content": "5.1. Illustrative Examples 193\n5.1.2 Drawing an English Ruler\nIn the case of computing the factorial function, there is no compelling reason for\npreferring recursion over a direct iteration with a loop. As a more complex example\nof the use of recursion, consider how to draw the markings of a typical English\nruler. For each inch, we place a tick with a numeric label. We denote the length\nof the tick designating a whole inch as the major tick length . Between the marks\nfor whole inches, the ruler contains a series of minor ticks , placed at intervals of\n1/2 inch, 1/4 inch, and so on. As the size of the interval decreases by half, the tick\nlength decreases by one. Figure 5.2 demonstrates several such rulers with varying\nmajor tick lengths (although not drawn to scale).\n---- 0 ----- 0 --- 0\n- - -\n-- -- --\n- - -\n--- --- --- 1\n- - -\n-- -- --\n- - -\n---- 1 ---- --- 2\n- - -\n-- -- --\n- - -\n--- --- --- 3\n- -\n-- --\n- -\n---- 2 ----- 1\n(a) (b) (c)\nFigure 5.2: Three sample outputs of an English ruler drawing: (a) a 2-inch ruler\nwith major tick length 4; (b) a 1-inch ruler with major tick length 5; (c) a 3-inch\nruler with major tick length 3.\nA Recursive Approach to Ruler Drawing\nThe English ruler pattern is a simple example of a fractal , that is, a shape that has\na self-recursive structure at various levels of magniﬁcation. Consider the rule with\nmajor tick length 5 shown in Figure 5.2(b). Ignoring the lines containing 0 and 1,\nlet us consider how to draw the sequence of ticks lying between these lines. The\ncentral tick (at 1/2 inch) has length 4. Observe that the two patterns of ticks above\nand below this central tick are identical, and each has a central tick of length 3.\nwww.it-ebooks.info"
  },
  {
    "page": 212,
    "content": "194 Chapter 5. Recursion\nIn general, an interval with a central tick length L≥1 is composed of:\n•An interval with a central tick length L−1\n•A single tick of length L\n•An interval with a central tick length L−1\nAlthough it is possible to draw such a ruler using an iterative process (see Ex-\nercise P-5.29), the task is considerably easier to accomplish with recursion. Our\nimplementation consists of three methods, as shown in Code Fragment 5.2.\nThe main method, drawRuler, manages the construction of the entire ruler. Its\narguments specify the total number of inches in the ruler and the major tick length.\nThe utility method, drawLine , draws a single tick with a speciﬁed number of dashes\n(and an optional integer label that is printed to the right of the tick).\nThe interesting work is done by the recursive drawInterval method. This method\ndraws the sequence of minor ticks within some interval, based upon the length ofthe interval’s central tick. We rely on the intuition shown at the top of this page,and with a base case when L=0 that draws nothing. For L≥1, the ﬁrst and last\nsteps are performed by recursively calling drawInterval( L−1). The middle step is\nperformed by calling method drawLine( L).\n1/∗∗Draws an English ruler for the given number of inches and major tick length. ∗/\n2public static void drawRuler( intnInches,intmajorLength){\n3drawLine(majorLength, 0); // draw inch 0 line and label\n4for(intj = 1; j<= nInches; j++){\n5drawInterval(majorLength −1); // draw interior ticks for inch\n6drawLine(majorLength, j); // draw inch j line and label\n7}\n8}\n9private static void drawInterval( intcentralLength){\n10if(centralLength >= 1){ // otherwise, do nothing\n11drawInterval(centralLength −1); // recursively draw top interval\n12drawLine(centralLength); // draw center tick line (without label)\n13drawInterval(centralLength −1); // recursively draw bottom interval\n14}\n15}\n16private static void drawLine( inttickLength, inttickLabel){\n17for(intj = 0; j<tickLength; j++)\n18System.out.print( \"-\");\n19if(tickLabel >= 0)\n20System.out.print( \" \"+ tickLabel);\n21System.out.print( \"\\n\");\n22}\n23/∗∗Draws a line with the given tick length (but no label). ∗/\n24private static void drawLine( inttickLength){\n25drawLine(tickLength, −1);\n26}\nCode Fragment 5.2: A recursive implementation of a method that draws a ruler.\nwww.it-ebooks.info"
  },
  {
    "page": 213,
    "content": "5.1. Illustrative Examples 195\nIllustrating Ruler Drawing Using a Recursion Trace\nThe execution of the recursive drawInterval method can be visualized using a re-\ncursion trace. The trace for drawInterval is more complicated than in the factorial\nexample, however, because each instance makes two recursive calls. To illustrate\nthis, we will show the recursion trace in a form that is reminiscent of an outline for\na document. See Figure 5.3.\n(previous pattern repeats)drawInterval(3)\ndrawInterval(2)\ndrawInterval(1)\ndrawInterval(1)drawInterval(0)\ndrawLine(1)\ndrawInterval(0)\ndrawInterval(0)\ndrawLine(1)\ndrawInterval(0)\ndrawLine(3)\ndrawInterval(2)drawLine(2)Output\nFigure 5.3: A partial recursion trace for the call drawInterval(3) . The second pattern\nof calls for drawInterval(2) is not shown, but it is identical to the ﬁrst.\nwww.it-ebooks.info"
  },
  {
    "page": 214,
    "content": "196 Chapter 5. Recursion\n5.1.3 Binary Search\nIn this section, we describe a classic recursive algorithm, binary search , used to\nefﬁciently locate a target value within a sorted sequence of nelements stored in\nan array. This is among the most important of computer algorithms, and it is the\nreason that we so often store data in sorted order (as in Figure 5.4).\n375 0 1 2 3 4 6 7 8 9 10 11 12 13 14 15\n9 2 4 5 7 8 12 14 17 19 22 25 27 28 33\nFigure 5.4: Values stored in sorted order within an array. The numbers at top are\nthe indices.\nWhen the sequence is unsorted , the standard approach to search for a target\nvalue is to use a loop to examine every element, until either ﬁnding the target or\nexhausting the data set. This algorithm is known as linear search , orsequential\nsearch , and it runs in O(n)time (i.e., linear time) since every element is inspected\nin the worst case.\nWhen the sequence is sorted andindexable , there is a more efﬁcient algorithm.\n(For intuition, think about how you would accomplish this task by hand!) If we\nconsider an arbitrary element of the sequence with value v, we can be sure that all\nelements prior to that in the sequence have values less than or equal to v, and that all\nelements after that element in the sequence have values greater than or equal to v.\nThis observation allows us to quickly “home in” on a search target using a variant\nof the children’s game “high-low.” We call an element of the sequence a candidate\nif, at the current stage of the search, we cannot rule out that this item matches the\ntarget. The algorithm maintains two parameters, lowandhigh, such that all the\ncandidate elements have index at least lowand at most high. Initially, low=0 and\nhigh=n−1. We then compare the target value to the median candidate , that is,\nthe element with index\nmid=⌊(low+high)/2⌋.\nWe consider three cases:\n•If the target equals the median candidate, then we have found the item we are\nlooking for, and the search terminates successfully.\n•If the target is less than the median candidate, then we recur on the ﬁrst half\nof the sequence, that is, on the interval of indices from lowtomid−1.\n•If the target is greater than the median candidate, then we recur on the second\nhalf of the sequence, that is, on the interval of indices from mid+1 tohigh.\nAn unsuccessful search occurs if low>high, as the interval [low,high]is empty.\nwww.it-ebooks.info"
  },
  {
    "page": 215,
    "content": "5.1. Illustrative Examples 197\nThis algorithm is known as binary search . We give a Java implementation in\nCode Fragment 5.3, and an illustration of the execution of the algorithm in Fig-\nure 5.5. Whereas sequential search runs in O(n)time, the more efﬁcient binary\nsearch runs in O(logn)time. This is a signiﬁcant improvement, given that if nis\n1 billion, log nis only 30. (We defer our formal analysis of binary search’s running\ntime to Proposition 5.2 in Section 5.2.)\n1/∗∗\n2∗Returns true if the target value is found in the indicated portion of the data array.\n3∗This search only considers the array portion from data[low] to data[high] inclusive.\n4∗/\n5public static boolean binarySearch( int[ ] data,inttarget,intlow,inthigh){\n6if(low>high)\n7return false ; // interval empty; no match\n8else{\n9intmid = (low + high) / 2;\n10if(target == data[mid])\n11 return true ; // found a match\n12else if(target<data[mid])\n13 returnbinarySearch(data, target, low, mid −1);// recur left of the middle\n14else\n15 returnbinarySearch(data, target, mid + 1, high); // recur right of the middle\n16}\n17}\nCode Fragment 5.3: An implementation of the binary search algorithm on a sorted\narray.\nmidhigh\nhigh lowlow midlow mid\nlow=mid=highhigh\n14 19 22 25 27 28 33 376 7 8 9 10 11 12 13 14 15\n7 5 4 2 9 8\n9 2 4 5 7 8 12 14 1737 33 28 27 25 22 19\n9 2 4 5 7 8 12 14 17 19 22 25 27 28 33 37\n19 22 25 27 28 33 375 0 1 2 3 4\n17 14 12\n9 2 4 5 7 8 12 17\nFigure 5.5: Example of a binary search for target value 22 on a sorted array with 16\nelements.\nwww.it-ebooks.info"
  },
  {
    "page": 216,
    "content": "198 Chapter 5. Recursion\n5.1.4 File Systems\nModern operating systems deﬁne ﬁle-system directories (also called “folders”) in\na recursive way. Namely, a ﬁle system consists of a top-level directory, and the\ncontents of this directory consists of ﬁles and other directories, which in turn can\ncontain ﬁles and other directories, and so on. The operating system allows directo-\nries to be nested arbitrarily deeply (as long as there is enough memory), although\nby necessity there must be some base directories that contain only ﬁles, not fur-\nther subdirectories. A representation of a portion of such a ﬁle system is given in\nFigure 5.6.\n/user/rt/courses/\ncs016/ cs252/\nprograms/ homeworks/ projects/\npapers/ demos/hw1 hw2 hw3 pr1 pr2 pr3grades\nmarket buylow sellhighgrades\nFigure 5.6: A portion of a ﬁle system demonstrating a nested organization.\nGiven the recursive nature of the ﬁle-system representation, it should not come\nas a surprise that many common behaviors of an operating system, such as copying\na directory or deleting a directory, are implemented with recursive algorithms. In\nthis section, we consider one such algorithm: computing the total disk usage for all\nﬁles and directories nested within a particular directory.\nFor illustration, Figure 5.7 portrays the disk space being used by all entries in\nour sample ﬁle system. We differentiate between the immediate disk space used by\neach entry and the cumulative disk space used by that entry and all nested features.\nFor example, the cs016 directory uses only 2K of immediate space, but a total of\n249K of cumulative space.\nwww.it-ebooks.info"
  },
  {
    "page": 217,
    "content": "5.1. Illustrative Examples 199\n/user/rt/courses/\ncs016/ cs252/\nprograms/ homeworks/ projects/\npapers/ demos/ hw1\n3Khw2\n2Khw3\n4Kpr1\n57Kpr2\n97Kpr3\n74Kgrades\n8K\nmarket\n4786Kbuylow\n26Ksellhigh\n55Kgrades\n3K2K 1K1K\n1K 1K 1K\n1K 1K10K 229K 4870K\n82K 4787K5124K\n249K 4874K\nFigure 5.7: The same portion of a ﬁle system given in Figure 5.6, but with additional\nannotations to describe the amount of disk space that is used. Within the icon for\neach ﬁle or directory is the amount of space directly used by that artifact. Above\nthe icon for each directory is an indication of the cumulative disk space used by\nthat directory and all its (recursive) contents.\nThe cumulative disk space for an entry can be computed with a simple recursive\nalgorithm. It is equal to the immediate disk space used by the entry plus the sum\nof the cumulative disk space usage of any entries that are stored directly within\nthe entry. For example, the cumulative disk space for cs016 is 249K because it\nuses 2K itself, 8K cumulatively in grades , 10K cumulatively in homeworks , and\n229K cumulatively in programs . Pseudocode for this algorithm is given in Code\nFragment 5.4.\nAlgorithm DiskUsage( path):\nInput: A string designating a path to a ﬁle-system entry\nOutput: The cumulative disk space used by that entry and any nested entries\ntotal=size(path) {immediate disk space used by the entry }\nifpath represents a directory then\nforeach child entry stored within directory path do\ntotal=total+DiskUsage (child ) {recursive call}\nreturn total\nCode Fragment 5.4: An algorithm for computing the cumulative disk space usage\nnested at a ﬁle-system entry. We presume that method sizereturns the immediate\ndisk space of an entry.\nwww.it-ebooks.info"
  },
  {
    "page": 218,
    "content": "200 Chapter 5. Recursion\nThe java.io.File Class\nTo implement a recursive algorithm for computing disk usage in Java, we rely on\nthejava.io.File class. An instance of this class represents an abstract pathname in\nthe operating system and allows for properties of that operating system entry to be\nqueried. We will rely on the following methods of the class:\n•new File(pathString) ornew File(parentFile, childString)\nA newFileinstance can be constructed either by providing the full path as\na string, or by providing an existing Fileinstance that represents a directory\nand a string that designates the name of a child entry within that directory.\n•ﬁle.length()Returns the immediate disk usage (measured in bytes) for the operating sys-tem entry represented by the Fileinstance (e.g., /user/rt/courses ).\n•ﬁle.isDirectory()Returnstrue if theFileinstance represents a directory; false otherwise.\n•ﬁle.list()Returns an array of strings designating the names of all entries within the\ngiven directory. In our sample ﬁle system, if we call this method on the\nFileassociated with path /user/rt/courses/cs016 , it returns an array with\ncontents:{\"grades\" ,\"homeworks\" ,\"programs\"}.\nJava Implementation\nWith use of the Fileclass, we now convert the algorithm from Code Fragment 5.4\ninto the Java implementation of Code Fragment 5.5.\n1/∗∗\n2∗Calculates the total disk usage (in bytes) of the portion of the ﬁle system rooted\n3∗at the given path, while printing a summary akin to the standard 'du'Unix tool.\n4∗/\n5public static long diskUsage(File root) {\n6longtotal = root.length(); // start with direct disk usage\n7if(root.isDirectory()) { // and if this is a directory,\n8for(String childname : root.list()) { // then for each child\n9 File child = newFile(root, childname); // compose full path to child\n10 total += diskUsage(child); // add child’s usage to total\n11}\n12}\n13System.out.println(total + \"\\t\"+ root); // descriptive output\n14returntotal; // return the grand total\n15}\nCode Fragment 5.5: A recursive method for reporting disk usage of a ﬁle system.\nwww.it-ebooks.info"
  },
  {
    "page": 219,
    "content": "5.1. Illustrative Examples 201\nRecursion Trace\nTo produce a different form of a recursion trace, we have included an extraneous\nprint statement within our Java implementation (line 13 of Code Fragment 5.5).\nThe precise format of that output intentionally mirrors the output that is produced\nby a classic Unix/Linux utility named du(for “disk usage”). It reports the amount\nof disk space used by a directory and all contents nested within, and can produce averbose report, as given in Figure 5.8.\nWhen executed on the sample ﬁle system portrayed in Figure 5.7, our imple-\nmentation of the diskUsage method produces the result given in Figure 5.8. During\nthe execution of the algorithm, exactly one recursive call is made for each entry inthe portion of the ﬁle system that is considered. Because each line is printed justbefore returning from a recursive call, the lines of output reﬂect the order in which\nthe recursive calls are completed . Notice that it computes and reports the cumula-\ntive disk space for a nested entry before computing and reporting the cumulative\ndisk space for the directory that contains it. For example, the recursive calls regard-ing entries grades ,homeworks , andprograms are computed before the cumulative\ntotal for the directory /user/rt/courses/cs016 that contains them.\n8 /user/rt/courses/cs016/grades3 /user/rt/courses/cs016/homeworks/hw12 /user/rt/courses/cs016/homeworks/hw24 /user/rt/courses/cs016/homeworks/hw3\n10 /user/rt/courses/cs016/homeworks\n57 /user/rt/courses/cs016/programs/pr197 /user/rt/courses/cs016/programs/pr274 /user/rt/courses/cs016/programs/pr3229 /user/rt/courses/cs016/programs\n249 /user/rt/courses/cs016\n26 /user/rt/courses/cs252/projects/papers/buylow55 /user/rt/courses/cs252/projects/papers/sellhigh82 /user/rt/courses/cs252/projects/papers4786 /user/rt/courses/cs252/projects/demos/market\n4787 /user/rt/courses/cs252/projects/demos\n4870 /user/rt/courses/cs252/projects3 /user/rt/courses/cs252/grades4874 /user/rt/courses/cs252\n5124 /user/rt/courses/\nFigure 5.8: A report of the disk usage for the ﬁle system shown in Figure 5.7, as\ngenerated by our diskUsage method from Code Fragment 5.5, or equivalently by\nthe Unix/Linux command duwith option -a(which lists both directories and ﬁles).\nwww.it-ebooks.info"
  },
  {
    "page": 220,
    "content": "202 Chapter 5. Recursion\n5.2 Analyzing Recursive Algorithms\nIn Chapter 4, we introduced mathematical techniques for analyzing the efﬁciency\nof an algorithm, based upon an estimate of the number of primitive operations that\nare executed by the algorithm. We use notations such as big-Oh to summarize the\nrelationship between the number of operations and the input size for a problem. In\nthis section, we demonstrate how to perform this type of running-time analysis to\nrecursive algorithms.\nWith a recursive algorithm, we will account for each operation that is performed\nbased upon the particular activation of the method that manages the ﬂow of control\nat the time it is executed. Stated another way, for each invocation of the method,\nwe only account for the number of operations that are performed within the body of\nthat activation. We can then account for the overall number of operations that are\nexecuted as part of the recursive algorithm by taking the sum, over all activations,\nof the number of operations that take place during each individual activation. (As\nan aside, this is also the way we analyze a nonrecursive method that calls other\nmethods from within its body.)\nTo demonstrate this style of analysis, we revisit the four recursive algorithms\npresented in Sections 5.1.1 through 5.1.4: factorial computation, drawing an En-\nglish ruler, binary search, and computation of the cumulative size of a ﬁle system.\nIn general, we may rely on the intuition afforded by a recursion trace in recogniz-\ning how many recursive activations occur, and how the parameterization of each\nactivation can be used to estimate the number of primitive operations that occur\nwithin the body of that activation. However, each of these recursive algorithms has\na unique structure and form.\nComputing Factorials\nIt is relatively easy to analyze the efﬁciency of our method for computing factorials,\nas described in Section 5.1.1. A sample recursion trace for our factorial method was\ngiven in Figure 5.1. To compute factorial(n) , we see that there are a total of n+1\nactivations, as the parameter decreases from nin the ﬁrst call, to n−1 in the second\ncall, and so on, until reaching the base case with parameter 0.\nIt is also clear, given an examination of the method body in Code Fragment 5.1,\nthat each individual activation of factorial executes a constant number of opera-\ntions. Therefore, we conclude that the overall number of operations for computing\nfactorial(n) isO(n), as there are n+1 activations, each of which accounts for O(1)\noperations.\nwww.it-ebooks.info"
  },
  {
    "page": 221,
    "content": "5.2. Analyzing Recursive Algorithms 203\nDrawing an English Ruler\nIn analyzing the English ruler application from Section 5.1.2, we consider the fun-\ndamental question of how many total lines of output are generated by an initial call\ntodrawInterval( c), where cdenotes the center length. This is a reasonable bench-\nmark for the overall efﬁciency of the algorithm as each line of output is based upon\na call to the drawLine utility, and each recursive call to drawInterval with nonzero\nparameter makes exactly one direct call to drawLine .\nSome intuition may be gained by examining the source code and the recur-\nsion trace. We know that a call to drawInterval( c)forc>0 spawns two calls to\ndrawInterval( c−1)and a single call to drawLine . We will rely on this intuition to\nprove the following claim.\nProposition 5.1: Forc≥0, a call to drawInterval( c)results in precisely 2c−1\nlines of output.\nJustiﬁcation: We provide a formal proof of this claim by induction (see Sec-\ntion 4.4.3). In fact, induction is a natural mathematical technique for proving the\ncorrectness and efﬁciency of a recursive process. In the case of the ruler, we note\nthat an application of drawInterval( 0)generates no output, and that 20−1=1−1=\n0. This serves as a base case for our claim.\nMore generally, the number of lines printed by drawInterval( c)is one more\nthan twice the number generated by a call to drawInterval( c−1), as one center line\nis printed between two such recursive calls. By induction, we have that the number\nof lines is thus 1 +2·(2c−1−1)=1+2c−2=2c−1.\nThis proof is indicative of a more mathematically rigorous tool, known as a\nrecurrence equation , that can be used to analyze the running time of a recursive\nalgorithm. That technique is discussed in Section 12.1.4, in the context of recursive\nsorting algorithms.\nPerforming a Binary Search\nWhen considering the running time of the binary search algorithm, as presented\nin Section 5.1.3, we observe that a constant number of primitive operations are\nexecuted during each recursive call of the binary search method. Hence, the running\ntime is proportional to the number of recursive calls performed. We will show that\nat most⌊logn⌋+1 recursive calls are made during a binary search of a sequence\nhaving nelements, leading to the following claim.\nProposition 5.2: The binary search algorithm runs in O(logn)time for a sorted\narray with nelements.\nwww.it-ebooks.info"
  },
  {
    "page": 222,
    "content": "204 Chapter 5. Recursion\nJustiﬁcation: To prove this claim, a crucial fact is that with each recursive call\nthe number of candidate elements still to be searched is given by the value\nhigh−low+1.\nMoreover, the number of remaining candidates is reduced by at least one-half with\neach recursive call. Speciﬁcally, from the deﬁnition of mid, the number of remain-\ning candidates is either\n(mid−1)−low+1=/floorleftbigglow+high\n2/floorrightbigg\n−low≤high−low+1\n2\nor\nhigh−(mid+1)+1=high−/floorleftbigglow+high\n2/floorrightbigg\n≤high−low+1\n2.\nInitially, the number of candidates is n; after the ﬁrst call in a binary search, it is\nat most n/2; after the second call, it is at most n/4; and so on. In general, after\nthejthcall in a binary search, the number of candidate elements remaining is at\nmost n/2j. In the worst case (an unsuccessful search), the recursive calls stop when\nthere are no more candidate elements. Hence, the maximum number of recursive\ncalls performed, is the smallest integer rsuch thatn\n2r<1.\nIn other words (recalling that we omit a logarithm’s base when it is 2), ris the\nsmallest integer such that r>logn. Thus, we have\nr=⌊logn⌋+1,\nwhich implies that binary search runs in O(logn)time.\nComputing Disk Space Usage\nOur ﬁnal recursive algorithm from Section 5.1 was that for computing the overall\ndisk space usage in a speciﬁed portion of a ﬁle system. To characterize the “prob-\nlem size” for our analysis, we let ndenote the number of ﬁle-system entries in the\nportion of the ﬁle system that is considered. (For example, the ﬁle system portrayed\nin Figure 5.6 has n=19 entries.)\nTo characterize the cumulative time spent for an initial call to diskUsage , we\nmust analyze the total number of recursive invocations that are made, as well as the\nnumber of operations that are executed within those invocations.\nWe begin by showing that there are precisely nrecursive invocations of the\nmethod, in particular, one for each entry in the relevant portion of the ﬁle system.\nIntuitively, this is because a call to diskUsage for a particular entry eof the ﬁle\nsystem is only made from within the for loop of Code Fragment 5.5 when process-\ning the entry for the unique directory that contains e, and that entry will only be\nexplored once.\nwww.it-ebooks.info"
  },
  {
    "page": 223,
    "content": "5.2. Analyzing Recursive Algorithms 205\nTo formalize this argument, we can deﬁne the nesting level of each entry such\nthat the entry on which we begin has nesting level 0, entries stored directly within\nit have nesting level 1, entries stored within those entries have nesting level 2, andso on. We can prove by induction that there is exactly one recursive invocation ofdiskUsage upon each entry at nesting level k. As a base case, when k=0, the only\nrecursive invocation made is the initial one. As the inductive step, once we know\nthere is exactly one recursive invocation for each entry at nesting level k, we can\nclaim that there is exactly one invocation for each entry eat nesting level k+1,\nmade within the for loop for the entry at level kthat contains e.\nHaving established that there is one recursive call for each entry of the ﬁle\nsystem, we return to the question of the overall computation time for the algorithm.It would be great if we could argue that we spend O(1)time in any single invocation\nof the method, but that is not the case. While there is a constant number of steps\nreﬂected in the call to root.length() to compute the disk usage directly at that entry,\nwhen the entry is a directory, the body of the diskUsage method includes a for loop\nthat iterates over all entries that are contained within that directory. In the worst\ncase, it is possible that one entry includes n−1 others.\nBased on this reasoning, we could conclude that there are O(n)recursive calls,\neach of which runs in O(n)time, leading to an overall running time that is O(n\n2).\nWhile this upper bound is technically true, it is not a tight upper bound. Remark-\nably, we can prove the stronger bound that the recursive algorithm for diskUsage\ncompletes in O(n)time! The weaker bound was pessimistic because it assumed\na worst-case number of entries for each directory. While it is possible that somedirectories contain a number of entries proportional to n, they cannot all contain\nthat many. To prove the stronger claim, we choose to consider the overall number\nof iterations of the for loop across all recursive calls. We claim there are preciselyn−1 such iterations of that loop overall. We base this claim on the fact that each\niteration of that loop makes a recursive call to diskUsage , and yet we have already\nconcluded that there are a total of ncalls todiskUsage (including the original call).\nWe therefore conclude that there are O(n)recursive calls, each of which uses O(1)\ntime outside the loop, and that the overall number of operations due to the loop\nisO(n). Summing all of these bounds, the overall number of operations is O(n).\nThe argument we have made is more advanced than with the earlier examples\nof recursion. The idea that we can sometimes get a tighter bound on a series ofoperations by considering the cumulative effect, rather than assuming that each\nachieves a worst case is a technique called amortization ; we will see another ex-\nample of such analysis in Section 7.2.3. Furthermore, a ﬁle system is an implicit\nexample of a data structure known as a tree, and our disk usage algorithm is really\na manifestation of a more general algorithm known as a tree traversal . Trees will\nbe the focus of Chapter 8, and our argument about the O(n)running time of the\ndisk usage algorithm will be generalized for tree traversals in Section 8.4.\nwww.it-ebooks.info"
  },
  {
    "page": 224,
    "content": "206 Chapter 5. Recursion\n5.3 Further Examples of Recursion\nIn this section, we provide additional examples of the use of recursion. We organize\nour presentation by considering the maximum number of recursive calls that may\nbe started from within the body of a single activation.\n•If a recursive call starts at most one other, we call this a linear recursion .\n•If a recursive call may start two others, we call this a binary recursion .\n•If a recursive call may start three or more others, this is multiple recursion .\n5.3.1 Linear Recursion\nIf a recursive method is designed so that each invocation of the body makes at\nmost one new recursive call, this is know as linear recursion . Of the recursions\nwe have seen so far, the implementation of the factorial method (Section 5.1.1) is a\nclear example of linear recursion. More interestingly, the binary search algorithm\n(Section 5.1.3) is also an example of linear recursion , despite the term “binary”\nin the name. The code for binary search (Code Fragment 5.3) includes a case\nanalysis, with two branches that lead to a further recursive call, but only one branch\nis followed during a particular execution of the body.\nA consequence of the deﬁnition of linear recursion is that any recursion trace\nwill appear as a single sequence of calls, as we originally portrayed for the factorial\nmethod in Figure 5.1 of Section 5.1.1. Note that the linear recursion terminol-\nogy reﬂects the structure of the recursion trace, not the asymptotic analysis of the\nrunning time; for example, we have seen that binary search runs in O(logn)time.\nSumming the Elements of an Array Recursively\nLinear recursion can be a useful tool for processing a sequence, such as a Java array.\nSuppose, for example, that we want to compute the sum of an array of nintegers.\nWe can solve this summation problem using linear recursion by observing that if\nn=0 the sum is trivially 0, and otherwise it is the sum of the ﬁrst n−1 integers in\nthe array plus the last value in the array. (See Figure 5.9.)\n4 3 6 2 8 9 3 2 8 5 1 7 2 8 35\n70 1 2 3 4 6 7 8 9 10 11 12 13 14 15\nFigure 5.9: Computing the sum of a sequence recursively, by adding the last number\nto the sum of the ﬁrst n−1.\nwww.it-ebooks.info"
  },
  {
    "page": 225,
    "content": "5.3. Further Examples of Recursion 207\nA recursive algorithm for computing the sum of an array of integers based on\nthis intuition is implemented in Code Fragment 5.6.\n1/∗∗Returns the sum of the ﬁrst n integers of the given array. ∗/\n2public static int linearSum( int[ ] data,intn){\n3if(n == 0)\n4return0;\n5else\n6returnlinearSum(data, n−1) + data[n−1];\n7}\nCode Fragment 5.6: Summing an array of integers using linear recursion.\nA recursion trace of the linearSum method for a small example is given in\nFigure 5.10. For an input of size n, thelinearSum algorithm makes n+1 method\ncalls. Hence, it will take O(n)time, because it spends a constant amount of time\nperforming the nonrecursive part of each call. Moreover, we can also see that the\nmemory space used by the algorithm (in addition to the array) is also O(n), as we\nuse a constant amount of memory space for each of the n+1 frames in the trace at\nthe time we make the ﬁnal recursive call (with n=0).\nreturn15 + data[4] = 15 + 8 = 23\nlinearSum(data, 4)\nlinearSum(data, 3)\nlinearSum(data, 2)\nlinearSum(data, 1)\nlinearSum(data, 0)linearSum(data, 5)\nreturn0return0 + data[0] = 0 + 4 = 4return4 + data[1] = 4 + 3 = 7return7 + data[2] = 7 + 6 = 13return13 + data[3] = 13 + 2 = 15\nFigure 5.10: Recursion trace for an execution of linearSum(data, 5) with input\nparameter data = 4, 3, 6, 2, 8 .\nwww.it-ebooks.info"
  },
  {
    "page": 226,
    "content": "208 Chapter 5. Recursion\nReversing a Sequence with Recursion\nNext, let us consider the problem of reversing the nelements of an array, so that\nthe ﬁrst element becomes the last, the second element becomes second to the last,\nand so on. We can solve this problem using linear recursion, by observing that the\nreversal of a sequence can be achieved by swapping the ﬁrst and last elements and\nthen recursively reversing the remaining elements. We present an implementation\nof this algorithm in Code Fragment 5.7, using the convention that the ﬁrst time we\ncall this algorithm we do so as reverseArray(data, 0, n −1).\n1/∗∗Reverses the contents of subarray data[low] through data[high] inclusive. ∗/\n2public static void reverseArray( int[ ] data,intlow,inthigh){\n3if(low<high){ // if at least two elements in subarray\n4inttemp = data[low]; // swap data[low] and data[high]\n5data[low] = data[high];\n6data[high] = temp;\n7reverseArray(data, low + 1, high −1);// recur on the rest\n8}\n9}\nCode Fragment 5.7: Reversing the elements of an array using linear recursion.\nWe note that whenever a recursive call is made, there will be two fewer elements\nin the relevant portion of the array. (See Figure 5.11.) Eventually a base case is\nreached when the condition low<high fails, either because low == high in the\ncase that nis odd, or because low == high + 1 in the case that nis even.\nThe above argument implies that the recursive algorithm of Code Fragment 5.7\nis guaranteed to terminate after a total of 1 +/floorleftbign\n2/floorrightbig\nrecursive calls. Because each call\ninvolves a constant amount of work, the entire process runs in O(n)time.\n9 48 9 5\n77775 0 1 2 3 4 6\n5 9 85 9 85 9 65 3 67\n4 3 6\n72222\n2 6 3 46 3 48 3 48\nFigure 5.11: A trace of the recursion for reversing a sequence. The highlighted\nportion has yet to be reversed.\nwww.it-ebooks.info"
  },
  {
    "page": 227,
    "content": "5.3. Further Examples of Recursion 209\nRecursive Algorithms for Computing Powers\nAs another interesting example of the use of linear recursion, we consider the prob-\nlem of raising a number xto an arbitrary nonnegative integer n. That is, we wish\nto compute the power function , deﬁned as power(x,n) =xn. (We use the name\n“power” for this discussion, to differentiate from the pow method of the Math class,\nwhich provides such functionality.) We will consider two different recursive for-\nmulations for the problem that lead to algorithms with very different performance.\nA trivial recursive deﬁnition follows from the fact that xn=x·xn−1forn>0.\npower(x,n) =/braceleftbigg1 ifn=0\nx·power(x,n−1)otherwise.\nThis deﬁnition leads to a recursive algorithm shown in Code Fragment 5.8.\n1/∗∗Computes the value of x raised to the nth power, for nonnegative integer n. ∗/\n2public static double power(doublex,intn){\n3if(n == 0)\n4return1;\n5else\n6returnx∗power(x, n−1);\n7}\nCode Fragment 5.8: Computing the power function using trivial recursion.\nA recursive call to this version of power(x,n)runs in O(n)time. Its recursion\ntrace has structure very similar to that of the factorial function from Figure 5.1,\nwith the parameter decreasing by one with each call, and constant work performed\nat each of n+1 levels.\nHowever, there is a much faster way to compute the power function using an\nalternative deﬁnition that employs a squaring technique. Let k=/floorleftbign\n2/floorrightbig\ndenote the\nﬂoor of the integer division (equivalent to n/2in Java when nis anint). We consider\nthe expression/parenleftbig\nxk/parenrightbig2. When nis even,/floorleftbign\n2/floorrightbig\n=n\n2and therefore/parenleftbig\nxk/parenrightbig2=/parenleftBig\nxn\n2/parenrightBig2\n=xn.\nWhen nis odd,/floorleftbign\n2/floorrightbig\n=n−1\n2and/parenleftbig\nxk/parenrightbig2=xn−1, and therefore xn=/parenleftbig\nxk/parenrightbig2·x, just as\n213=/parenleftbig\n26·26/parenrightbig\n·2. This analysis leads to the following recursive deﬁnition:\npower(x,n)=\n\n1 ifn=0/parenleftbig\npower/parenleftbig\nx,/floorleftbign\n2/floorrightbig/parenrightbig/parenrightbig2·xifn>0 is odd/parenleftbig\npower/parenleftbig\nx,/floorleftbign\n2/floorrightbig/parenrightbig/parenrightbig2ifn>0 is even\nIf we were to implement this recursion making tworecursive calls to compute\npower(x,/floorleftbign\n2/floorrightbig\n)·power(x,/floorleftbign\n2/floorrightbig\n), a trace of the recursion would demonstrate O(n)\ncalls. We can perform signiﬁcantly fewer operations by computing power(x,/floorleftbign\n2/floorrightbig\n)\nand storing it in a variable as a partial result, and then multiplying it by itself. An\nimplementation based on this recursive deﬁnition is given in Code Fragment 5.9.\nwww.it-ebooks.info"
  },
  {
    "page": 228,
    "content": "210 Chapter 5. Recursion\n1/∗∗Computes the value of x raised to the nth power, for nonnegative integer n. ∗/\n2public static double power(doublex,intn){\n3if(n == 0)\n4return1;\n5else{\n6doublepartial = power(x, n/2); // rely on truncated division of n\n7doubleresult = partial ∗partial;\n8if(n % 2 == 1) // if n odd, include extra factor of x\n9 result ∗= x;\n10returnresult;\n11}\n12}\nCode Fragment 5.9: Computing the power function using repeated squaring.\nTo illustrate the execution of our improved algorithm, Figure 5.12 provides a\nrecursion trace of the computation power(2, 13) .\nreturn64∗64∗2 = 8192\npower(2, 13)\npower(2, 6)\npower(2, 3)\npower(2, 1)\npower(2, 0)return1return1∗1∗2 = 2return2∗2∗2 = 8return8∗8 = 64\nFigure 5.12: Recursion trace for an execution of power(2, 13) .\nTo analyze the running time of the revised algorithm, we observe that the ex-\nponent in each recursive call of method power(x,n) is at most half of the preceding\nexponent. As we saw with the analysis of binary search, the number of times that\nwe can divide nby two before getting to one or less is O(logn). Therefore, our new\nformulation of power results in O(logn)recursive calls. Each individual activation\nof the method uses O(1)operations (excluding the recursive call), and so the total\nnumber of operations for computing power(x,n) isO(logn). This is a signiﬁcant\nimprovement over the original O(n)-time algorithm.\nThe improved version also provides signiﬁcant saving in reducing the memory\nusage. The ﬁrst version has a recursive depth of O(n), and therefore, O(n)frames\nare simultaneously stored in memory. Because the recursive depth of the improved\nversion is O(logn), its memory usage is O(logn)as well.\nwww.it-ebooks.info"
  },
  {
    "page": 229,
    "content": "5.3. Further Examples of Recursion 211\n5.3.2 Binary Recursion\nWhen a method makes two recursive calls, we say that it uses binary recursion .\nWe have already seen an example of binary recursion when drawing the English\nruler (Section 5.1.2). As another application of binary recursion, let us revisit the\nproblem of summing the nintegers of an array. Computing the sum of one or zero\nvalues is trivial. With two or more values, we can recursively compute the sum of\nthe ﬁrst half, and the sum of the second half, and add those sums together. Our\nimplementation of such an algorithm, in Code Fragment 5.10, is initially invoked\nasbinarySum(data, 0, n −1).\n1/∗∗Returns the sum of subarray data[low] through data[high] inclusive. ∗/\n2public static int binarySum( int[ ] data,intlow,inthigh){\n3if(low>high) // zero elements in subarray\n4return0;\n5else if(low == high) // one element in subarray\n6returndata[low];\n7else{\n8intmid = (low + high) / 2;\n9returnbinarySum(data, low, mid) + binarySum(data, mid+1, high);\n10}\n11}\nCode Fragment 5.10: Summing the elements of a sequence using binary recursion.\nTo analyze algorithm binarySum , we consider, for simplicity, the case where\nnis a power of two. Figure 5.13 shows the recursion trace of an execution of\nbinarySum(data, 0, 7) . We label each box with the values of parameters lowand\nhigh for that call. The size of the range is divided in half at each recursive call,\nand so the depth of the recursion is 1 +log2n. Therefore, binarySum uses O(logn)\namount of additional space, which is a big improvement over the O(n)space used\nby thelinearSum method of Code Fragment 5.6. However, the running time of\nbinarySum isO(n), as there are 2 n−1 method calls, each requiring constant time.\n0,0 1,1 2,2 4,4 6,6 7,7 3,3 5,50,1 4,5 6,7 2,30,3 4,70,7\nFigure 5.13: Recursion trace for the execution of binarySum(data, 0, 7) .\nwww.it-ebooks.info"
  },
  {
    "page": 230,
    "content": "212 Chapter 5. Recursion\n5.3.3 Multiple Recursion\nGeneralizing from binary recursion, we deﬁne multiple recursion as a process in\nwhich a method may make more than two recursive calls. Our recursion for an-\nalyzing the disk space usage of a ﬁle system (see Section 5.1.4) is an example of\nmultiple recursion, because the number of recursive calls made during one invoca-\ntion was equal to the number of entries within a given directory of the ﬁle system.\nAnother common application of multiple recursion is when we want to enumer-\nate various conﬁgurations in order to solve a combinatorial puzzle. For example,\nthe following are all instances of what are known as summation puzzles :\npot+pan=bib\ndog+cat=pig\nboy+girl=baby\nTo solve such a puzzle, we need to assign a unique digit (that is, 0 ,1,..., 9) to each\nletter in the equation, in order to make the equation true. Typically, we solve such\na puzzle by using our human observations of the particular puzzle we are trying to\nsolve to eliminate conﬁgurations (that is, possible partial assignments of digits to\nletters) until we can work through the feasible conﬁgurations that remain, testing\nfor the correctness of each one.\nIf the number of possible conﬁgurations is not too large, however, we can use\na computer to simply enumerate all the possibilities and test each one, without em-\nploying any human observations. Such an algorithm can use multiple recursion\nto work through the conﬁgurations in a systematic way. To keep the description\ngeneral enough to be used with other puzzles, we consider an algorithm that enu-\nmerates and tests all k-length sequences, without repetitions, chosen from a given\nuniverse U. We show pseudocode for such an algorithm in Code Fragment 5.11,\nbuilding the sequence of kelements with the following steps:\n1.Recursively generating the sequences of k−1 elements\n2.Appending to each such sequence an element not already contained in it.\nThroughout the execution of the algorithm, we use a set Uto keep track of the\nelements not contained in the current sequence, so that an element ehas not been\nused yet if and only if eis in U.\nAnother way to look at the algorithm of Code Fragment 5.11 is that it enumer-\nates every possible size- kordered subset of U, and tests each subset for being a\npossible solution to our puzzle.\nFor summation puzzles, U={0,1,2,3,4,5,6,7,8,9}and each position in the\nsequence corresponds to a given letter. For example, the ﬁrst position could stand\nforb, the second for o, the third for y, and so on.\nwww.it-ebooks.info"
  },
  {
    "page": 231,
    "content": "5.3. Further Examples of Recursion 213\nAlgorithm PuzzleSolve( k,S,U):\nInput: An integer k, sequence S, and set U\nOutput: An enumeration of all k-length extensions to Susing elements in U\nwithout repetitions\nforeach einUdo\nAdd eto the end of S\nRemove efrom U {eis now being used}\nifk==1then\nTest whether Sis a conﬁguration that solves the puzzle\nifSsolves the puzzle then\naddSto output {a solution}\nelse\nPuzzleSolve (k−1,S,U) {a recursive call}\nRemove efrom the end of S\nAdd eback to U {eis now considered as unused }\nCode Fragment 5.11: Solving a combinatorial puzzle by enumerating and testing\nall possible conﬁgurations.\nIn Figure 5.14, we show a recursion trace of a call to PuzzleSolve( 3,S,U),\nwhere Sis empty and U={a,b,c}. During the execution, all the permutations\nof the three characters are generated and tested. Note that the initial call makes\nthree recursive calls, each of which in turn makes two more. If we had executed\nPuzzleSolve( 3,S,U)on a set Uconsisting of four elements, the initial call would\nhave made four recursive calls, each of which would have a trace looking like the\none in Figure 5.14.\ninitial call\nPuzzleSolve(3, (), {a,b,c})\nPuzzleSolve(2, b, {a,c}) PuzzleSolve(2, c, {a,b})\nPuzzleSolve(1, ca, {b})PuzzleSolve(2, a, {b,c})\nPuzzleSolve(1, ab, {c}) PuzzleSolve(1, ba, {c})\nPuzzleSolve(1, bc, {a}) PuzzleSolve(1, ac, {b}) PuzzleSolve(1, cb, {a})\nacbabc bac cab\nbca cba\nFigure 5.14: Recursion trace for an execution of PuzzleSolve( 3,S,U), where Sis\nempty and U={a,b,c}. This execution generates and tests all permutations of a,b,\nandc. We show the permutations generated directly below their respective boxes.\nwww.it-ebooks.info"
  },
  {
    "page": 232,
    "content": "214 Chapter 5. Recursion\n5.4 Designing Recursive Algorithms\nAn algorithm that uses recursion typically has the following form:\n•Test for base cases. We begin by testing for a set of base cases (there should\nbe at least one). These base cases should be deﬁned so that every possible\nchain of recursive calls will eventually reach a base case, and the handling of\neach base case should not use recursion.\n•Recur. If not a base case, we perform one or more recursive calls. This recur-\nsive step may involve a test that decides which of several possible recursive\ncalls to make. We should deﬁne each possible recursive call so that it makes\nprogress towards a base case.\nParameterizing a Recursion\nTo design a recursive algorithm for a given problem, it is useful to think of the\ndifferent ways we might deﬁne subproblems that have the same general structure\nas the original problem. If one has difﬁculty ﬁnding the repetitive structure needed\nto design a recursive algorithm, it is sometimes useful to work out the problem on\na few concrete examples to see how the subproblems should be deﬁned.\nA successful recursive design sometimes requires that we redeﬁne the origi-\nnal problem to facilitate similar-looking subproblems. Often, this involved repa-\nrameterizing the signature of the method. For example, when performing a bi-\nnary search in an array, a natural method signature for a caller would appear as\nbinarySearch(data, target) . However, in Section 5.1.3, we deﬁned our method\nwith calling signature binarySearch(data, target, low, high) , using the additional\nparameters to demarcate subarrays as the recursion proceeds. This change in pa-\nrameterization is critical for binary search. Several other examples in this chapter\n(e.g.,reverseArray ,linearSum ,binarySum ) also demonstrated the use of additional\nparameters in deﬁning recursive subproblems.\nIf we wish to provide a cleaner public interface to an algorithm without expos-\ning the user to the recursive parameterization, a standard technique is to make the\nrecursive version private, and to introduce a cleaner public method (that calls the\nprivate one with appropriate parameters). For example, we might offer the follow-\ning simpler version of binarySearch for public use:\n/∗∗Returns true if the target value is found in the data array. ∗/\npublic static boolean binarySearch( int[ ] data,inttarget){\nreturnbinarySearch(data, target, 0, data.length −1);// use parameterized version\n}\nwww.it-ebooks.info"
  },
  {
    "page": 233,
    "content": "5.5. Recursion Run Amok 215\n5.5 Recursion Run Amok\nAlthough recursion is a very powerful tool, it can easily be misused in various\nways. In this section, we examine several cases in which a poorly implemented re-\ncursion causes drastic inefﬁciency, and we discuss some strategies for recognizing\nand avoid such pitfalls.\nWe begin by revisiting the element uniqueness problem , deﬁned on page 174\nof Section 4.3.3. We can use the following recursive formulation to determine if\nallnelements of a sequence are unique. As a base case, when n=1, the elements\nare trivially unique. For n≥2, the elements are unique if and only if the ﬁrst n−1\nelements are unique, the last n−1 items are unique, and the ﬁrst and last elements\nare different (as that is the only pair that was not already checked as a subcase). A\nrecursive implementation based on this idea is given in Code Fragment 5.12, named\nunique3 (to differentiate it from unique1 andunique2 from Chapter 4).\n1/∗∗Returns true if there are no duplicate values from data[low] through data[high]. ∗/\n2public static boolean unique3(int[ ] data,intlow,inthigh){\n3if(low>= high)return true ; // at most one item\n4else if(!unique3(data, low, high −1))return false ;// duplicate in ﬁrst n −1\n5else if(!unique3(data, low+1, high)) return false ;// duplicate in last n −1\n6else return (data[low] != data[high]); // do ﬁrst and last diﬀer?\n7}\nCode Fragment 5.12: Recursive unique3 for testing element uniqueness.\nUnfortunately, this is a terribly inefﬁcient use of recursion. The nonrecursive\npart of each call uses O(1)time, so the overall running time will be proportional to\nthe total number of recursive invocations. To analyze the problem, we let ndenote\nthe number of entries under consideration, that is, let n=1 + high−low.\nIfn=1, then the running time of unique3 isO(1), since there are no recursive\ncalls for this case. In the general case, the important observation is that a single call\ntounique3 for a problem of size nmay result in two recursive calls on problems of\nsizen−1. Those two calls with size n−1 could in turn result in four calls (two\neach) with a range of size n−2, and thus eight calls with size n−3 and so on.\nThus, in the worst case, the total number of method calls is given by the geometric\nsummation\n1+2+4+···+2n−1,\nwhich is equal to 2n−1 by Proposition 4.5. Thus, the running time of method\nunique3 isO(2n). This is an incredibly inefﬁcient method for solving the ele-\nment uniqueness problem. Its inefﬁciency comes not from the fact that it uses\nrecursion—it comes from the fact that it uses recursion poorly, which is something\nwe address in Exercise C-5.12.\nwww.it-ebooks.info"
  },
  {
    "page": 234,
    "content": "216 Chapter 5. Recursion\nAn Ineﬃcient Recursion for Computing Fibonacci Numbers\nIn Section 2.2.3, we introduced a process for generating the progression of Fi-\nbonacci numbers, which can be deﬁned recursively as follows:\nF0=0\nF1=1\nFn=Fn−2+Fn−1forn>1.\nIronically, a recursive implementation based directly on this deﬁnition results in themethodﬁbonacciBad shown in Code Fragment 5.13, which computes a Fibonacci\nnumber by making two recursive calls in each non-base case.\n1/∗∗Returns the nth Fibonacci number (ineﬃciently). ∗/\n2public static long ﬁbonacciBad( intn){\n3if(n<= 1)\n4returnn;\n5else\n6returnﬁbonacciBad(n−2) + ﬁbonacciBad(n −1);\n7}\nCode Fragment 5.13: Computing the nthFibonacci number using binary recursion.\nUnfortunately, such a direct implementation of the Fibonacci formula results\nin a terribly inefﬁcient method. Computing the nthFibonacci number in this way\nrequires an exponential number of calls to the method. Speciﬁcally, let cndenote\nthe number of calls performed in the execution of ﬁbonacciBad (n). Then, we have\nthe following values for the cn’s:\nc0=1\nc1=1\nc2=1+c0+c1=1+1+1=3\nc3=1+c1+c2=1+1+3=5\nc4=1+c2+c3=1+3+5=9\nc5=1+c3+c4=1+5+9=15\nc6=1+c4+c5=1+9+15=25\nc7=1+c5+c6=1+15+25=41\nc8=1+c6+c7=1+25+41=67\nIf we follow the pattern forward, we see that the number of calls more than doublesfor each two consecutive indices. That is, c\n4is more than twice c2,c5is more than\ntwice c3,c6is more than twice c4, and so on. Thus, cn>2n/2, which means that\nﬁbonacciBad (n)makes a number of calls that is exponential in n.\nwww.it-ebooks.info"
  },
  {
    "page": 235,
    "content": "5.5. Recursion Run Amok 217\nAn Eﬃcient Recursion for Computing Fibonacci Numbers\nWe were tempted into using the bad recursive formulation because of the way the\nnthFibonacci number, Fn, depends on the two previous values, Fn−2andFn−1. But\nnotice that after computing Fn−2, the call to compute Fn−1requires its own recursive\ncall to compute Fn−2, as it does not have knowledge of the value of Fn−2that was\ncomputed at the earlier level of recursion. That is duplicative work. Worse yet, bothof those calls will need to (re)compute the value of F\nn−3, as will the computation\nofFn−1. This snowballing effect is what leads to the exponential running time of\nﬁbonacciBad .\nWe can compute Fnmuch more efﬁciently using a recursion in which each\ninvocation makes only one recursive call. To do so, we need to redeﬁne the expec-\ntations of the method. Rather than having the method return a single value, whichis the n\nthFibonacci number, we deﬁne a recursive method that returns an array with\ntwo consecutive Fibonacci numbers {Fn,Fn−1}, using the convention F−1=0. Al-\nthough it seems to be a greater burden to report two consecutive Fibonacci numbersinstead of one, passing this extra information from one level of the recursion to thenext makes it much easier to continue the process. (It allows us to avoid having\nto recompute the second value that was already known within the recursion.) An\nimplementation based on this strategy is given in Code Fragment 5.14.\n1/∗∗Returns array containing the pair of Fibonacci numbers, F(n) and F(n −1).∗/\n2public static long [ ] ﬁbonacciGood( intn){\n3if(n<= 1){\n4long[ ] answer ={n, 0};\n5returnanswer;\n6}else{\n7long[ ] temp = ﬁbonacciGood(n −1); // returns{Fn−1,Fn−2}\n8long[ ] answer ={temp[0] + temp[1], temp[0] };// we want{Fn,Fn−1}\n9returnanswer;\n10}\n11}\nCode Fragment 5.14: Computing the nthFibonacci number using linear recursion.\nIn terms of efﬁciency, the difference between the bad and good recursions for\nthis problem is like night and day. The ﬁbonacciBad method uses exponential\ntime. We claim that the execution of method ﬁbonacciGood( n)runs in O(n)time.\nEach recursive call to ﬁbonacciGood decreases the argument nby 1; therefore, a\nrecursion trace includes a series of nmethod calls. Because the nonrecursive work\nfor each call uses constant time, the overall computation executes in O(n)time.\nwww.it-ebooks.info"
  },
  {
    "page": 236,
    "content": "218 Chapter 5. Recursion\n5.5.1 Maximum Recursive Depth in Java\nAnother danger in the misuse of recursion is known as inﬁnite recursion . If each\nrecursive call makes another recursive call, without ever reaching a base case, then\nwe have an inﬁnite series of such calls. This is a fatal error. An inﬁnite recursion\ncan quickly swamp computing resources, not only due to rapid use of the CPU,\nbut because each successive call creates a frame requiring additional memory. A\nblatant example of an ill-formed recursion is the following:\n1/∗∗Don't call this (inﬁnite) version. ∗/\n2public static int ﬁbonacci( intn){\n3returnﬁbonacci(n); // After all Fndoes equal Fn\n4}\nHowever, there are far more subtle errors that can lead to an inﬁnite recursion.\nRevisiting our implementation of binary search (Code Fragment 5.3), when we\nmake a recursive call on the right portion of the sequence (line 15), we specify the\nsubarray from index mid+1 tohigh. Had that line instead been written as\nreturnbinarySearch(data, target, mid, high); // sending mid, not mid+1\nthis could result in an inﬁnite recursion. In particular, when searching a range of\ntwo elements, it becomes possible to make a recursive call on the identical range.\nA programmer should ensure that each recursive call is in some way progress-\ning toward a base case (for example, by having a parameter value that decreases\nwith each call). To combat against inﬁnite recursions, the designers of Java made\nan intentional decision to limit the overall space used to store activation frames\nfor simultaneously active method calls. If this limit is reached, the Java Virtual\nMachine throws a StackOverﬂowError . (We will further discuss the “stack” data\nstructure in Section 6.1.) The precise value of this limit depends upon the Java\ninstallation, but a typical value might allow upward of 1000 simultaneous calls.\nFor many applications of recursion, allowing up to 1000 nested calls sufﬁces.\nFor example, our binarySearch method (Section 5.1.3) has O(logn)recursive depth,\nand so for the default recursive limit to be reached, there would need to be 21000\nelements (far, far more than the estimated number of atoms in the universe). How-\never, we have seen several linear recursions that have recursive depth proportional\nton. Java’s limit on the recursive depth might disrupt such computations.\nIt is possible to reconﬁgure the Java Virtual Machine so that it allows for greater\nspace to be devoted to nested method calls. This is done by setting the -Xss runtime\noption when starting Java, either as a command-line option or through the settings\nof an IDE. But it often possible to rely upon the intuition of a recursive algorithm,\nyet to reimplement it more directly using traditional loops rather than method calls\nto express the necessary repetition. We discuss just such an approach to conclude\nthe chapter.\nwww.it-ebooks.info"
  },
  {
    "page": 237,
    "content": "5.6. Eliminating Tail Recursion 219\n5.6 Eliminating Tail Recursion\nThe main beneﬁt of a recursive approach to algorithm design is that it allows us to\nsuccinctly take advantage of a repetitive structure present in many problems. By\nmaking our algorithm description exploit the repetitive structure in a recursive way,\nwe can often avoid complex case analyses and nested loops. This approach can\nlead to more readable algorithm descriptions, while still being quite efﬁcient.\nHowever, the usefulness of recursion comes at a modest cost. In particular,\nthe Java Virtual Machine must maintain frames that keep track of the state of each\nnested call. When computer memory is at a premium, it can be beneﬁcial to derive\nnonrecursive implementations of recursive algorithms.\nIn general, we can use the stack data structure, which we will introduce in\nSection 6.1, to convert a recursive algorithm into a nonrecursive algorithm by man-\naging the nesting of the recursive structure ourselves, rather than relying on the\ninterpreter to do so. Although this only shifts the memory usage from the inter-\npreter to our stack, we may be able to further reduce the memory usage by storing\nthe minimal information necessary.\nEven better, some forms of recursion can be eliminated without any use of aux-\niliary memory. One such form is known as tail recursion . A recursion is a tail\nrecursion if any recursive call that is made from one context is the very last opera-\ntion in that context, with the return value of the recursive call (if any) immediately\nreturned by the enclosing recursion. By necessity, a tail recursion must be a lin-\near recursion (since there is no way to make a second recursive call if you must\nimmediately return the result of the ﬁrst).\nOf the recursive methods demonstrated in this chapter, the binarySearch method\nof Code Fragment 5.3 and the reverseArray method of Code Fragment 5.7 are ex-\namples of tail recursion. Several others of our linear recursions are almost like\ntail recursion, but not technically so. For example, our factorial method of Code\nFragment 5.1 is nota tail recursion. It concludes with the command:\nreturnn∗factorial(n−1);\nThis is not a tail recursion because an additional multiplication is performed after\nthe recursive call is completed, and the result returned is not the same. For similar\nreasons, the linearSum method of Code Fragment 5.6, both power methods from\nCode Fragments 5.8 and 5.9, and the ﬁbonacciGood method of Code Fragment 5.13\nfail to be tail recursions.\nTail recursions are special, as they can be automatically reimplemented nonre-\ncursively by enclosing the body in a loop for repetition, and replacing a recursive\ncall with new parameters by a reassignment of the existing parameters to those\nvalues. In fact, many programming language implementations may convert tail\nrecursions in this way as an optimization.\nwww.it-ebooks.info"
  },
  {
    "page": 238,
    "content": "220 Chapter 5. Recursion\n1/∗∗Returns true if the target value is found in the data array. ∗/\n2public static boolean binarySearchIterative( int[ ] data,inttarget){\n3intlow = 0;\n4inthigh = data.length −1;\n5while(low<= high){\n6intmid = (low + high) / 2;\n7if(target == data[mid]) // found a match\n8 return true ;\n9else if(target<data[mid])\n10 high = mid−1; // only consider values left of mid\n11else\n12 low = mid + 1; // only consider values right of mid\n13}\n14return false ; // loop ended without success\n15}\nCode Fragment 5.15: A nonrecursive implementation of binary search.\nAs a tangible example, our binarySearch method can be reimplemented as\nshown in Code Fragment 5.15. We initialize variables lowandhigh to represent\nthe full extent of the array just prior to our while loop. Then, during each pass of\nthe loop, we either ﬁnd the target, or we narrow the range of the candidate subar-\nray. Where we made the recursive call binarySearch(data, target, low, mid −1)\nin the original version, we simply replace high = mid−1in our new version\nand then continue to the next iteration of the loop. Our original base case con-dition of low>high has simply been replaced by the opposite loop condition,\nwhilelow<= high . In our new implementation, we return false to designate a\nfailed search if the while loop ends without having ever returned true from within.\nMost other linear recursions can be expressed quite efﬁciently with iteration,\neven if they were not formally tail recursions. For example, there are trivial nonre-cursive implementations for computing factorials, computing Fibonacci numbers,\nsumming elements of an array, or reversing the contents of an array. For example,\nCode Fragment 5.16 provides a nonrecursive method to reverse the contents of anarray (as compared to the earlier recursive method from Code Fragment 5.7).\n1/∗∗Reverses the contents of the given array. ∗/\n2public static void reverseIterative( int[ ] data){\n3intlow = 0, high = data.length −1;\n4while(low<high){ // swap data[low] and data[high]\n5inttemp = data[low];\n6data[low++] = data[high]; // post-increment of low\n7data[high−−] = temp; // post-decrement of high\n8}\n9}\nCode Fragment 5.16: Reversing the elements of a sequence using iteration.\nwww.it-ebooks.info"
  },
  {
    "page": 239,
    "content": "5.7. Exercises 221\n5.7 Exercises\nReinforcement\nR-5.1 Describe a recursive algorithm for ﬁnding the maximum element in an array, A,\nofnelements. What is your running time and space usage?\nR-5.2 Explain how to modify the recursive binary search algorithm so that it returns the\nindex of the target in the sequence or −1 (if the target is not found).\nR-5.3 Draw the recursion trace for the computation of power(2,5), using the traditional\nalgorithm implemented in Code Fragment 5.8.\nR-5.4 Draw the recursion trace for the computation of power(2,18), using the repeated\nsquaring algorithm, as implemented in Code Fragment 5.9.\nR-5.5 Draw the recursion trace for the execution of reverseArray(data, 0, 4) , from\nCode Fragment 5.7, on array data = 4, 3, 6, 2, 6 .\nR-5.6 Draw the recursion trace for the execution of method PuzzleSolve (3,S,U), from\nCode Fragment 5.11, where Sis empty and U={a,b,c,d}.\nR-5.7 Describe a recursive algorithm for computing the nthHarmonic number , deﬁned\nasHn=∑n\nk=11/k.\nR-5.8 Describe a recursive algorithm for converting a string of digits into the integer it\nrepresents. For example, '13531'represents the integer 13 ,531.\nR-5.9 Develop a nonrecursive implementation of the version of the power method from\nCode Fragment 5.9 that uses repeated squaring.\nR-5.10 Describe a way to use recursion to compute the sum of all the elements in an\nn×n(two-dimensional) array of integers.\nCreativity\nC-5.11 Describe a recursive algorithm to compute the integer part of the base-two loga-\nrithm of nusing only addition and integer division.\nC-5.12 Describe an efﬁcient recursive algorithm for solving the element uniqueness\nproblem, which runs in time that is at most O(n2)in the worst case without using\nsorting.\nC-5.13 Give a recursive algorithm to compute the product of two positive integers, mand\nn, using only addition and subtraction.\nC-5.14 In Section 5.2 we prove by induction that the number of lines printed by a call to\ndrawInterval( c)is 2c−1. Another interesting question is how many dashes are\nprinted during that process. Prove by induction that the number of dashes printed\nbydrawInterval( c)is 2c+1−c−2.\nwww.it-ebooks.info"
  },
  {
    "page": 240,
    "content": "222 Chapter 5. Recursion\nC-5.15 Write a recursive method that will output all the subsets of a set of nelements\n(without repeating any subsets).\nC-5.16 In the Towers of Hanoi puzzle, we are given a platform with three pegs, a,b, and\nc, sticking out of it. On peg ais a stack of ndisks, each larger than the next, so\nthat the smallest is on the top and the largest is on the bottom. The puzzle is to\nmove all the disks from peg ato peg c, moving one disk at a time, so that we\nnever place a larger disk on top of a smaller one. See Figure 5.15 for an exampleof the case n=4. Describe a recursive algorithm for solving the Towers of Hanoi\npuzzle for arbitrary n. (Hint: Consider ﬁrst the subproblem of moving all but\nthen\nthdisk from peg ato another peg using the third as “temporary storage.”)\nFigure 5.15: An illustration of the Towers of Hanoi puzzle.\nC-5.17 Write a short recursive Java method that takes a character string sand outputs its\nreverse. For example, the reverse of 'pots&pans 'would be'snap&stop '.\nC-5.18 Write a short recursive Java method that determines if a string sis a palindrome,\nthat is, it is equal to its reverse. Examples of palindromes include 'racecar'\nand'gohangasalamiimalasagnahog '.\nC-5.19 Use recursion to write a Java method for determining if a string shas more vowels\nthan consonants.\nC-5.20 Write a short recursive Java method that rearranges an array of integer values sothat all the even values appear before all the odd values.\nC-5.21 Given an unsorted array, A, of integers and an integer k, describe a recursive\nalgorithm for rearranging the elements in Aso that all elements less than or equal\ntokcome before any elements larger than k. What is the running time of your\nalgorithm on an array of nvalues?\nC-5.22 Suppose you are given an array, A, containing ndistinct integers that are listed\nin increasing order. Given a number k, describe a recursive algorithm to ﬁnd two\nintegers in Athat sum to k, if such a pair exists. What is the running time of your\nalgorithm?\nC-5.23 Describe a recursive algorithm that will check if an array Aof integers contains\nan integer A[i]that is the sum of two integers that appear earlier in A, that is, such\nthatA[i]=A[j]+A[k]forj,k<i.\nwww.it-ebooks.info"
  },
  {
    "page": 241,
    "content": "Chapter Notes 223\nC-5.24 Isabel has an interesting way of summing up the values in an array Aofnintegers,\nwhere nis a power of two. She creates an array Bof half the size of Aand sets\nB[i] =A[2i]+A[2i+1], for i=0,1,...,(n/2)−1. If Bhas size 1, then she\noutputs B[0]. Otherwise, she replaces Awith B, and repeats the process. What is\nthe running time of her algorithm?\nC-5.25 Describe a fast recursive algorithm for reversing a singly linked list L, so that the\nordering of the nodes becomes opposite of what it was before.\nC-5.26 Give a recursive deﬁnition of a singly linked list class that does not use any Node\nclass.\nProjects\nP-5.27 Implement a recursive method with calling signature ﬁnd(path, ﬁlename) that\nreports all entries of the ﬁle system rooted at the given path having the given ﬁle\nname.\nP-5.28 Write a program for solving summation puzzles by enumerating and testing all\npossible conﬁgurations. Using your program, solve the three puzzles given in\nSection 5.3.3.\nP-5.29 Provide a nonrecursive implementation of the drawInterval method for the En-\nglish ruler project of Section 5.1.2. There should be precisely 2c−1 lines of\noutput if crepresents the length of the center tick. If incrementing a counter from\n0 to 2c−2, the number of dashes for each tick line should be exactly one more\nthan the number of consecutive 1’s at the end of the binary representation of the\ncounter.\nP-5.30 Write a program that can solve instances of the Tower of Hanoi problem (from\nExercise C-5.16).\nChapter Notes\nThe use of recursion in programs belongs to the folklore of computer science (for example,\nsee the article of Dijkstra [31]). It is also at the heart of functional programming languages\n(for example, see the book by Abelson, Sussman, and Sussman [1]). Interestingly, binary\nsearch was ﬁrst published in 1946, but was not published in a fully correct form until 1962.\nFor further discussions on lessons learned, see papers by Bentley [13] and Lesuisse [64].\nwww.it-ebooks.info"
  },
  {
    "page": 242,
    "content": "www.it-ebooks.info"
  },
  {
    "page": 243,
    "content": "Chapter\n6Stacks, Queues, and Deques\nContents\n6.1 Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n6.1.1 The Stack Abstract Data Type . . . . . . . . . . . . . . . 227\n6.1.2 A Simple Array-Based Stack Implementation . . . . . . . 230\n6.1.3 Implementing a Stack with a Singly Linked List . . . . . . 233\n6.1.4 Reversing an Array Using a Stack . . . . . . . . . . . . . 234\n6.1.5 Matching Parentheses and HTML Tags . . . . . . . . . . 235\n6.2 Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n6.2.1 The Queue Abstract Data Type . . . . . . . . . . . . . . 239\n6.2.2 Array-Based Queue Implementation . . . . . . . . . . . . 241\n6.2.3 Implementing a Queue with a Singly Linked List . . . . . . 245\n6.2.4 A Circular Queue . . . . . . . . . . . . . . . . . . . . . . 246\n6.3 Double-Ended Queues . . . . . . . . . . . . . . . . . . . . . 248\n6.3.1 The Deque Abstract Data Type . . . . . . . . . . . . . . 248\n6.3.2 Implementing a Deque . . . . . . . . . . . . . . . . . . . 250\n6.3.3 Deques in the Java Collections Framework . . . . . . . . . 251\n6.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\nwww.it-ebooks.info"
  },
  {
    "page": 244,
    "content": "226 Chapter 6. Stacks, Queues, and Deques\n6.1 Stacks\nAstack is a collection of objects that are inserted and removed according to the\nlast-in, ﬁrst-out (LIFO ) principle. A user may insert objects into a stack at any\ntime, but may only access or remove the most recently inserted object that remains\n(at the so-called “top” of the stack). The name “stack” is derived from the metaphor\nof a stack of plates in a spring-loaded, cafeteria plate dispenser. In this case, the\nfundamental operations involve the “pushing” and “popping” of plates on the stack.\nWhen we need a new plate from the dispenser, we “pop” the top plate off the stack,\nand when we add a plate, we “push” it down on the stack to become the new top\nplate. Perhaps an even more amusing example is a PEZ®candy dispenser, which\nstores mint candies in a spring-loaded container that “pops” out the topmost candy\nin the stack when the top of the dispenser is lifted (see Figure 6.1).\nFigure 6.1: A schematic drawing of a PEZ®dispenser; a physical implementation\nof the stack ADT. (PEZ®is a registered trademark of PEZ Candy, Inc.)\nStacks are a fundamental data structure. They are used in many applications,\nincluding the following.\nExample 6.1: Internet Web browsers store the addresses of recently visited sites\non a stack. Each time a user visits a new site, that site’s address is “pushed” onto the\nstack of addresses. The browser then allows the user to “pop” back to previously\nvisited sites using the “back” button.\nExample 6.2: Text editors usually provide an “undo” mechanism that cancels re-\ncent editing operations and reverts to former states of a document. This undo oper-\nation can be accomplished by keeping text changes in a stack.\nwww.it-ebooks.info"
  },
  {
    "page": 245,
    "content": "6.1. Stacks 227\n6.1.1 The Stack Abstract Data Type\nStacks are the simplest of all data structures, yet they are also among the most\nimportant, as they are used in a host of different applications, and as a tool for\nmany more sophisticated data structures and algorithms. Formally, a stack is an\nabstract data type (ADT) that supports the following two update methods:\npush( e):Adds element eto the top of the stack.\npop() :Removes and returns the top element from the stack\n(ornullif the stack is empty).\nAdditionally, a stack supports the following accessor methods for convenience:\ntop() :Returns the top element of the stack, without removing it\n(ornullif the stack is empty).\nsize() :Returns the number of elements in the stack.\nisEmpty() :Returns a boolean indicating whether the stack is empty.\nBy convention, we assume that elements added to the stack can have arbitrary type\nand that a newly created stack is empty.\nExample 6.3: The following table shows a series of stack operations and their\neffects on an initially empty stack Sof integers.\nMethod Return Value Stack Contents\npush(5) – (5)\npush(3) – (5, 3)\nsize() 2 (5, 3)\npop() 3 (5)\nisEmpty() false (5)\npop() 5 ()\nisEmpty() true ()\npop() null ()\npush(7) – (7)\npush(9) – (7, 9)\ntop() 9 (7, 9)\npush(4) – (7, 9, 4)\nsize() 3 (7, 9, 4)\npop() 4 (7, 9)\npush(6) – (7, 9, 6)\npush(8) – (7, 9, 6, 8)\npop() 8 (7, 9, 6)\nwww.it-ebooks.info"
  },
  {
    "page": 246,
    "content": "228 Chapter 6. Stacks, Queues, and Deques\nA Stack Interface in Java\nIn order to formalize our abstraction of a stack, we deﬁne what is known as its\napplication programming interface (API) in the form of a Java interface , which\ndescribes the names of the methods that the ADT supports and how they are to be\ndeclared and used. This interface is deﬁned in Code Fragment 6.1.\nWe rely on Java’s generics framework (described in Section 2.5.2), allow-\ning the elements stored in the stack to belong to any object type <E>. For ex-\nample, a variable representing a stack of integers could be declared with type\nStack<Integer>. The formal type parameter is used as the parameter type for\nthepush method, and the return type for both popandtop.\nRecall, from the discussion of Java interfaces in Section 2.3.1, that the interface\nserves as a type deﬁnition but that it cannot be directly instantiated. For the ADT\nto be of any use, we must provide one or more concrete classes that implement the\nmethods of the interface associated with that ADT. In the following subsections, we\nwill give two such implementations of the Stack interface: one that uses an array\nfor storage and another that uses a linked list.\nThe java.util.Stack Class\nBecause of the importance of the stack ADT, Java has included, since its original\nversion, a concrete class named java.util.Stack that implements the LIFO seman-\ntics of a stack. However, Java’s Stack class remains only for historic reasons, and\nits interface is not consistent with most other data structures in the Java library.\nIn fact, the current documentation for the Stack class recommends that it not be\nused, as LIFO functionality (and more) is provided by a more general data struc-\nture known as a double-ended queue (which we describe in Section 6.3).\nFor the sake of comparison, Table 6.1 provides a side-by-side comparison of\nthe interface for our stack ADT and the java.util.Stack class. In addition to\nsome differences in method names, we note that methods pop andpeek of the\njava.util.Stack class throw a custom EmptyStackException if called when the stack\nis empty (whereas nullis returned in our abstraction).\nOur Stack ADT Classjava.util.Stack\nsize() size()\n⇐ isEmpty() empty()\npush( e) push( e)\npop() pop()\n⇐ top() peek()\nTable 6.1: Methods of our stack ADT and corresponding methods of the class\njava.util.Stack , with differences highlighted in the right margin.\nwww.it-ebooks.info"
  },
  {
    "page": 247,
    "content": "6.1. Stacks 229\n1/∗∗\n2∗A collection of objects that are inserted and removed according to the last-in\n3∗ﬁrst-out principle. Although similar in purpose, this interface diﬀers from\n4∗java.util.Stack.\n5∗\n6∗@author Michael T. Goodrich\n7∗@author Roberto Tamassia\n8∗@author Michael H. Goldwasser\n9∗/\n10public interface Stack<E>{\n11\n12/∗∗\n13 ∗Returns the number of elements in the stack.\n14 ∗@return number of elements in the stack\n15 ∗/\n16intsize();\n17\n18/∗∗\n19 ∗Tests whether the stack is empty.\n20 ∗@return true if the stack is empty, false otherwise\n21 ∗/\n22boolean isEmpty();\n2324/∗∗\n25 ∗Inserts an element at the top of the stack.\n26 ∗@param e the element to be inserted\n27 ∗/\n28voidpush(E e);\n29\n30/∗∗\n31 ∗Returns, but does not remove, the element at the top of the stack.\n32 ∗@return top element in the stack (or null if empty)\n33 ∗/\n34E top();\n3536/∗∗\n37 ∗Removes and returns the top element from the stack.\n38 ∗@return element removed (or null if empty)\n39 ∗/\n40E pop();\n41}\nCode Fragment 6.1:\nInterface Stack documented with comments in Javadoc style\n(Section 1.9.4). Note also the use of the generic parameterized type, E, which\nallows a stack to contain elements of any speciﬁed (reference) type.\nwww.it-ebooks.info"
  },
  {
    "page": 248,
    "content": "230 Chapter 6. Stacks, Queues, and Deques\n6.1.2 A Simple Array-Based Stack Implementation\nAs our ﬁrst implementation of the stack ADT, we store elements in an array, named\ndata, with capacity Nfor some ﬁxed N. We oriented the stack so that the bottom\nelement of the stack is always stored in cell data[0], and the top element of the\nstack in cell data[t]for index tthat is equal to one less than the current size of the\nstack. (See Figure 6.2.)\n0B D E F G K L M A C\n1 2 t N−1data:\nFigure 6.2: Representing a stack with an array; the top element is in cell data[t].\nRecalling that arrays start at index 0 in Java, when the stack holds elements\nfromdata[0]todata[t]inclusive, it has size t+1. By convention, when the stack is\nempty it will have tequal to−1 (and thus has size t+1, which is 0). A complete\nJava implementation based on this strategy is given in Code Fragment 6.2 (with\nJavadoc comments omitted due to space considerations).\n1public class ArrayStack <E>implements Stack<E>{\n2public static ﬁnal int CAPACITY=1000; // default array capacity\n3privateE[ ] data; // generic array used for storage\n4private int t =−1; // index of the top element in stack\n5publicArrayStack(){this(CAPACITY);}// constructs stack with default capacity\n6publicArrayStack( intcapacity){ // constructs stack with given capacity\n7data = (E[ ]) newObject[capacity]; // safe cast; compiler may give warning\n8}\n9public int size(){return(t + 1);}\n10public boolean isEmpty(){return(t ==−1);}\n11public void push(E e) throwsIllegalStateException {\n12if(size() == data.length) throw new IllegalStateException( \"Stack is full\" );\n13data[++t] = e; // increment t before storing new item\n14}\n15publicE top(){\n16if(isEmpty()) return null ;\n17returndata[t];\n18}\n19publicE pop(){\n20if(isEmpty()) return null ;\n21E answer = data[t];\n22data[t] = null; // dereference to help garbage collection\n23t−−;\n24returnanswer;\n25}\n26}\nCode Fragment 6.2: Array-based implementation of the Stack interface.\nwww.it-ebooks.info"
  },
  {
    "page": 249,
    "content": "6.1. Stacks 231\nA Drawback of This Array-Based Stack Implementation\nThe array implementation of a stack is simple and efﬁcient. Nevertheless, this\nimplementation has one negative aspect—it relies on a ﬁxed-capacity array, which\nlimits the ultimate size of the stack.\nFor convenience, we allow the user of a stack to specify the capacity as a pa-\nrameter to the constructor (and offer a default constructor that uses capacity of\n1,000). In cases where a user has a good estimate on the number of items needing\nto go in the stack, the array-based implementation is hard to beat. However, if the\nestimate is wrong, there can be grave consequences. If the application needs much\nless space than the reserved capacity, memory is wasted. Worse yet, if an attempt\nis made to push an item onto a stack that has already reached its maximum ca-\npacity, the implementation of Code Fragment 6.2 throws an IllegalStateException ,\nrefusing to store the new element. Thus, even with its simplicity and efﬁciency, the\narray-based stack implementation is not necessarily ideal.\nFortunately, we will later demonstrate two approaches for implementing a stack\nwithout such a size limitation and with space always proportional to the actual num-\nber of elements stored in the stack. One approach, given in the next subsection uses\na singly linked list for storage; in Section 7.2.1, we will provide a more advanced\narray-based approach that overcomes the limit of a ﬁxed capacity.\nAnalyzing the Array-Based Stack Implementation\nThe correctness of the methods in the array-based implementation follows from\nour deﬁnition of index t. Note well that when pushing an element, tis incremented\nbefore placing the new element, so that it uses the ﬁrst available cell.\nTable 6.2 shows the running times for methods of this array-based stack im-\nplementation. Each method executes a constant number of statements involving\narithmetic operations, comparisons, and assignments, or calls to sizeandisEmpty ,\nwhich both run in constant time. Thus, in this implementation of the stack ADT,\neach method runs in constant time, that is, they each run in O(1)time.\nMethod Running Time\nsize O(1)\nisEmpty O(1)\ntop O(1)\npush O(1)\npop O(1)\nTable 6.2: Performance of a stack realized by an array. The space usage is O(N),\nwhere Nis the size of the array, determined at the time the stack is instantiated, and\nindependent from the number n≤Nof elements that are actually in the stack.\nwww.it-ebooks.info"
  },
  {
    "page": 250,
    "content": "232 Chapter 6. Stacks, Queues, and Deques\nGarbage Collection in Java\nWe wish to draw attention to one interesting aspect involving the implementation of\nthepopmethod in Code Fragment 6.2. We set a local variable, answer , to reference\nthe element that is being popped, and then we intentionally reset data[t]tonull at\nline 22, before decrementing t. The assignment to nullwas not technically required,\nas our stack would still operate correctly without it.\nOur reason for returning the cell to a null reference is to assist Java’s garbage\ncollection mechanism, which searches memory for objects that are no longer ac-\ntively referenced and reclaims their space for future use. (For more details, seeSection 15.1.3.) If we continued to store a reference to the popped element in ourarray, the stack class would ignore it (eventually overwriting the reference if moreelements get added to the stack). But, if there were no other active references to the\nelement in the user’s application, that spurious reference in the stack’s array would\nstop Java’s garbage collector from reclaiming the element.\nSample Usage\nWe conclude this section by providing a demonstration of code that creates and usesan instance of the ArrayStack class. In this example, we declare the parameterized\ntype of the stack as the Integer wrapper class. This causes the signature of the push\nmethod to accept an Integer instance as a parameter, and for the return type of both\ntopandpopto be anInteger . Of course, with Java’s autoboxing and unboxing (see\nSection 1.3), a primitive intcan be sent as a parameter to push .\nStack<Integer>S =newArrayStack <>();// contents: ()\nS.push(5); // contents: (5)\nS.push(3); // contents: (5, 3)\nSystem.out.println(S.size()); // contents: (5, 3) outputs 2\nSystem.out.println(S.pop()); // contents: (5) outputs 3\nSystem.out.println(S.isEmpty()); // contents: (5) outputs false\nSystem.out.println(S.pop()); // contents: () outputs 5\nSystem.out.println(S.isEmpty()); // contents: () outputs true\nSystem.out.println(S.pop()); // contents: () outputs null\nS.push(7); // contents: (7)\nS.push(9); // contents: (7, 9)\nSystem.out.println(S.top()); // contents: (7, 9) outputs 9\nS.push(4); // contents: (7, 9, 4)\nSystem.out.println(S.size()); // contents: (7, 9, 4) outputs 3\nSystem.out.println(S.pop()); // contents: (7, 9) outputs 4\nS.push(6); // contents: (7, 9, 6)\nS.push(8); // contents: (7, 9, 6, 8)\nSystem.out.println(S.pop()); // contents: (7, 9, 6) outputs 8\nCode Fragment 6.3: Sample usage of our ArrayStack class.\nwww.it-ebooks.info"
  },
  {
    "page": 251,
    "content": "6.1. Stacks 233\n6.1.3 Implementing a Stack with a Singly Linked List\nIn this section, we demonstrate how the Stack interface can be easily implemented\nusing a singly linked list for storage. Unlike our array-based implementation, the\nlinked-list approach has memory usage that is always proportional to the number\nof actual elements currently in the stack, and without an arbitrary capacity limit.\nIn designing such an implementation, we need to decide if the top of the stack\nis at the front or back of the list. There is clearly a best choice here, however, since\nwe can insert and delete elements in constant time only at the front. With the top\nof the stack stored at the front of the list, all methods execute in constant time.\nThe Adapter Pattern\nThe adapter design pattern applies to any context where we effectively want to\nmodify an existing class so that its methods match those of a related, but different,\nclass or interface. One general way to apply the adapter pattern is to deﬁne a new\nclass in such a way that it contains an instance of the existing class as a hidden\nﬁeld, and then to implement each method of the new class using methods of this\nhidden instance variable. By applying the adapter pattern in this way, we have\ncreated a new class that performs some of the same functions as an existing class,\nbut repackaged in a more convenient way.\nIn the context of the stack ADT, we can adapt our SinglyLinkedList class of\nSection 3.2.1 to deﬁne a new LinkedStack class, shown in Code Fragment 6.4.\nThis class declares a SinglyLinkedList namedlistas a private ﬁeld, and uses the\nfollowing correspondences:\nStack Method Singly Linked List Method\nsize() list.size()\nisEmpty() list.isEmpty()\npush( e) list.addFirst( e)\npop() list.removeFirst()\ntop() list.ﬁrst()\n1public class LinkedStack <E>implements Stack<E>{\n2privateSinglyLinkedList <E>list =newSinglyLinkedList <>();// an empty list\n3publicLinkedStack(){} // new stack relies on the initially empty list\n4public int size(){returnlist.size();}\n5public boolean isEmpty(){returnlist.isEmpty();}\n6public void push(E element){list.addFirst(element); }\n7publicE top(){returnlist.ﬁrst();}\n8publicE pop(){returnlist.removeFirst(); }\n9}\nCode Fragment 6.4: Implementation of a Stack using aSinglyLinkedList as storage.\nwww.it-ebooks.info"
  },
  {
    "page": 252,
    "content": "234 Chapter 6. Stacks, Queues, and Deques\n6.1.4 Reversing an Array Using a Stack\nAs a consequence of the LIFO protocol, a stack can be used as a general toll to\nreverse a data sequence. For example, if the values 1, 2, and 3 are pushed onto a\nstack in that order, they will be popped from the stack in the order 3, 2, and then 1.\nWe demonstrate this concept by revisiting the problem of reversing the elements\nof an array. (We provided a recursive algorithm for this task in Section 5.3.1.) We\ncreate an empty stack for auxiliary storage, push all of the array elements onto the\nstack, and then pop those elements off of the stack while overwriting the cells of the\narray from beginning to end. In Code Fragment 6.5, we give a Java implementation\nof this algorithm. We show an example use of this method in Code Fragment 6.6.\n1/∗∗A generic method for reversing an array. ∗/\n2public static <E>voidreverse(E[ ] a){\n3Stack<E>buﬀer = newArrayStack <>(a.length);\n4for(inti=0; i<a.length; i++)\n5buﬀer.push(a[i]);\n6for(inti=0; i<a.length; i++)\n7a[i] = buﬀer.pop();\n8}\nCode Fragment 6.5: A generic method that reverses the elements in an array with\nobjects of type E, using a stack declared with the interface Stack<E>as its type.\n1/∗∗Tester routine for reversing arrays ∗/\n2public static void main(String args[ ]) {\n3Integer[ ] a ={4, 8, 15, 16, 23, 42 };// autoboxing allows this\n4String[ ] s ={\"Jack\",\"Kate\",\"Hurley\" ,\"Jin\",\"Michael\"};\n5System.out.println( \"a = \"+ Arrays.toString(a));\n6System.out.println( \"s = \"+ Arrays.toString(s));\n7System.out.println( \"Reversing...\" );\n8reverse(a);\n9reverse(s);\n10System.out.println( \"a = \"+ Arrays.toString(a));\n11System.out.println( \"s = \"+ Arrays.toString(s));\n12}\nThe output from this method is the following:\na = [4, 8, 15, 16, 23, 42]\ns = [Jack, Kate, Hurley, Jin, Michael]\nReversing...\na = [42, 23, 16, 15, 8, 4]\ns = [Michael, Jin, Hurley, Kate, Jack]\nCode Fragment 6.6: A test of the reverse method using two arrays.\nwww.it-ebooks.info"
  },
  {
    "page": 307,
    "content": "7.5. The Java Collections Framework 289\n7.5.1 List Iterators in Java\nThejava.util.LinkedList class does not expose a position concept to users in its API,\nas we do in our positional list ADT. Instead, the preferred way to access and update\naLinkedList object in Java, without using indices, is to use a ListIterator that is\nreturned by the list’s listIterator() method. Such an iterator provides forward and\nbackward traversal methods as well as local update methods. It views its current\nposition as being before the ﬁrst element, between two elements, or after the last\nelement. That is, it uses a list cursor , much like a screen cursor is viewed as being\nlocated between two characters on a screen. Speciﬁcally, the java.util.ListIterator\ninterface includes the following methods:\nadd( e):Adds the element eat the current position of the iterator.\nhasNext() :Returns true if there is an element after the current position\nof the iterator.\nhasPrevious() :Returns true if there is an element before the current position\nof the iterator.\nprevious() :Returns the element ebefore the current position and sets\nthe current position to be before e.\nnext() :Returns the element eafter the current position and sets the\ncurrent position to be after e.\nnextIndex() :Returns the index of the next element.\npreviousIndex() :Returns the index of the previous element.\nremove() :Removes the element returned by the most recent next or\nprevious operation.\nset(e):Replaces the element returned by the most recent call to the\nnext orprevious operation with e.\nIt is risky to use multiple iterators over the same list while modifying its contents.\nIf insertions, deletions, or replacements are required at multiple “places” in a list, it\nis safer to use positions to specify these locations. But the java.util.LinkedList class\ndoes not expose its position objects to the user. So, to avoid the risks of modifying\na list that has created multiple iterators, the iterators have a “fail-fast” feature that\ninvalidates such an iterator if its underlying collection is modiﬁed unexpectedly.\nFor example, if a java.util.LinkedList object Lhas returned ﬁve different iterators\nand one of them modiﬁes L, aConcurrentModiﬁcationException is thrown if any\nof the other four is subsequently used. That is, Java allows many list iterators to\nbe traversing a linked list Lat the same time, but if one of them modiﬁes L(using\nanadd,set, orremove method), then all the other iterators for Lbecome invalid.\nLikewise, if Lis modiﬁed by one of its own update methods, then all existing\niterators for Limmediately become invalid.\nwww.it-ebooks.info"
  },
  {
    "page": 308,
    "content": "290 Chapter 7. List and Iterator ADTs\n7.5.2 Comparison to Our Positional List ADT\nJava provides functionality similar to our array list and positional lists ADT in the\njava.util.List interface, which is implemented with an array in java.util.ArrayList\nand with a linked list in java.util.LinkedList .\nMoreover, Java uses iterators to achieve a functionality similar to what our\npositional list ADT derives from positions. Table 7.4 shows corresponding meth-\nods between our (array and positional) list ADTs and the java.util interfaces List\nandListIterator interfaces, with notes about their implementations in the java.util\nclassesArrayList andLinkedList .\nPositional List java.util.List ListIteratorNotesADT Method Method Method\nsize() size() O(1)time\nisEmpty() isEmpty() O(1)time\nget(i)AisO(1),\nLisO(min{i,n−i})\nﬁrst() listIterator() ﬁrst element is next\nlast() listIterator (size() ) last element is previous\nbefore( p) previous() O(1)time\nafter( p) next() O(1)time\nset(p,e) set(e) O(1)time\nset(i,e)AisO(1),\nLisO(min{i,n−i})\nadd( i,e) O(n)time\naddFirst( e) add( 0,e) AisO(n),LisO(1)\naddFirst( e) addFirst( e) only exists in L,O(1)\naddLast( e) add( e) O(1)time\naddLast( e) addLast( e) only exists in L,O(1)\naddAfter( p,e) add( e)insertion is at cursor;\nAisO(n),LisO(1)\naddBefore( p,e) add( e)insertion is at cursor;\nAisO(n),LisO(1)\nremove( p) remove()deletion is at cursor;\nAisO(n),LisO(1)\nremove( i)AisO(1),\nLisO(min{i,n−i})\nTable 7.4: Correspondences between methods in our positional list ADT and the\njava.util interfaces List andListIterator . We use AandLas abbreviations for\njava.util.ArrayList andjava.util.LinkedList (or their running times).\nwww.it-ebooks.info"
  },
  {
    "page": 309,
    "content": "7.5. The Java Collections Framework 291\n7.5.3 List-Based Algorithms in the Java Collections Framework\nIn addition to the classes that are provided in the Java Collections Framework, there\nare a number of simple algorithms that it provides as well. These algorithms are\nimplemented as static methods in the java.util.Collections class (not to be confused\nwith thejava.util.Collection interface) and they include the following methods:\ncopy( Ldest,Lsrc):Copies all elements of the Lsrclist into corresponding in-\ndices of the Ldestlist.\ndisjoint( C,D):Returns a boolean value indicating whether the collections\nCandDare disjoint.\nﬁll(L,e):Replaces each element of the list Lwith element e.\nfrequency( C,e):Returns the number of elements in the collection Cthat are\nequal to e.\nmax( C):Returns the maximum element in the collection C, based on\nthe natural ordering of its elements.\nmin( C):Returns the minimum element in the collection C, based on\nthe natural ordering of its elements.\nreplaceAll( L,e,f):Replaces each element in Lthat is equal to ewith element f.\nreverse( L):Reverses the ordering of elements in the list L.\nrotate( L,d):Rotates the elements in the list Lby the distance d(which\ncan be negative), in a circular fashion.\nshuﬄe( L):Pseudorandomly permutes the ordering of the elements in\nthe list L.\nsort( L):Sorts the list L, using the natural ordering of its elements.\nswap( L,i,j):Swap the elements at indices iandjof list L.\nwww.it-ebooks.info"
  },
  {
    "page": 310,
    "content": "292 Chapter 7. List and Iterator ADTs\nConverting Lists into Arrays\nLists are a beautiful concept and they can be applied in a number of different con-\ntexts, but there are some instances where it would be useful if we could treat a list\nlike an array. Fortunately, the java.util.Collection interface includes the following\nhelpful methods for generating an array that has the same elements as the given\ncollection:\ntoArray() :Returns an array of elements of type Object containing\nall the elements in this collection.\ntoArray( A):Returns an array of elements of the same element type asAcontaining all the elements in this collection.\nIf the collection is a list, then the returned array will have its elements stored in thesame order as that of the original list. Thus, if we have a useful array-based methodthat we want to use on a list or other type of collection, then we can do so by simplyusing that collection’s toArray() method to produce an array representation of that\ncollection.\nConverting Arrays into Lists\nIn a similar vein, it is often useful to be able to convert an array into an equivalent\nlist. Fortunately, the java.util.Arrays class includes the following method:\nasList( A):Returns a list representation of the array A, with the same\nelement type as the elements of A.\nThe list returned by this method uses the array Aas its internal representation for\nthe list. So this list is guaranteed to be an array-based list and any changes made to\nit will automatically be reﬂected in A. Because of these types of side effects, use of\ntheasList method should always be done with caution, so as to avoid unintended\nconsequences. But, used with care, this method can often save us a lot of work. Forinstance, the following code fragment could be used to randomly shufﬂe an arrayofInteger objects,arr:\nInteger[ ] arr ={1, 2, 3, 4, 5, 6, 7, 8 }; // allowed by autoboxing\nList<Integer>listArr = Arrays.asList(arr);\nCollections.shuﬄe(listArr); // this has side eﬀect of shuﬄing arr\nIt is worth noting that the array Asent to the asList method should be a reference\ntype (hence, our use of Integer rather than intin the above example). This is\nbecause the Listinterface is generic, and requires that the element type be an object.\nwww.it-ebooks.info"
  },
  {
    "page": 311,
    "content": "7.6. Sorting a Positional List 293\n7.6 Sorting a Positional List\nIn Section 3.1.2, we introduced the insertion-sort algorithm in the context of an\narray-based sequence. In this section, we develop an implementation that operates\non aPositionalList , relying on the same high-level algorithm in which each element\nis placed relative to a growing collection of previously sorted elements.\nWe maintain a variable named marker that represents the rightmost position of\nthe currently sorted portion of a list. During each pass, we consider the position just\npast the marker as the pivot and consider where the pivot’s element belongs relative\nto the sorted portion; we use another variable, named walk, to move leftward from\nthe marker, as long as there remains a preceding element with value larger than the\npivot’s. A typical conﬁguration of these variables is diagrammed in Figure 7.9. A\nJava implementation of this strategy is given in Code 7.15.\n15 22 25 29 36 23 53 11 42\nmarkerpivot walk\nFigure 7.9: Overview of one step of our insertion-sort algorithm. The shaded ele-\nments, those up to and including marker , have already been sorted. In this step, the\npivot’s element should be relocated immediately before the walk position.\n1/∗∗Insertion-sort of a positional list of integers into nondecreasing order ∗/\n2public static void insertionSort(PositionalList <Integer>list){\n3Position<Integer>marker = list.ﬁrst(); // last position known to be sorted\n4while(marker != list.last()) {\n5Position<Integer>pivot = list.after(marker);\n6intvalue = pivot.getElement(); // number to be placed\n7if(value>marker.getElement()) // pivot is already sorted\n8 marker = pivot;\n9else{ // must relocate pivot\n10 Position<Integer>walk = marker; // ﬁnd leftmost item greater than value\n11 while(walk != list.ﬁrst() && list.before(walk).getElement() >value)\n12 walk = list.before(walk);\n13 list.remove(pivot); // remove pivot entry and\n14 list.addBefore(walk, value); // reinsert value in front of walk\n15}\n16}\n17}\nCode Fragment 7.15: Java code for performing insertion-sort on a positional list.\nwww.it-ebooks.info"
  },
  {
    "page": 312,
    "content": "294 Chapter 7. List and Iterator ADTs\n7.7 Case Study: Maintaining Access Frequencies\nThe positional list ADT is useful in a number of settings. For example, a program\nthat simulates a game of cards could model each person’s hand as a positional list\n(Exercise P-7.60). Since most people keep cards of the same suit together, inserting\nand removing cards from a person’s hand could be implemented using the methods\nof the positional list ADT, with the positions being determined by a natural order\nof the suits. Likewise, a simple text editor embeds the notion of positional insertion\nand deletion, since such editors typically perform all updates relative to a cursor ,\nwhich represents the current position in the list of characters of text being edited.\nIn this section, we will consider maintaining a collection of elements while\nkeeping track of the number of times each element is accessed. Keeping such access\ncounts allows us to know which elements are among the most popular. Examples\nof such scenarios include a Web browser that keeps track of a user’s most accessed\npages, or a music collection that maintains a list of the most frequently played\nsongs for a user. We will model this with a new favorites list ADT that supports the\nsizeandisEmpty methods as well as the following:\naccess( e):Accesses the element e, adding it to the favorites list if it is\nnot already present, and increments its access count.\nremove( e):Removes element efrom the favorites list, if present.\ngetFavorites( k):Returns an iterable collection of the kmost accessed elements.\n7.7.1 Using a Sorted List\nOur ﬁrst approach for managing a list of favorites is to store elements in a linked\nlist, keeping them in nonincreasing order of access counts. We access or remove\nan element by searching the list from the most frequently accessed to the least\nfrequently accessed. Reporting the kmost accessed elements is easy, as they are\nthe ﬁrst kentries of the list.\nTo maintain the invariant that elements are stored in nonincreasing order of\naccess counts, we must consider how a single access operation may affect the order.\nThe accessed element’s count increases by one, and so it may become larger than\none or more of its preceding neighbors in the list, thereby violating the invariant.\nFortunately, we can reestablish the sorted invariant using a technique similar to\na single pass of the insertion-sort algorithm, introduced in the previous section. We\ncan perform a backward traversal of the list, starting at the position of the element\nwhose access count has increased, until we locate a valid position after which the\nelement can be relocated.\nwww.it-ebooks.info"
  },
  {
    "page": 313,
    "content": "7.7. Case Study: Maintaining Access Frequencies 295\nUsing the Composition Pattern\nWe wish to implement a favorites list by making use of a PositionalList for storage.\nIf elements of the positional list were simply elements of the favorites list, we would\nbe challenged to maintain access counts and to keep the proper count with theassociated element as the contents of the list are reordered. We use a general object-oriented design pattern, the composition pattern , in which we deﬁne a single object\nthat is composed of two or more other objects. (See, for example, Section 2.5.2.)\nSpeciﬁcally, we deﬁne a nonpublic nested class, Item , that stores the element\nand its access count as a single instance. We then maintain our favorites list asaPositionalList ofitem instances, so that the access count for a user’s element is\nembedded alongside it in our representation. (An Item is never exposed to a user\nof aFavoritesList .)\n1/∗∗Maintains a list of elements ordered according to access frequency. ∗/\n2public class FavoritesList <E>{\n3// ---------------- nested Item class ----------------\n4protected static class Item<E>{\n5privateE value;\n6private int count = 0;\n7/∗∗Constructs new item with initial count of zero. ∗/\n8publicItem(E val){value = val;}\n9public int getCount(){returncount;}\n10publicE getValue(){returnvalue;}\n11public void increment(){count++;}\n12}//----------- end of nested Item class -----------\n13\n14PositionalList <Item<E>>list =newLinkedPositionalList <>();// list of Items\n15publicFavoritesList(){} // constructs initially empty favorites list\n16\n17// nonpublic utilities\n18/∗∗Provides shorthand notation to retrieve user 's element stored at Position p. ∗/\n19protected E value(Position <Item<E>>p){returnp.getElement().getValue(); }\n2021/∗∗Provides shorthand notation to retrieve count of item stored at Position p. ∗/\n22protected int count(Position <Item<E>>p){returnp.getElement().getCount();}\n23\n24/∗∗Returns Position having element equal to e (or null if not found). ∗/\n25protected Position<Item<E>>ﬁndPosition(E e){\n26Position<Item<E>>walk = list.ﬁrst();\n27while(walk != null&& !e.equals(value(walk)))\n28 walk = list.after(walk);\n29returnwalk;\n30}\nCode Fragment 7.16:\nClassFavoritesList . (Continues in Code Fragment 7.17.)\nwww.it-ebooks.info"
  },
  {
    "page": 314,
    "content": "296 Chapter 7. List and Iterator ADTs\n31/∗∗Moves item at Position p earlier in the list based on access count. ∗/\n32protected void moveUp(Position <Item<E>>p){\n33intcnt = count(p); // revised count of accessed item\n34Position<Item<E>>walk = p;\n35while(walk != list.ﬁrst() && count(list.before(walk)) <cnt)\n36 walk = list.before(walk); // found smaller count ahead of item\n37if(walk != p)\n38 list.addBefore(walk, list.remove(p)); // remove/reinsert item\n39}\n40\n41// public methods\n42/∗∗Returns the number of items in the favorites list. ∗/\n43public int size(){returnlist.size();}\n44\n45/∗∗Returns true if the favorites list is empty. ∗/\n46public boolean isEmpty(){returnlist.isEmpty();}\n47\n48/∗∗Accesses element e (possibly new), increasing its access count. ∗/\n49public void access(E e){\n50Position<Item<E>>p = ﬁndPosition(e); // try to locate existing element\n51if(p ==null)\n52 p = list.addLast( newItem<E>(e)); // if new, place at end\n53p.getElement().increment(); // always increment count\n54moveUp(p); // consider moving forward\n55}\n5657/∗∗Removes element equal to e from the list of favorites (if found). ∗/\n58public void remove(E e){\n59Position<Item<E>>p = ﬁndPosition(e); // try to locate existing element\n60if(p !=null)\n61 list.remove(p);\n62}\n63\n64/∗∗Returns an iterable collection of the k most frequently accessed elements. ∗/\n65publicIterable<E>getFavorites( intk)throwsIllegalArgumentException {\n66if(k<0||k>size())\n67 throw new IllegalArgumentException( \"Invalid k\" );\n68PositionalList <E>result =newLinkedPositionalList <>();\n69Iterator<Item<E>>iter = list.iterator();\n70for(intj=0; j<k; j++)\n71 result.addLast(iter.next().getValue());\n72returnresult;\n73}\n74}\nCode Fragment 7.17:\nClassFavoritesList . (Continued from Code Fragment 7.16.)\nwww.it-ebooks.info"
  },
  {
    "page": 315,
    "content": "7.7. Case Study: Maintaining Access Frequencies 297\n7.7.2 Using a List with the Move-to-Front Heuristic\nThe previous implementation of a favorites list performs the access( e)method in\ntime proportional to the index of ein the favorites list. That is, if eis the kthmost\npopular element in the favorites list, then accessing it takes O(k)time. In many\nreal-life access sequences (e.g., Web pages visited by a user), once an element is\naccessed it is more likely to be accessed again in the near future. Such scenarios\nare said to possess locality of reference .\nAheuristic , or rule of thumb, that attempts to take advantage of the locality of\nreference that is present in an access sequence is the move-to-front heuristic . To\napply this heuristic, each time we access an element we move it all the way to the\nfront of the list. Our hope, of course, is that this element will be accessed again in\nthe near future. Consider, for example, a scenario in which we have nelements and\nthe following series of n2accesses:\n•element 1 is accessed ntimes.\n•element 2 is accessed ntimes.\n•···\n•element nis accessed ntimes.\nIf we store the elements sorted by their access counts, inserting each element the\nﬁrst time it is accessed, then\n•each access to element 1 runs in O(1)time.\n•each access to element 2 runs in O(2)time.\n•···\n•each access to element nruns in O(n)time.\nThus, the total time for performing the series of accesses is proportional to\nn+2n+3n+···+n·n=n(1+2+3+···+n)=n·n(n+1)\n2,\nwhich is O(n3).\nOn the other hand, if we use the move-to-front heuristic, inserting each element\nthe ﬁrst time it is accessed, then\n•each subsequent access to element 1 takes O(1)time.\n•each subsequent access to element 2 takes O(1)time.\n•···\n•each subsequent access to element nruns in O(1)time.\nSo the running time for performing all the accesses in this case is O(n2). Thus,\nthe move-to-front implementation has faster access times for this scenario. Still,\nthe move-to-front approach is just a heuristic, for there are access sequences where\nusing the move-to-front approach is slower than simply keeping the favorites list\nordered by access counts.\nwww.it-ebooks.info"
  },
  {
    "page": 316,
    "content": "298 Chapter 7. List and Iterator ADTs\nThe Trade-Oﬀs with the Move-to-Front Heuristic\nIf we no longer maintain the elements of the favorites list ordered by their access\ncounts, when we are asked to ﬁnd the kmost accessed elements, we need to search\nfor them. We will implement the getFavorites( k)method as follows:\n1.We copy all entries of our favorites list into another list, named temp .\n2.We scan the temp listktimes. In each scan, we ﬁnd the entry with the largest\naccess count, remove this entry from temp , and add it to the results.\nThis implementation of method getFavorites( k)takes O(kn)time. Thus, when kis\na constant, method getFavorites( k)runs in O(n)time. This occurs, for example,\nwhen we want to get the “top ten” list. However, if kis proportional to n, then the\nmethodgetFavorites( k)runs in O(n2)time. This occurs, for example, when we\nwant a “top 25%” list.\nIn Chapter 9 we will introduce a data structure that will allow us to implement\ngetFavorites inO(n+klogn)time (see Exercise P-9.51), and more advanced tech-\nniques could be used to perform getFavorites inO(n+klogk)time.\nWe could easily achieve O(nlogn)time if we use a standard sorting algorithm\nto reorder the temporary list before reporting the top k(see Chapter 12); this ap-\nproach would be preferred to the original in the case that kisΩ(logn). (Recall the\nbig-Omega notation introduced in Section 4.3.1 to give an asymptotic lower boundon the running time of an algorithm.) There is a specialized sorting algorithm (seeSection 12.3.2) that can take advantage of the fact that access counts are integers in\norder to achieve O(n)time forgetFavorites , for any value of k.\nImplementing the Move-to-Front Heuristic in Java\nWe give an implementation of a favorites list using the move-to-front heuristic in\nCode Fragment 7.18. The new FavoritesListMTF class inherits most of its func-\ntionality from the original FavoritesList as a base class.\nBy our original design, the access method of the original class relies on a pro-\ntected utility named moveUp to enact the potential shifting of an element forward\nin the list, after its access count had been incremented. Therefore, we implementthe move-to-front heuristic by simply overriding the moveUp method so that each\naccessed element is moved directly to the front of the list (if not already there).This action is easily implemented by means of the positional list ADT.\nThe more complex portion of our FavoritesListMTF class is the new deﬁni-\ntion for the getFavorites method. We rely on the ﬁrst of the approaches outlined\nabove, inserting copies of the items into a temporary list and then repeatedly ﬁnd-ing, reporting, and removing an element that has the largest access count of thoseremaining.\nwww.it-ebooks.info"
  },
  {
    "page": 317,
    "content": "7.7. Case Study: Maintaining Access Frequencies 299\n1/∗∗Maintains a list of elements ordered with move-to-front heuristic. ∗/\n2public class FavoritesListMTF <E>extends FavoritesList <E>{\n3\n4/∗∗Moves accessed item at Position p to the front of the list. ∗/\n5protected void moveUp(Position <Item<E>>p){\n6if(p != list.ﬁrst())\n7 list.addFirst(list.remove(p)); // remove/reinsert item\n8}\n9\n10/∗∗Returns an iterable collection of the k most frequently accessed elements. ∗/\n11publicIterable<E>getFavorites( intk)throwsIllegalArgumentException {\n12if(k<0||k>size())\n13 throw new IllegalArgumentException( \"Invalid k\" );\n1415// we begin by making a copy of the original list\n16PositionalList <Item<E>>temp =newLinkedPositionalList <>();\n17for(Item<E>item : list)\n18 temp.addLast(item);\n1920// we repeated ﬁnd, report, and remove element with largest count\n21PositionalList <E>result =newLinkedPositionalList <>();\n22for(intj=0; j<k; j++){\n23 Position<Item<E>>highPos = temp.ﬁrst();\n24 Position<Item<E>>walk = temp.after(highPos);\n25 while(walk != null){\n26 if(count(walk) >count(highPos))\n27 highPos = walk;\n28 walk = temp.after(walk);\n29}\n30 // we have now found element with highest count\n31 result.addLast(value(highPos));\n32 temp.remove(highPos);\n33}\n34returnresult;\n35}\n36}\nCode Fragment 7.18:\nClassFavoritesListMTF implementing the move-to-front\nheuristic. This class extends FavoritesList (Code Fragments 7.16 and 7.17) and\noverrides methods moveUp andgetFavorites .\nwww.it-ebooks.info"
  },
  {
    "page": 318,
    "content": "300 Chapter 7. List and Iterator ADTs\n7.8 Exercises\nReinforcement\nR-7.1 Draw a representation, akin to Example 7.1, of an initially empty list Lafter per-\nforming the following sequence of operations: add( 0,4),add( 0,3),add( 0,2),\nadd( 2,1),add( 1,5),add( 1,6),add( 3,7),add( 0,8).\nR-7.2 Give an implementation of the stack ADT using an array list for storage.\nR-7.3 Give an implementation of the deque ADT using an array list for storage.\nR-7.4 Give a justiﬁcation of the running times shown in Table 7.1 for the methods of\nan array list implemented with a (nonexpanding) array.\nR-7.5 Thejava.util.ArrayList includes a method, trimToSize() , that replaces the un-\nderlying array with one whose capacity precisely equals the number of elements\ncurrently in the list. Implement such a method for our dynamic version of the\nArrayList class from Section 7.2.\nR-7.6 Redo the justiﬁcation of Proposition 7.2 assuming that the the cost of growing\nthe array from size kto size 2 kis 3kcyber-dollars. How much should each push\noperation be charged to make the amortization work?\nR-7.7 Consider an implementation of the array list ADT using a dynamic array, but\ninstead of copying the elements into an array of double the size (that is, from Nto\n2N) when its capacity is reached, we copy the elements into an array with ⌈N/4⌉\nadditional cells, going from capacity NtoN+⌈N/4⌉. Show that performing a\nsequence of npush operations (that is, insertions at the end) still runs in O(n)\ntime in this case.\nR-7.8 Suppose we are maintaining a collection Cof elements such that, each time we\nadd a new element to the collection, we copy the contents of Cinto a new array\nlist of just the right size. What is the running time of adding nelements to an\ninitially empty collection Cin this case?\nR-7.9 Theaddmethod for a dynamic array, as described in Code Fragment 7.5, has\nthe following inefﬁciency. In the case when a resize occurs, the resize operation\ntakes time to copy all the elements from the old array to a new array, and then\nthe subsequent loop in the body of addshifts some of them to make room for a\nnew element. Give an improved implementation of the addmethod, so that, in\nthe case of a resize, the elements are copied into their ﬁnal place in the new array\n(that is, no shifting is done).\nR-7.10 Reimplement the ArrayStack class, from Section 6.1.2, using dynamic arrays to\nsupport unlimited capacity.\nR-7.11 Describe an implementation of the positional list methods addLast andaddBe-\nforerealized by using only methods in the set {isEmpty ,ﬁrst,last,before ,after ,\naddAfter ,addFirst}.\nwww.it-ebooks.info"
  },
  {
    "page": 319,
    "content": "7.8. Exercises 301\nR-7.12 Suppose we want to extend the PositionalList abstract data type with a method,\nindexOf( p), that returns the current index of the element stored at position p.\nShow how to implement this method using only other methods of the Positional-\nListinterface (not details of our LinkedPositionalList implementation).\nR-7.13 Suppose we want to extend the PositionalList abstract data type with a method,\nﬁndPosition( e), that returns the ﬁrst position containing an element equal to\ne(ornull if no such position exists). Show how to implement this method\nusing only existing methods of the PositionalList interface (not details of our\nLinkedPositionalList implementation).\nR-7.14 TheLinkedPositionalList implementation of Code Fragments 7.9–7.12 does not\ndo any error checking to test if a given position pis actually a member of the\nrelevant list. Give a detailed explanation of the effect of a call L.addAfter( p,e)\non a list L, yet with a position pthat belongs to some other list M.\nR-7.15 To better model a FIFO queue in which entries may be deleted before reach-\ning the front, design a LinkedPositionalQueue class that supports the complete\nqueue ADT, yet with enqueue returning a position instance and support for a\nnew method, remove(p) , that removes the element associated with position p\nfrom the queue. You may use the adapter design pattern (Section 6.1.3), using a\nLinkedPositionalList as your storage.\nR-7.16 Describe how to implement a method, alternateIterator() , for a positional list\nthat returns an iterator that reports only those elements having even index in the\nlist.\nR-7.17 Redesign the Progression class, from Section 2.2.3, so that it formally imple-\nments the Iterator<long>interface.\nR-7.18 Thejava.util.Collection interface includes a method, contains( o), that returns\ntrue if the collection contains any object that equals Object o. Implement such a\nmethod in the ArrayList class of Section 7.2.\nR-7.19 Thejava.util.Collection interface includes a method, clear() , that removes all\nelements from a collection. Implement such a method in the ArrayList class of\nSection 7.2.\nR-7.20 Demonstrate how to use the java.util.Colletions.reverse method to reverse an ar-\nray of objects.\nR-7.21 Given the set of element {a,b,c,d,e,f}stored in a list, show the ﬁnal state of\nthe list, assuming we use the move-to-front heuristic and access the elementsaccording to the following sequence: (a,b,c,d,e,f,a,c,f,b,d,e).\nR-7.22 Suppose that we have made kntotal accesses to the elements in a list Lofn\nelements, for some integer k≥1. What are the minimum and maximum number\nof elements that have been accessed fewer than ktimes?\nR-7.23 LetLbe a list of nitems maintained according to the move-to-front heuristic.\nDescribe a series of O(n)accesses that will reverse L.\nR-7.24 Implement a resetCounts() method for the FavoritesList class that resets all ele-\nments’ access counts to zero (while leaving the order of the list unchanged).\nwww.it-ebooks.info"
  },
  {
    "page": 320,
    "content": "302 Chapter 7. List and Iterator ADTs\nCreativity\nC-7.25 Give an array-based list implementation, with ﬁxed capacity, treating the array\ncircularly so that it achieves O(1)time for insertions and removals at index 0, as\nwell as insertions and removals at the end of the array list. Your implementation\nshould also provide for a constant-time getmethod.\nC-7.26 Complete the previous exercise, except using a dynamic array to provide un-\nbounded capacity.\nC-7.27 Modify our ArrayList implementation to support the Cloneable interface, as de-\nscribed in Section 3.6.\nC-7.28 In Section 7.5.3, we demonstrated how the Collections.shuﬄe method can be\nadapted to shufﬂe a reference-type array. Give a direct implementation of a\nshuﬄe method for an array of intvalues. You may use the method, nextInt( n)\nof theRandom class, which returns a random number between 0 and n−1, in-\nclusive. Your method should guarantee that every possible ordering is equally\nlikely. What is the running time of your method?\nC-7.29 Revise the array list implementation given in Section 7.2.1 so that when the ac-\ntual number of elements, n, in the array goes below N/4, where Nis the array\ncapacity, the array shrinks to half its size.\nC-7.30 Prove that when using a dynamic array that grows and shrinks as in the previous\nexercise, the following series of 2 noperations takes O(n)time: ninsertions at\nthe end of an initially empty list, followed by ndeletions, each from the end of\nthe list.\nC-7.31 Give a formal proof that any sequence of npush or pop operations (that is, in-\nsertions or deletions at the end) on an initially empty dynamic array takes O(n)\ntime, if using the strategy described in Exercise C-7.29.\nC-7.32 Consider a variant of Exercise C-7.29, in which an array of capacity Nis resized\nto capacity precisely that of the number of elements, any time the number of\nelements in the array goes strictly below N/4. Give a formal proof that any\nsequence of npush or pop operations on an initially empty dynamic array takes\nO(n)time.\nC-7.33 Consider a variant of Exercise C-7.29, in which an array of capacity N, is resized\nto capacity precisely that of the number of elements, any time the number of\nelements in the array goes strictly below N/2. Show that there exists a sequence\nofnpush and pop operations that requires Ω(n2)time to execute.\nC-7.34 Describe how to implement the queue ADT using two stacks as instance vari-\nables, such that all queue operations execute in amortized O(1)time. Give a\nformal proof of the amortized bound.\nC-7.35 Reimplement the ArrayQueue class, from Section 6.2.2, using dynamic arrays to\nsupport unlimited capacity. Be especially careful about the treatment of a circular\narray when resizing.\nwww.it-ebooks.info"
  },
  {
    "page": 321,
    "content": "7.8. Exercises 303\nC-7.36 Suppose we want to extend the PositionalList interface to include a method,\npositionAtIndex( i), that returns the position of the element having index i(or\nthrows an IndexOutOfBoundsException , if warranted). Show how to imple-\nment this method, using only existing methods of the PositionalList interface,\nby traversing the appropriate number of steps from the front of the list.\nC-7.37 Repeat the previous problem, but use knowledge of the size of the list to traverse\nfrom the end of the list that is closest to the desired index.\nC-7.38 Explain how any implementation of the PositionalList ADT can be made to sup-\nport all methods of the ListADT, described in Section 7.1, assuming an imple-\nmentation is given for the positionAtIndex( i)method, proposed in Exercise C-\n7.36.\nC-7.39 Suppose we want to extend the PositionalList abstract data type with a method,\nmoveToFront( p), that moves the element at position pto the front of a list (if\nnot already there), while keeping the relative order of the remaining elements un-\nchanged. Show how to implement this method using only existing methods of thePositionalList interface (not details of our LinkedPositionalList implementation).\nC-7.40 Redo the previous problem, but providing an implementation within the classLinkedPositionalList that does not create or destroy any nodes.\nC-7.41 Modify our LinkedPositionalList implementation to support the Cloneable inter-\nface, as described in Section 3.6.\nC-7.42 Describe a nonrecursive method for reversing a positional list represented with adoubly linked list using a single pass through the list.\nC-7.43 Page 281 describes an array-based representation for implementing the posi-\ntional list ADT. Give a pseudocode description of the addBefore method for that\nrepresentation.\nC-7.44 Describe a method for performing a card shufﬂe of a list of 2 nelements, by\nconverting it into two lists. A card shufﬂe is a permutation where a list Lis cut\ninto two lists, L\n1andL2, where L1is the ﬁrst half of LandL2is the second half\nofL, and then these two lists are merged into one by taking the ﬁrst element in\nL1, then the ﬁrst element in L2, followed by the second element in L1, the second\nelement in L2, and so on.\nC-7.45 How might the LinkedPositionalList class be redesigned to detect the error de-\nscribed in Exercise R-7.14.\nC-7.46 Modify the LinkedPositionalList class to support a method swap( p,q)that causes\nthe underlying nodes referenced by positions pandqto be exchanged for each\nother. Relink the existing nodes; do not create any new nodes.\nC-7.47 An array is sparse if most of its entries are null. A list Lcan be used to implement\nsuch an array, A, efﬁciently. In particular, for each nonnull cell A[i], we can store\na pair(i,e)inL, where eis the element stored at A[i]. This approach allows us to\nrepresent Ausing O(m)storage, where mis the number of nonnull entries in A.\nDescribe and analyze efﬁcient ways of performing the methods of the array list\nADT on such a representation.\nwww.it-ebooks.info"
  },
  {
    "page": 322,
    "content": "304 Chapter 7. List and Iterator ADTs\nC-7.48 Design a circular positional list ADT that abstracts a circularly linked list in the\nsame way that the positional list ADT abstracts a doubly linked list.\nC-7.49 Provide an implementation of the listiterator() method, in the context of the class\nLinkedPositionalList , that returns an object that supports the java.util.ListIterator\ninterface described in Section 7.5.1.\nC-7.50 Describe a scheme for creating list iterators that fail fast , that is, they all become\ninvalid as soon as the underlying list changes.\nC-7.51 There is a simple algorithm, called bubble-sort , for sorting a list Lofncompa-\nrable elements. This algorithm scans the list n−1 times, where, in each scan,\nthe algorithm compares the current element with the next one and swaps them\nif they are out of order. Give a pseudocode description of bubble-sort that is as\nefﬁcient as possible assuming Lis implemented with a doubly linked list. What\nis the running time of this algorithm?\nC-7.52 Redo Exercise C-7.51 assuming Lis implemented with an array list.\nC-7.53 Describe an efﬁcient method for maintaining a favorites list L, with the move-to-\nfront heuristic, such that elements that have not been accessed in the most recent\nnaccesses are automatically purged from the list.\nC-7.54 Suppose we have an n-element list Lmaintained according to the move-to-front\nheuristic. Describe a sequence of n2accesses that is guaranteed to take Ω(n3)\ntime to perform on L.\nC-7.55 A useful operation in databases is the natural join . If we view a database as a list\nofordered pairs of objects, then the natural join of databases AandBis the list\nof all ordered triples (x,y,z)such that the pair (x,y)is in Aand the pair (y,z)is\ninB. Describe and analyze an efﬁcient algorithm for computing the natural join\nof a list Aofnpairs and a list Bofmpairs.\nC-7.56 When Bob wants to send Alice a message Mon the Internet, he breaks Minto\nndata packets , numbers the packets consecutively, and injects them into the\nnetwork. When the packets arrive at Alice’s computer, they may be out of order,\nso Alice must assemble the sequence of npackets in order before she can be sure\nshe has the entire message. Describe an efﬁcient scheme for Alice to do this.\nWhat is the running time of this algorithm?\nC-7.57 Implement the FavoritesList class using an array list.\nProjects\nP-7.58 Develop an experiment, using techniques similar to those in Section 4.1, to test\nthe efﬁciency of nsuccessive calls to the addmethod of an ArrayList , for vari-\nousn, under each of the following three scenarios:\na.Each add takes place at index 0.\nb.Each add takes place at index size()/2 .\nc.Each add takes place at index size() .\nAnalyze your empirical results.\nwww.it-ebooks.info"
  },
  {
    "page": 323,
    "content": "Chapter Notes 305\nP-7.59 Reimplement the LinkedPositionalList class so that an invalid position is reported\nin a scenario such as the one described in Exercise R-7.14.\nP-7.60 Implement a CardHand class that supports a person arranging a group of cards\nin his or her hand. The simulator should represent the sequence of cards using\na single positional list ADT so that cards of the same suit are kept together. Im-\nplement this strategy by means of four “ﬁngers” into the hand, one for each of\nthe suits of hearts, clubs, spades, and diamonds, so that adding a new card to the\nperson’s hand or playing a correct card from the hand can be done in constant\ntime. The class should support the following methods:\n•addCard( r,s): Add a new card with rank rand suit sto the hand.\n•play( s): Remove and return a card of suit sfrom the player’s hand; if there\nis no card of suit s, then remove and return an arbitrary card from the hand.\n•iterator() : Return an iterator for all cards currently in the hand.\n•suitIterator( s): Return an iterator for all cards of suit sthat are currently in\nthe hand.\nP-7.61 Write a simple text editor, which stores and displays a string of characters using\nthe positional list ADT, together with a cursor object that highlights a position in\nthe string. The editor must support the following operations:\n•left: Move cursor left one character (do nothing if at beginning).\n•right : Move cursor right one character (do nothing if at end).\n•insert c: Insert the character cjust after the cursor.\n•delete : Delete the character just after the cursor (if not at end).\nChapter Notes\nThe treatment of data structures as collections (and other principles of object-oriented de-\nsign) can be found in object-oriented design books by Booch [16], Budd [19], and Liskov\nand Guttag [67]. Lists and iterators are pervasive concepts in the Java Collections Frame-\nwork. Our positional list ADT is derived from the “position” abstraction introduced by\nAho, Hopcroft, and Ullman [6], and the list ADT of Wood [96]. Implementations of lists\nvia arrays and linked lists are discussed by Knuth [60].\nwww.it-ebooks.info"
  },
  {
    "page": 324,
    "content": "www.it-ebooks.info"
  },
  {
    "page": 325,
    "content": "Chapter\n8Trees\nContents\n8.1 General Trees . . . . . . . . . . . . . . . . . . . . . . . . . 308\n8.1.1 Tree Deﬁnitions and Properties . . . . . . . . . . . . . . . 309\n8.1.2 The Tree Abstract Data Type . . . . . . . . . . . . . . . 312\n8.1.3 Computing Depth and Height . . . . . . . . . . . . . . . . 314\n8.2 Binary Trees . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n8.2.1 The Binary Tree Abstract Data Type . . . . . . . . . . . . 319\n8.2.2 Properties of Binary Trees . . . . . . . . . . . . . . . . . 321\n8.3 Implementing Trees . . . . . . . . . . . . . . . . . . . . . . 323\n8.3.1 Linked Structure for Binary Trees . . . . . . . . . . . . . . 323\n8.3.2 Array-Based Representation of a Binary Tree . . . . . . . 331\n8.3.3 Linked Structure for General Trees . . . . . . . . . . . . . 333\n8.4 Tree Traversal Algorithms . . . . . . . . . . . . . . . . . . . 334\n8.4.1 Preorder and Postorder Traversals of General Trees . . . . 334\n8.4.2 Breadth-First Tree Traversal . . . . . . . . . . . . . . . . 336\n8.4.3 Inorder Traversal of a Binary Tree . . . . . . . . . . . . . 337\n8.4.4 Implementing Tree Traversals in Java . . . . . . . . . . . 339\n8.4.5 Applications of Tree Traversals . . . . . . . . . . . . . . . 343\n8.4.6 Euler Tours . . . . . . . . . . . . . . . . . . . . . . . . . 348\n8.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\nwww.it-ebooks.info"
  },
  {
    "page": 326,
    "content": "308 Chapter 8. Trees\n8.1 General Trees\nProductivity experts say that breakthroughs come by thinking “nonlinearly.” In\nthis chapter, we will discuss one of the most important nonlinear data structures in\ncomputing— trees . Tree structures are indeed a breakthrough in data organization,\nfor they allow us to implement a host of algorithms much faster than when using\nlinear data structures, such as arrays or linked lists. Trees also provide a natural\norganization for data, and consequently have become ubiquitous structures in ﬁle\nsystems, graphical user interfaces, databases, websites, and many other computer\nsystems.\nIt is not always clear what productivity experts mean by “nonlinear” thinking,\nbut when we say that trees are “nonlinear,” we are referring to an organizational\nrelationship that is richer than the simple “before” and “after” relationships be-\ntween objects in sequences. The relationships in a tree are hierarchical , with some\nobjects being “above” and some “below” others. Actually, the main terminology\nfor tree data structures comes from family trees, with the terms “parent,” “child,”\n“ancestor,” and “descendant” being the most common words used to describe rela-\ntionships. We show an example of a family tree in Figure 8.1.\nEldaahNebaioth\nKedar\nAdbeel\nMibsam\nMishma\nDumah\nMassa\nHadad\nTema\nJetur\nNaphish\nKedemahIshmael\nGadNaphtaliDanJudahLeviSimeon\nAsher\nIssachar\nZebulun\nDinah\nJoseph\nBenjaminEliphaz\nReuel\nJeush\nJalam\nReubenKorah\nJacob (Israel)Esau\nIsaac\nZimran\nJokshan\nMedan\nMidian\nIshbak\nShuahAbraham\nSheba\nDedan\nEphah\nEpher\nHanoch\nAbida\nFigure 8.1: A family tree showing some descendants of Abraham, as recorded in\nGenesis, chapters 25–36.\nwww.it-ebooks.info"
  },
  {
    "page": 327,
    "content": "8.1. General Trees 309\n8.1.1 Tree Deﬁnitions and Properties\nAtreeis an abstract data type that stores elements hierarchically. With the excep-\ntion of the top element, each element in a tree has a parent element and zero or\nmore children elements. A tree is usually visualized by placing elements inside\novals or rectangles, and by drawing the connections between parents and children\nwith straight lines. (See Figure 8.2.) We typically call the top element the root\nof the tree, but it is drawn as the highest element, with the other elements being\nconnected below (just the opposite of a botanical tree).\nEurope Asia Africa AustraliaCanada Overseas S. AmericaDomestic International TV CD TunerSales Purchasing Manufacturing R&DElectronics R’Us\nFigure 8.2: A tree with 17 nodes representing the organization of a ﬁctitious cor-\nporation. The root stores Electronics R’Us . The children of the root store R&D ,\nSales ,Purchasing , and Manufacturing . The internal nodes store Sales ,Interna-\ntional ,Overseas ,Electronics R’Us , and Manufacturing .\nFormal Tree Deﬁnition\nFormally, we deﬁne a treeTas a set of nodes storing elements such that the nodes\nhave a parent-child relationship that satisﬁes the following properties:\n•IfTis nonempty, it has a special node, called the rootofT, that has no parent.\n•Each node vofTdifferent from the root has a unique parent node w; every\nnode with parent wis achild ofw.\nNote that according to our deﬁnition, a tree can be empty, meaning that it does not\nhave any nodes. This convention also allows us to deﬁne a tree recursively such\nthat a tree Tis either empty or consists of a node r, called the root of T, and a\n(possibly empty) set of subtrees whose roots are the children of r.\nwww.it-ebooks.info"
  },
  {
    "page": 328,
    "content": "310 Chapter 8. Trees\nOther Node Relationships\nTwo nodes that are children of the same parent are siblings . A node visexternal\nifvhas no children. A node visinternal if it has one or more children. External\nnodes are also known as leaves .\nExample 8.1: In Section 5.1.4, we discussed the hierarchical relationship be-\ntween ﬁles and directories in a computer’s ﬁle system, although at the time we\ndid not emphasize the nomenclature of a ﬁle system as a tree. In Figure 8.3, we\nrevisit an earlier example. We see that the internal nodes of the tree are associ-\nated with directories and the leaves are associated with regular ﬁles. In the Unix\nand Linux operating systems, the root of the tree is appropriately called the “root\ndirectory,” and is represented by the symbol “ /.”\n/user/rt/courses/\ncs016/ cs252/\nprograms/ homeworks/ projects/\npapers/ demos/hw1 hw2 hw3 pr1 pr2 pr3grades\nmarket buylow sellhighgrades\nFigure 8.3: Tree representing a portion of a ﬁle system.\nA node uis an ancestor of a node vifu=voruis an ancestor of the parent\nofv. Conversely, we say that a node vis adescendant of a node uifuis an ancestor\nofv. For example, in Figure 8.3, cs252/ is an ancestor of papers/ , andpr3is a\ndescendant of cs016/ . The subtree ofTrooted at a node vis the tree consisting of\nall the descendants of vinT(including vitself). In Figure 8.3, the subtree rooted at\ncs016/ consists of the nodes cs016/ ,grades ,homeworks/ ,programs/ ,hw1,hw2,\nhw3,pr1,pr2, andpr3.\nEdges and Paths in Trees\nAnedgeof tree Tis a pair of nodes (u,v)such that uis the parent of v, or vice\nversa. A path ofTis a sequence of nodes such that any two consecutive nodes in\nthe sequence form an edge. For example, the tree in Figure 8.3 contains the path\n(cs252/ ,projects/ ,demos/ ,market ).\nwww.it-ebooks.info"
  },
  {
    "page": 329,
    "content": "8.1. General Trees 311\nOrdered Trees\nA tree is ordered if there is a meaningful linear order among the children of each\nnode; that is, we purposefully identify the children of a node as being the ﬁrst,\nsecond, third, and so on. Such an order is usually visualized by arranging siblings\nleft to right, according to their order.\nExample 8.2: The components of a structured document, such as a book, are hier-\narchically organized as a tree whose internal nodes are parts, chapters, and sections,\nand whose leaves are paragraphs, tables, ﬁgures, and so on. (See Figure 8.4.) The\nroot of the tree corresponds to the book itself. We could, in fact, consider expanding\nthe tree further to show paragraphs consisting of sentences, sentences consisting of\nwords, and words consisting of characters. Such a tree is an example of an ordered\ntree, because there is a well-deﬁned order among the children of each node.\n... ... ¶ ¶ ...¶ ¶Book\nPart A Part B References Preface\n... ... ... ... Ch. 1 Ch. 5 Ch. 6 Ch. 9 ¶ ¶ ¶ ¶\n... ... ... ... § 1.4 § 1.1 § 5.7 § 5.1 § 9.6 § 9.1 § 6.5 § 6.1\nFigure 8.4: An ordered tree associated with a book.\nLet’s look back at the other examples of trees that we have described thus far,\nand consider whether the order of children is signiﬁcant. A family tree that de-\nscribes generational relationships, as in Figure 8.1, is often modeled as an orderedtree, with siblings ordered according to their birth.\nIn contrast, an organizational chart for a company, as in Figure 8.2, is typically\nconsidered an unordered tree. Likewise, when using a tree to describe an inher-itance hierarchy, as in Figure 2.7, there is no particular signiﬁcance to the orderamong the subclasses of a parent class. Finally, we consider the use of a tree inmodeling a computer’s ﬁle system, as in Figure 8.3. Although an operating systemoften displays entries of a directory in a particular order (e.g., alphabetical, chrono-\nlogical), such an order is not typically inherent to the ﬁle system’s representation.\nwww.it-ebooks.info"
  },
  {
    "page": 330,
    "content": "312 Chapter 8. Trees\n8.1.2 The Tree Abstract Data Type\nAs we did with positional lists in Section 7.3, we deﬁne a tree ADT using the\nconcept of a position as an abstraction for a node of a tree. An element is stored\nat each position, and positions satisfy parent-child relationships that deﬁne the tree\nstructure. A position object for a tree supports the method:\ngetElement() :Returns the element stored at this position.\nThe tree ADT then supports the following accessor methods , allowing a user\nto navigate the various positions of a tree T:\nroot() :Returns the position of the root of the tree\n(ornullif empty).\nparent( p):Returns the position of the parent of position p\n(ornullifpis the root).\nchildren( p):Returns an iterable collection containing the children of\nposition p(if any).\nnumChildren( p):Returns the number of children of position p.\nIf a tree Tis ordered, then children( p)reports the children of pin order.\nIn addition to the above fundamental accessor methods, a tree supports the\nfollowing query methods :\nisInternal( p):Returnstrue if position phas at least one child.\nisExternal( p):Returnstrue if position pdoes not have any children.\nisRoot( p):Returnstrue if position pis the root of the tree.\nThese methods make programming with trees easier and more readable, since\nwe can use them in the conditionals of ifstatements and while loops.\nTrees support a number of more general methods, unrelated to the speciﬁc\nstructure of the tree. These incude:\nsize() :Returns the number of positions (and hence elements)\nthat are contained in the tree.\nisEmpty() :Returnstrue if the tree does not contain any positions\n(and thus no elements).\niterator() :Returns an iterator for all elements in the tree\n(so that the tree itself is Iterable ).\npositions() :Returns an iterable collection of all positions of the tree.\nIf an invalid position is sent as a parameter to any method of a tree, then an\nIllegalArgumentException is thrown.\nWe do not deﬁne any methods for creating or modifying trees at this point.\nWe prefer to describe different tree update methods in conjunction with speciﬁc\nimplementations of the tree interface, and speciﬁc applications of trees.\nwww.it-ebooks.info"
  },
  {
    "page": 331,
    "content": "8.1. General Trees 313\nA Tree Interface in Java\nIn Code Fragment 8.1, we formalize the Tree ADT by deﬁning the Tree interface\nin Java. We rely upon the same deﬁnition of the Position interface as introduced\nfor positional lists in Section 7.3.2. Note well that we declare the Tree interface\nto formally extend Java’s Iterable interface (and we include a declaration of the\nrequirediterator() method).\n1/∗∗An interface for a tree where nodes can have an arbitrary number of children. ∗/\n2public interface Tree<E>extends Iterable<E>{\n3Position<E>root();\n4Position<E>parent(Position <E>p)throwsIllegalArgumentException;\n5Iterable<Position<E>>children(Position <E>p)\n6 throwsIllegalArgumentException;\n7intnumChildren(Position <E>p)throwsIllegalArgumentException;\n8boolean isInternal(Position <E>p)throwsIllegalArgumentException;\n9boolean isExternal(Position <E>p)throwsIllegalArgumentException;\n10boolean isRoot(Position <E>p)throwsIllegalArgumentException;\n11intsize();\n12boolean isEmpty();\n13Iterator<E>iterator();\n14Iterable<Position<E>>positions();\n15}\nCode Fragment 8.1: Deﬁnition of the Tree interface.\nAn AbstractTree Base Class in Java\nIn Section 2.3, we discussed the role of interfaces and abstract classes in Java.\nWhile an interface is a type deﬁnition that includes public declarations of vari-\nous methods, an interface cannot include deﬁnitions for any of those methods. In\ncontrast, an abstract class may deﬁne concrete implementations for some of its\nmethods, while leaving other abstract methods without deﬁnition.\nAn abstract class is designed to serve as a base class, through inheritance, for\none or more concrete implementations of an interface. When some of the func-\ntionality of an interface is implemented in an abstract class, less work remains to\ncomplete a concrete implementation. The standard Java libraries include many such\nabstract classes, including several within the Java Collections Framework. To make\ntheir purpose clear, those classes are conventionally named beginning with the word\nAbstract . For example, there is an AbstractCollection class that implements some\nof the functionality of the Collection interface, an AbstractQueue class that imple-\nments some of the functionality of the Queue interface, and an AbstractList class\nthat implements some of the functionality of the Listinterface.\nwww.it-ebooks.info"
  },
  {
    "page": 332,
    "content": "314 Chapter 8. Trees\nIn the case of our Tree interface, we will deﬁne an AbstractTree base class,\ndemonstrating how many tree-based algorithms can be described independently of\nthe low-level representation of a tree data structure. In fact, if a concrete implemen-\ntation provides three fundamental methods— root() ,parent( p), andchildren( p)—\nall other behaviors of the Tree interface can be derived within the AbstractTree\nbase class.\nCode Fragment 8.2 presents an initial implementation of an AbstractTree base\nclass that provides the most trivial methods of the Tree interface. We will defer\nuntil Section 8.4 a discussion of general tree-traversal algorithms that can be used\nto produced the positions() iteration within the AbstractTree class. As with our\npositional list ADT in Chapter 7, the iteration of positions of the tree can easily be\nadapted to produce an iteration of the elements of a tree, or even to determine the\nsize of a tree (although our concrete tree implementations will provide more direct\nmeans for reporting the size).\n1/∗∗An abstract base class providing some functionality of the Tree interface. ∗/\n2public abstract class AbstractTree <E>implements Tree<E>{\n3public boolean isInternal(Position <E>p){returnnumChildren(p) >0;}\n4public boolean isExternal(Position <E>p){returnnumChildren(p) == 0; }\n5public boolean isRoot(Position <E>p){returnp == root();}\n6public boolean isEmpty(){returnsize() == 0;}\n7}\nCode Fragment 8.2: An initial implementation of the AbstractTree base class. (We\nadd additional functionality to this class as the chapter continues.)\n8.1.3 Computing Depth and Height\nLetpbe a position within tree T. The depth ofpis the number of ancestors of\np, other than pitself. For example, in the tree of Figure 8.2, the node storing\nInternational has depth 2. Note that this deﬁnition implies that the depth of the\nroot of Tis 0. The depth of pcan also be recursively deﬁned as follows:\n•Ifpis the root, then the depth of pis 0.\n•Otherwise, the depth of pis one plus the depth of the parent of p.\nBased on this deﬁnition, we present a simple recursive algorithm, depth , in Code\nFragment 8.3, for computing the depth of a position pin tree T. This method calls\nitself recursively on the parent of p, and adds 1 to the value returned.\nThe running time of depth( p)for position pisO(dp+1), where dpdenotes\nthe depth of pin the tree, because the algorithm performs a constant-time recursive\nstep for each ancestor of p. Thus, algorithm depth( p)runs in O(n)worst-case time,\nwhere nis the total number of positions of T, because a position of Tmay have\ndepth n−1 if all nodes form a single branch. Although such a running time is a\nfunction of the input size, it is more informative to characterize the running time in\nterms of the parameter dp, as this parameter may be much smaller than n.\nwww.it-ebooks.info"
  },
  {
    "page": 333,
    "content": "8.1. General Trees 315\n1/∗∗Returns the number of levels separating Position p from the root. ∗/\n2public int depth(Position <E>p){\n3if(isRoot(p))\n4 return0;\n5else\n6 return1 + depth(parent(p));\n7}\nCode Fragment 8.3: Methoddepth , as implemented within the AbstractTree class.\nHeight\nWe next deﬁne the height of a tree to be equal to the maximum of the depths of\nits positions (or zero, if the tree is empty). For example, the tree of Figure 8.2 has\nheight 4, as the node storing Africa (and its siblings) has depth 4. It is easy to see\nthat the position with maximum depth must be a leaf.\nIn Code Fragment 8.4, we present a method that computes the height of a tree\nbased on this deﬁnition. Unfortunately, such an approach is not very efﬁcient,\nand so name the algorithm heightBad and declare it as a private method of the\nAbstractTree class (so that it cannot be used by others).\n1/∗∗Returns the height of the tree. ∗/\n2private int heightBad(){ // works, but quadratic worst-case time\n3inth = 0;\n4for(Position<E>p : positions())\n5 if(isExternal(p)) // only consider leaf positions\n6 h = Math.max(h, depth(p));\n7returnh;\n8}\nCode Fragment 8.4: MethodheightBad of theAbstractTree class. Note that this\nmethod calls the depth method from Code Fragment 8.3.\nAlthough we have not yet deﬁned the positions() method, we will see that it\ncan be implemented such that the entire iteration runs in O(n)time, where nis\nthe number of positions of T. Because heightBad calls algorithm depth( p)on\neach leaf of T, its running time is O(n+∑p∈L(dp+1)), where Lis the set of leaf\npositions of T. In the worst case, the sum ∑p∈L(dp+1)is proportional to n2. (See\nExercise C-8.31.) Thus, algorithm heightBad runs in O(n2)worst-case time.\nWe can compute the height of a tree more efﬁciently, in O(n)worst-case time,\nby considering a recursive deﬁnition. To do this, we will parameterize a function\nbased on a position within the tree, and calculate the height of the subtree rooted at\nthat position. Formally, we deﬁne the height of a position pin a tree Tas follows:\n•Ifpis a leaf, then the height of pis 0.\n•Otherwise, the height of pis one more than the maximum of the heights of\np’s children.\nwww.it-ebooks.info"
  },
  {
    "page": 334,
    "content": "316 Chapter 8. Trees\nThe following proposition relates our original deﬁnition of the height of a tree\nto the height of the rootposition using this recursive formula.\nProposition 8.3: The height of the root of a nonempty tree T, according to the\nrecursive deﬁnition, equals the maximum depth among all leaves of tree T.\nWe leave the justiﬁcation of this proposition as Exercise R-8.3.\nAn implementation of a recursive algorithm to compute the height of a subtree\nrooted at a given position pis presented in Code Fragment 8.5. The overall height\nof a nonempty tree can be computed by sending the root of the tree as a parameter.\n1/∗∗Returns the height of the subtree rooted at Position p. ∗/\n2public int height(Position <E>p){\n3inth = 0; // base case if p is external\n4for(Position<E>c : children(p))\n5 h = Math.max(h, 1 + height(c));\n6returnh;\n7}\nCode Fragment 8.5: Methodheight for computing the height of a subtree rooted at\na position pof anAbstractTree .\nIt is important to understand why method height is more efﬁcient than method\nheightBad . The algorithm is recursive, and it progresses in a top-down fashion. If\nthe method is initially called on the root of T, it will eventually be called once for\neach position of T. This is because the root eventually invokes the recursion on\neach of its children, which in turn invokes the recursion on each of their children,\nand so on.\nWe can determine the running time of the recursive height algorithm by sum-\nming, over all the positions, the amount of time spent on the nonrecursive part of\neach call. (Review Section 5.2 for analyses of recursive processes.) In our imple-\nmentation, there is a constant amount of work per position, plus the overhead of\ncomputing the maximum over the iteration of children. Although we do not yet\nhave a concrete implementation of children( p), we assume that such an iteration is\nexecuted in O(cp+1)time, where cpdenotes the number of children of p. Algo-\nrithmheight( p)spends O(cp+1)time at each position pto compute the maximum,\nand its overall running time is O(∑p(cp+1))= O(n+∑pcp). In order to complete\nthe analysis, we make use of the following property.\nProposition 8.4: LetTbe a tree with npositions, and let cpdenote the number of\nchildren of a position pofT. Then, summing over the positions of T,∑pcp=n−1.\nJustiﬁcation: Each position of T, with the exception of the root, is a child of\nanother position, and thus contributes one unit to the above sum.\nBy Proposition 8.4, the running time of algorithm height , when called on the\nroot of T, isO(n), where nis the number of positions of T.\nwww.it-ebooks.info"
  },
  {
    "page": 335,
    "content": "8.2. Binary Trees 317\n8.2 Binary Trees\nAbinary tree is an ordered tree with the following properties:\n1.Every node has at most two children.\n2.Each child node is labeled as being either a left child or aright child .\n3.A left child precedes a right child in the order of children of a node.\nThe subtree rooted at a left or right child of an internal node vis called a left subtree\norright subtree , respectively, of v. A binary tree is proper if each node has either\nzero or two children. Some people also refer to such trees as being fullbinary\ntrees. Thus, in a proper binary tree, every internal node has exactly two children.\nA binary tree that is not proper is improper .\nExample 8.5: An important class of binary trees arises in contexts where we wish\nto represent a number of different outcomes that can result from answering a series\nof yes-or-no questions. Each internal node is associated with a question. Starting at\nthe root, we go to the left or right child of the current node, depending on whether\nthe answer to the question is “Yes” or “No.” With each decision, we follow an\nedge from a parent to a child, eventually tracing a path in the tree from the root\nto a leaf. Such binary trees are known as decision trees , because a leaf position p\nin such a tree represents a decision of what to do if the questions associated with\np’s ancestors are answered in a way that leads to p. A decision tree is a proper\nbinary tree. Figure 8.5 illustrates a decision tree that provides recommendations to\na prospective investor.\nYes\nYes\nYes NoNoNoAre you nervous?\nWill you need to access most of the\nmoney within the next 5 years?\nAre you willing to accept risks in\nexchange for higher expected returns?Money market fund.\nStock portfolio.Savings account.\nDiversiﬁed portfolio with stocks,\nbonds, and short-term instruments.\nFigure 8.5: A decision tree providing investment advice.\nwww.it-ebooks.info"
  },
  {
    "page": 336,
    "content": "318 Chapter 8. Trees\nExample 8.6: An arithmetic expression can be represented by a binary tree whose\nleaves are associated with variables or constants, and whose internal nodes are\nassociated with one of the operators +,−,∗, and/, as demonstrated in Figure 8.6.\nEach node in such a tree has a value associated with it.\n•If a node is leaf, then its value is that of its variable or constant.\n•If a node is internal, then its value is deﬁned by applying its operation to the\nvalues of its children.\nA typical arithmetic expression tree is a proper binary tree, since each operator\n+,−,∗, and/takes exactly two operands. Of course, if we were to allow unary\noperators, like negation ( −), as in “−x,” then we could have an improper binary\ntree.\n∗+−\n+ 3\n9 5+\n2− 3−6\n3 1 7 4/\n∗\nFigure 8.6: A binary tree representing an arithmetic expression. This tree repre-\nsents the expression ((((3+1)∗3)/((9−5)+2))−((3∗(7−4))+6)). The value\nassociated with the internal node labeled “ /” is 2.\nA Recursive Binary Tree Deﬁnition\nIncidentally, we can also deﬁne a binary tree in a recursive way. In that case, a\nbinary tree is either:\n•An empty tree.\n•A nonempty tree having a root node r, which stores an element, and two\nbinary trees that are respectively the left and right subtrees of r. We note that\none or both of those subtrees can be empty by this deﬁnition.\nwww.it-ebooks.info"
  },
  {
    "page": 337,
    "content": "8.2. Binary Trees 319\n8.2.1 The Binary Tree Abstract Data Type\nAs an abstract data type, a binary tree is a specialization of a tree that supports three\nadditional accessor methods:\nleft(p):Returns the position of the left child of p\n(ornullifphas no left child).\nright( p):Returns the position of the right child of p\n(ornullifphas no right child).\nsibling( p):Returns the position of the sibling of p\n(ornullifphas no sibling).\nJust as in Section 8.1.2 for the tree ADT, we do not deﬁne specialized update meth-\nods for binary trees here. Instead, we will consider some possible update methods\nwhen we describe speciﬁc implementations and applications of binary trees.\nDeﬁning a BinaryTree Interface\nCode Fragment 8.6 formalizes the binary tree ADT by deﬁning a BinaryTree in-\nterface in Java. This interface extends the Tree interface that was given in Sec-\ntion 8.1.2 to add the three new behaviors. In this way, a binary tree is expected to\nsupport all the functionality that was deﬁned for general trees (e.g., root,isExternal ,\nparent ), and the new behaviors left,right , andsibling .\n1/∗∗An interface for a binary tree, in which each node has at most two children. ∗/\n2public interface BinaryTree <E>extends Tree<E>{\n3/∗∗Returns the Position of p 's left child (or null if no child exists). ∗/\n4Position<E>left(Position <E>p)throwsIllegalArgumentException;\n5/∗∗Returns the Position of p 's right child (or null if no child exists). ∗/\n6Position<E>right(Position <E>p)throwsIllegalArgumentException;\n7/∗∗Returns the Position of p 's sibling (or null if no sibling exists). ∗/\n8Position<E>sibling(Position <E>p)throwsIllegalArgumentException;\n9}\nCode Fragment 8.6: ABinaryTree interface that extends the Tree interface from\nCode Fragment 8.1.\nDeﬁning an AbstractBinaryTree Base Class\nWe continue our use of abstract base classes to promote greater reusability within\nour code. The AbstractBinaryTree class, presented in Code Fragment 8.7, inherits\nfrom the AbstractTree class from Section 8.1.2. It provides additional concrete\nmethods that can be derived from the newly declared leftandright methods (which\nremain abstract).\nwww.it-ebooks.info"
  },
  {
    "page": 338,
    "content": "320 Chapter 8. Trees\nThe newsibling method is derived from a combination of left,right , andparent .\nTypically, we identify the sibling of a position pas the “other” child of p’s parent.\nHowever, pdoes not have a sibling if it is the root, or if it is the only child of its\nparent.\nWe can also use the presumed leftandright methods to provide concrete im-\nplementations of the numChildren andchildren methods, which are part of the\noriginalTree interface. Using the terminology of Section 7.4, the implementa-\ntion of the children method relies on producing a snapshot . We create an empty\njava.util.ArrayList , which qualiﬁes as being an iterable container, and then add any\nchildren that exist, ordered so that a left child is reported before a right child.\n1/∗∗An abstract base class providing some functionality of the BinaryTree interface. ∗/\n2public abstract class AbstractBinaryTree <E>extends AbstractTree <E>\n3 implements BinaryTree <E>{\n4/∗∗Returns the Position of p 's sibling (or null if no sibling exists). ∗/\n5publicPosition<E>sibling(Position <E>p){\n6Position<E>parent = parent(p);\n7if(parent == null)return null ; // p must be the root\n8if(p == left(parent)) // p is a left child\n9 returnright(parent); // (right child might be null)\n10else // p is a right child\n11 returnleft(parent); // (left child might be null)\n12}\n13/∗∗Returns the number of children of Position p. ∗/\n14public int numChildren(Position <E>p){\n15intcount=0;\n16if(left(p) != null)\n17 count++;\n18if(right(p) != null)\n19 count++;\n20returncount;\n21}\n22/∗∗Returns an iterable collection of the Positions representing p's children. ∗/\n23publicIterable<Position<E>>children(Position <E>p){\n24List<Position<E>>snapshot = newArrayList <>(2);// max capacity of 2\n25if(left(p) != null)\n26 snapshot.add(left(p));\n27if(right(p) != null)\n28 snapshot.add(right(p));\n29returnsnapshot;\n30}\n31}\nCode Fragment 8.7: AnAbstractBinaryTree class that extends the AbstractTree\nclass of Code Fragment 8.2 and implements the BinaryTree interface of Code Frag-\nment 8.6.\nwww.it-ebooks.info"
  },
  {
    "page": 339,
    "content": "8.2. Binary Trees 321\n8.2.2 Properties of Binary Trees\nBinary trees have several interesting properties dealing with relationships between\ntheir heights and number of nodes. We denote the set of all nodes of a tree Tat the\nsame depth daslevel dofT. In a binary tree, level 0 has at most one node (the\nroot), level 1 has at most two nodes (the children of the root), level 2 has at most\nfour nodes, and so on. (See Figure 8.7.) In general, level dhas at most 2dnodes.\n...0 ......1\n2\n31 ...2\n4\n8Level Nodes\nFigure 8.7: Maximum number of nodes in the levels of a binary tree.\nWe can see that the maximum number of nodes on the levels of a binary tree\ngrows exponentially as we go down the tree. From this simple observation, we can\nderive the following properties relating the height of a binary tree Twith its number\nof nodes. A detailed justiﬁcation of these properties is left as Exercise R-8.8.\nProposition 8.7: LetTbe a nonempty binary tree, and let n,nE,nI, and hdenote\nthe number of nodes, number of external nodes, number of internal nodes, and\nheight of T, respectively. Then Thas the following properties:\n1.h+1≤n≤2h+1−1\n2.1≤nE≤2h\n3.h≤nI≤2h−1\n4.log(n+1)−1≤h≤n−1\nAlso, if Tis proper, then Thas the following properties:\n1.2h+1≤n≤2h+1−1\n2.h+1≤nE≤2h\n3.h≤nI≤2h−1\n4.log(n+1)−1≤h≤(n−1)/2\nwww.it-ebooks.info"
  },
  {
    "page": 340,
    "content": "322 Chapter 8. Trees\nRelating Internal Nodes to External Nodes in a Proper Binary Tree\nIn addition to the earlier binary tree properties, the following relationship exists\nbetween the number of internal nodes and external nodes in a proper binary tree.\nProposition 8.8: In a nonempty proper binary tree T, with nEexternal nodes and\nnIinternal nodes, we have nE=nI+1.\nJustiﬁcation: We justify this proposition by removing nodes from Tand divid-\ning them up into two “piles,” an internal-node pile and an external-node pile, until\nTbecomes empty. The piles are initially empty. By the end, we will show that the\nexternal-node pile has one more node than the internal-node pile. We consider two\ncases:\nCase 1: IfThas only one node v, we remove vand place it on the external-node\npile. Thus, the external-node pile has one node and the internal-node pile is\nempty.\nCase 2: Otherwise ( Thas more than one node), we remove from Tan (arbitrary)\nexternal node wand its parent v, which is an internal node. We place won\nthe external-node pile and von the internal-node pile. If vhas a parent u,\nthen we reconnect uwith the former sibling zofw, as shown in Figure 8.8.\nThis operation, removes one internal node and one external node, and leaves\nthe tree being a proper binary tree.\nRepeating this operation, we eventually are left with a ﬁnal tree consisting\nof a single node. Note that the same number of external and internal nodes\nhave been removed and placed on their respective piles by the sequence of\noperations leading to this ﬁnal tree. Now, we remove the node of the ﬁnal\ntree and we place it on the external-node pile. Thus, the external-node pile\nhas one more node than the internal-node pile.\nvu\nw zu\nzu\nz\n(a) (b) (c)\nFigure 8.8: Operation that removes an external node and its parent node, used in\nthe justiﬁcation of Proposition 8.8.\nNote that the above relationship does not hold, in general, for improper binary\ntrees and nonbinary trees, although there are other interesting relationships that do\nhold. (See Exercises C-8.30 through C-8.32.)\nwww.it-ebooks.info"
  },
  {
    "page": 341,
    "content": "8.3. Implementing Trees 323\n8.3 Implementing Trees\nTheAbstractTree andAbstractBinaryTree classes that we have deﬁned thus far in\nthis chapter are both abstract base classes . Although they provide a great deal of\nsupport, neither of them can be directly instantiated. We have not yet deﬁned key\nimplementation details for how a tree will be represented internally, and how we\ncan effectively navigate between parents and children.\nThere are several choices for the internal representation of trees. We describe\nthe most common representations in this section. We begin with the case of a\nbinary tree , since its shape is more strictly deﬁned.\n8.3.1 Linked Structure for Binary Trees\nA natural way to realize a binary tree Tis to use a linked structure , with a node\n(see Figure 8.9a) that maintains references to the element stored at a position p\nand to the nodes associated with the children and parent of p. If pis the root\nofT, then the parent ﬁeld of pisnull. Likewise, if pdoes not have a left child\n(respectively, right child), the associated ﬁeld is null. The tree itself maintains an\ninstance variable storing a reference to the root node (if any), and a variable, called\nsize, that represents the overall number of nodes of T. We show such a linked\nstructure representation of a binary tree in Figure 8.9b.\nparent\nelementright leftroot\n∅∅\n∅ ∅ ∅∅\n∅\nBaltimore Chicago New York Providence Seattlesize5\n(a) (b)\nFigure 8.9: A linked structure for representing: (a) a single node; (b) a binary tree.\nwww.it-ebooks.info"
  },
  {
    "page": 342,
    "content": "324 Chapter 8. Trees\nOperations for Updating a Linked Binary Tree\nTheTree andBinaryTree interfaces deﬁne a variety of methods for inspecting an\nexisting tree, yet they do not declare any update methods. Presuming that a newly\nconstructed tree is empty, we would like to have means for changing the structure\nof content of a tree.\nAlthough the principle of encapsulation suggests that the outward behaviors of\nan abstract data type need not depend on the internal representation, the efﬁciency of\nthe operations depends greatly upon the representation. We therefore prefer to haveeach concrete implementation of a tree class support the most suitable behaviors for\nupdating a tree. In the case of a linked binary tree, we suggest that the followingupdate methods be supported:\naddRoot( e):Creates a root for an empty tree, storing eas the element,\nand returns the position of that root; an error occurs if thetree is not empty.\naddLeft( p,e):Creates a left child of position p, storing element e, and\nreturns the position of the new node; an error occurs if p\nalready has a left child.\naddRight( p,e):Creates a right child of position p, storing element e, and\nreturns the position of the new node; an error occurs if p\nalready has a right child.\nset(p,e):Replaces the element stored at position pwith element e,\nand returns the previously stored element.\nattach( p,T\n1,T2):Attaches the internal structure of trees T1andT2as the\nrespective left and right subtrees of leaf position pand\nresets T1andT2to empty trees; an error condition occurs\nifpis not a leaf.\nremove( p):Removes the node at position p, replacing it with its child\n(if any), and returns the element that had been stored at p;\nan error occurs if phas two children.\nWe have speciﬁcally chosen this collection of operations because each can be\nimplemented in O(1)worst-case time with our linked representation. The most\ncomplex of these are attach andremove , due to the case analyses involving the\nvarious parent-child relationships and boundary conditions, yet there remains onlya constant number of operations to perform. (The implementation of both methodscould be greatly simpliﬁed if we used a tree representation with a sentinel node,\nakin to our treatment of positional lists; see Exercise C-8.38.)\nwww.it-ebooks.info"
  },
  {
    "page": 343,
    "content": "8.3. Implementing Trees 325\nJava Implementation of a Linked Binary Tree Structure\nWe now present a concrete implementation of a LinkedBinaryTree class that im-\nplements the binary tree ADT, and supports the update methods described on the\nprevious page. The new class formally extends the AbstractBinaryTree base class,\ninheriting several concrete implementations of methods from that class (as well as\nthe formal designation that it implements the BinaryTree interface).\nThe low-level details of our linked tree implementation are reminiscent of tech-\nniques used when implementing the LinkedPositionalList class in Section 7.3.3. We\ndeﬁne a nonpublic nested Node class to represent a node, and to serve as a Position\nfor the public interface. As was portrayed in Figure 8.9, a node maintains a refer-\nence to an element, as well as references to its parent, its left child, and its right\nchild (any of which might be null). The tree instance maintains a reference to the\nroot node (possibly null), and a count of the number of nodes in the tree.\nWe also provide a validate utility that is called anytime a Position is received as\na parameter, to ensure that it is a valid node. In the case of a linked tree, we adopt\na convention in which we set a node’s parent pointer to itself when it is removedfrom a tree, so that we can later recognize it as an invalid position.\nThe entire LinkedBinaryTree class is presented in Code Fragments 8.8–8.11.\nWe provide the following guide to that code:\n•Code Fragment 8.8 contains the deﬁnition of the nested Node class, which\nimplements the Position interface. It also deﬁnes a method, createNode ,\nthat returns a new node instance. Such a design uses what is known as thefactory method pattern , allowing us to later subclass our tree in order to use\na specialized node type. (See Section 11.2.1.) Code Fragment 8.8 concludeswith the declaration of the instance variables of the outer LinkedBinaryTree\nclass and its constructor.\n•Code Fragment 8.9 includes the protected validate(p) method, followed by\nthe accessors size,root,left, andright . We note that all other methods of the\nTree andBinaryTree interfaces are derived from these four concrete methods,\nvia theAbstractTree andAbstractBinaryTree base classes.\n•Code Fragments 8.10 and 8.11 provide the six update methods for a linkedbinary tree, as described on the preceding page. We note that the three\nmethods— addRoot ,addLeft , andaddRight —each rely on use of the factory\nmethod,createNode , to produce a new node instance.\nTheremove method, given at the end of Code Fragment 8.11, intentionally\nsets the parent ﬁeld of a deleted node to refer to itself, in accordance with our\nconventional representation of a defunct node (as detected within the validate\nmethod). It resets all other ﬁelds to null, to aid in garbage collection.\nwww.it-ebooks.info"
  },
  {
    "page": 344,
    "content": "326 Chapter 8. Trees\n1/∗∗Concrete implementation of a binary tree using a node-based, linked structure. ∗/\n2public class LinkedBinaryTree <E>extends AbstractBinaryTree <E>{\n3\n4//---------------- nested Node class ----------------\n5protected static class Node<E>implements Position<E>{\n6privateE element; // an element stored at this node\n7privateNode<E>parent; // a reference to the parent node (if any)\n8privateNode<E>left; // a reference to the left child (if any)\n9privateNode<E>right; // a reference to the right child (if any)\n10/∗∗Constructs a node with the given element and neighbors. ∗/\n11publicNode(E e, Node <E>above, Node< E>leftChild, Node <E>rightChild){\n12 element = e;\n13 parent = above;\n14 left = leftChild;\n15 right = rightChild;\n16}\n17// accessor methods\n18publicE getElement(){returnelement;}\n19publicNode<E>getParent(){returnparent;}\n20publicNode<E>getLeft(){returnleft;}\n21publicNode<E>getRight(){returnright;}\n22// update methods\n23public void setElement(E e){element = e;}\n24public void setParent(Node <E>parentNode){parent = parentNode; }\n25public void setLeft(Node <E>leftChild){left = leftChild;}\n26public void setRight(Node <E>rightChild){right = rightChild; }\n27}//----------- end of nested Node class -----------\n2829/∗∗Factory function to create a new node storing element e. ∗/\n30protected Node<E>createNode(E e, Node <E>parent,\n31 Node<E>left, Node <E>right){\n32return new Node<E>(e, parent, left, right);\n33}\n34\n35// LinkedBinaryTree instance variables\n36protected Node<E>root =null; // root of the tree\n37private int size = 0; // number of nodes in the tree\n3839// constructor\n40publicLinkedBinaryTree() {} // constructs an empty binary tree\nCode Fragment 8.8:\nAn implementation of the LinkedBinaryTree class.\n(Continues in Code Fragments 8.9–8.11.)\nwww.it-ebooks.info"
  },
  {
    "page": 345,
    "content": "8.3. Implementing Trees 327\n41// nonpublic utility\n42/∗∗Validates the position and returns it as a node. ∗/\n43protected Node<E>validate(Position <E>p)throwsIllegalArgumentException {\n44if(!(pinstanceof Node))\n45 throw new IllegalArgumentException( \"Not valid position type\" );\n46Node<E>node = (Node <E>) p; // safe cast\n47if(node.getParent() == node) // our convention for defunct node\n48 throw new IllegalArgumentException( \"p is no longer in the tree\" );\n49returnnode;\n50}\n51\n52// accessor methods (not already implemented in AbstractBinaryTree)\n53/∗∗Returns the number of nodes in the tree. ∗/\n54public int size(){\n55returnsize;\n56}\n5758/∗∗Returns the root Position of the tree (or null if tree is empty). ∗/\n59publicPosition<E>root(){\n60returnroot;\n61}\n6263/∗∗Returns the Position of p 's parent (or null if p is root). ∗/\n64publicPosition<E>parent(Position <E>p)throwsIllegalArgumentException {\n65Node<E>node = validate(p);\n66returnnode.getParent();\n67}\n6869/∗∗Returns the Position of p 's left child (or null if no child exists). ∗/\n70publicPosition<E>left(Position <E>p)throwsIllegalArgumentException {\n71Node<E>node = validate(p);\n72returnnode.getLeft();\n73}\n74\n75/∗∗Returns the Position of p 's right child (or null if no child exists). ∗/\n76publicPosition<E>right(Position <E>p)throwsIllegalArgumentException {\n77Node<E>node = validate(p);\n78returnnode.getRight();\n79}\nCode Fragment 8.9:\nAn implementation of the LinkedBinaryTree class.\n(Continued from Code Fragment 8.8; continues in Code Fragments 8.10 and 8.11.)\nwww.it-ebooks.info"
  },
  {
    "page": 346,
    "content": "328 Chapter 8. Trees\n80// update methods supported by this class\n81/∗∗Places element e at the root of an empty tree and returns its new Position. ∗/\n82publicPosition<E>addRoot(E e) throwsIllegalStateException {\n83if(!isEmpty()) throw new IllegalStateException( \"Tree is not empty\" );\n84root = createNode(e, null,null,null);\n85size = 1;\n86returnroot;\n87}\n88\n89/∗∗Creates a new left child of Position p storing element e; returns its Position. ∗/\n90publicPosition<E>addLeft(Position <E>p, E e)\n91 throwsIllegalArgumentException {\n92Node<E>parent = validate(p);\n93if(parent.getLeft() != null)\n94 throw new IllegalArgumentException( \"p already has a left child\" );\n95Node<E>child = createNode(e, parent, null,null);\n96parent.setLeft(child);\n97size++;\n98returnchild;\n99}\n100\n101/∗∗Creates a new right child of Position p storing element e; returns its Position. ∗/\n102publicPosition<E>addRight(Position <E>p, E e)\n103 throwsIllegalArgumentException {\n104 Node<E>parent = validate(p);\n105 if(parent.getRight() != null)\n106 throw new IllegalArgumentException( \"p already has a right child\" );\n107 Node<E>child = createNode(e, parent, null,null);\n108 parent.setRight(child);\n109 size++;\n110 returnchild;\n111}\n112\n113/∗∗Replaces the element at Position p with e and returns the replaced element. ∗/\n114publicE set(Position <E>p, E e)throwsIllegalArgumentException {\n115 Node<E>node = validate(p);\n116 E temp = node.getElement();\n117 node.setElement(e);\n118 returntemp;\n119}\nCode Fragment 8.10: An implementation of the LinkedBinaryTree class.\n(Continued from Code Fragments 8.8 and 8.9; continues in Code Fragment 8.11.)\nwww.it-ebooks.info"
  },
  {
    "page": 347,
    "content": "8.3. Implementing Trees 329\n120/∗∗Attaches trees t1 and t2 as left and right subtrees of external p. ∗/\n121public void attach(Position <E>p, LinkedBinaryTree <E>t1,\n122 LinkedBinaryTree <E>t2)throwsIllegalArgumentException {\n123 Node<E>node = validate(p);\n124 if(isInternal(p)) throw new IllegalArgumentException( \"p must be a leaf\" );\n125 size += t1.size() + t2.size();\n126 if(!t1.isEmpty()){ // attach t1 as left subtree of node\n127 t1.root.setParent(node);\n128 node.setLeft(t1.root);\n129 t1.root = null;\n130 t1.size = 0;\n131}\n132 if(!t2.isEmpty()){ // attach t2 as right subtree of node\n133 t2.root.setParent(node);\n134 node.setRight(t2.root);\n135 t2.root = null;\n136 t2.size = 0;\n137}\n138}\n139/∗∗Removes the node at Position p and replaces it with its child, if any. ∗/\n140publicE remove(Position <E>p)throwsIllegalArgumentException {\n141 Node<E>node = validate(p);\n142 if(numChildren(p) == 2)\n143 throw new IllegalArgumentException( \"p has two children\" );\n144 Node<E>child = (node.getLeft() != null? node.getLeft() : node.getRight() );\n145 if(child != null)\n146 child.setParent(node.getParent()); // child’s grandparent becomes its parent\n147 if(node == root)\n148 root = child; // child becomes root\n149 else{\n150 Node<E>parent = node.getParent();\n151 if(node == parent.getLeft())\n152 parent.setLeft(child);\n153 else\n154 parent.setRight(child);\n155}\n156 size−−;\n157 E temp = node.getElement();\n158 node.setElement( null); // help garbage collection\n159 node.setLeft( null);\n160 node.setRight( null);\n161 node.setParent(node); // our convention for defunct node\n162 returntemp;\n163}\n164}//----------- end of LinkedBinaryTree class -----------\nCode Fragment 8.11: An implementation of the LinkedBinaryTree class.\n(Continued from Code Fragments 8.8–8.10.)\nwww.it-ebooks.info"
  },
  {
    "page": 348,
    "content": "330 Chapter 8. Trees\nPerformance of the Linked Binary Tree Implementation\nTo summarize the efﬁciencies of the linked structure representation, we analyze the\nrunning times of the LinkedBinaryTree methods, including derived methods that\nare inherited from the AbstractTree andAbstractBinaryTree classes:\n•Thesizemethod, implemented in LinkedBinaryTree , uses an instance vari-\nable storing the number of nodes of a tree and therefore takes O(1)time.\nMethodisEmpty , inherited from AbstractTree , relies on a single call to size\nand thus takes O(1)time.\n•The accessor methods root,left,right , andparent are implemented directly\ninLinkedBinaryTree and take O(1)time each. The sibling ,children , and\nnumChildren methods are derived in AbstractBinaryTree using on a constant\nnumber of calls to these other accessors, so they run in O(1)time as well.\n•TheisInternal andisExternal methods, inherited from the AbstractTree class,\nrely on a call to numChildren , and thus run in O(1)time as well. The isRoot\nmethod, also implemented in AbstractTree , relies on a comparison to the\nresult of the root method and runs in O(1)time.\n•The update method, set, clearly runs in O(1)time. More signiﬁcantly, all of\nthe methods addRoot ,addLeft ,addRight ,attach , andremove run in O(1)\ntime, as each involves relinking only a constant number of parent-child rela-\ntionships per operation.\n•Methods depth andheight were each analyzed in Section 8.1.3. The depth\nmethod at position pruns in O(dp+1)time where dpis its depth; the height\nmethod on the root of the tree runs in O(n)time.\nThe overall space requirement of this data structure is O(n), for a tree with\nnnodes, as there is an instance of the Node class for every node, in addition to the\ntop-level sizeandroot ﬁelds. Table 8.1 summarizes the performance of the linked\nstructure implementation of a binary tree.\nMethod Running Time\nsize,isEmpty O(1)\nroot,parent ,left,right ,sibling ,children ,numChildren O(1)\nisInternal ,isExternal ,isRoot O(1)\naddRoot ,addLeft ,addRight ,set,attach ,remove O(1)\ndepth( p)O(dp+1)\nheight O(n)\nTable 8.1: Running times for the methods of an n-node binary tree implemented\nwith a linked structure. The space usage is O(n).\nwww.it-ebooks.info"
  },
  {
    "page": 349,
    "content": "8.3. Implementing Trees 331\n8.3.2 Array-Based Representation of a Binary Tree\nAn alternative representation of a binary tree Tis based on a way of numbering the\npositions of T. For every position pofT, letf(p)be the integer deﬁned as follows.\n•Ifpis the root of T, then f(p)=0.\n•Ifpis the left child of position q, then f(p)=2f(q)+1.\n•Ifpis the right child of position q, then f(p)=2f(q)+2.\nThe numbering function fis known as a level numbering of the positions in a\nbinary tree T, for it numbers the positions on each level of Tin increasing order\nfrom left to right. (See Figure 8.10.) Note well that the level numbering is based\nonpotential positions within a tree, not the actual shape of a speciﬁc tree, so they\nare not necessarily consecutive. For example, in Figure 8.10(b), there are no nodes\nwith level numbering 13 or 14, because the node with level numbering 6 has no\nchildren.\n(a)\n. . . . . .4\n10 11 12 13 14 8 70\n2\n6 51\n3\n9\n(b)\n15+−\n+∗\n3\n9 5+\n2−∗\n3−6\n3 1 7 4/0\n1 2\n5 4 3 6\n12 11 10\n25 26 209\n197 8\n16\nFigure 8.10: Binary tree level numbering: (a) general scheme; (b) an example.\nwww.it-ebooks.info"
  },
  {
    "page": 350,
    "content": "332 Chapter 8. Trees\nThe level numbering function fsuggests a representation of a binary tree Tby\nmeans of an array-based structure A, with the element at position pofTstored at\nindex f(p)of the array. We show an example of an array-based representation of a\nbinary tree in Figure 8.11.\n/\n4 20\n2 1\n3 4 5 6\n12 11 8 7\n3 1+∗\n9 5−+\n0 6 12 1 2 3 4 5 7 8 9 10 11 13 145 ∗+ + 4−2 3 1 9 /\nFigure 8.11: Representation of a binary tree by means of an array.\nOne advantage of an array-based representation of a binary tree is that a posi-\ntion pcan be represented by the single integer f(p), and that position-based meth-\nods such as root,parent ,left, andright can be implemented using simple arithmetic\noperations on the number f(p). Based on our formula for the level numbering, the\nleft child of phas index 2 f(p)+1, the right child of phas index 2 f(p)+2, and\nthe parent of phas index⌊(f(p)−1)/2⌋. We leave the details of a complete array-\nbased implementation as Exercise R-8.16.\nThe space usage of an array-based representation depends greatly on the shape\nof the tree. Let nbe the number of nodes of T, and let fMbe the maximum value\noff(p)over all the nodes of T. The array Arequires length N=1+fM, since\nelements range from A[0]toA[fM]. Note that Amay have a number of empty cells\nthat do not refer to existing positions of T. In fact, in the worst case, N=2n−1,\nthe justiﬁcation of which is left as an exercise (R-8.14). In Section 9.3, we will\nsee a class of binary trees, called “heaps” for which N=n. Thus, in spite of the\nworst-case space usage, there are applications for which the array representationof a binary tree is space efﬁcient. Still, for general binary trees, the exponentialworst-case space requirement of this representation is prohibitive.\nAnother drawback of an array representation is that many update operations for\ntrees cannot be efﬁciently supported. For example, removing a node and promotingits child takes O(n)time because it is not just the child that moves locations within\nthe array, but all descendants of that child.\nwww.it-ebooks.info"
  },
  {
    "page": 351,
    "content": "8.3. Implementing Trees 333\n8.3.3 Linked Structure for General Trees\nWhen representing a binary tree with a linked structure, each node explicitly main-\ntains ﬁelds leftandright as references to individual children. For a general tree,\nthere is no a priori limit on the number of children that a node may have. A natural\nway to realize a general tree Tas a linked structure is to have each node store a\nsingle container of references to its children. For example, a children ﬁeld of a\nnode can be an array or list of references to the children of the node (if any). Such\na linked representation is schematically illustrated in Figure 8.12.\nelementparent\nchildren\nBaltimore ChicagoNew York\nProvidence Seattle\n(a) (b)\nFigure 8.12: The linked structure for a general tree: (a) the structure of a node; (b) a\nlarger portion of the data structure associated with a node and its children.\nTable 8.2 summarizes the performance of the implementation of a general tree\nusing a linked structure. The analysis is left as an exercise (R-8.13), but we note\nthat, by using a collection to store the children of each position p, we can implement\nchildren(p)by simply iterating that collection.\nMethod Running Time\nsize,isEmpty O(1)\nroot,parent ,isRoot ,isInternal ,isExternal O(1)\nnumChildren( p)O(1)\nchildren( p)O(cp+1)\ndepth( p)O(dp+1)\nheight O(n)\nTable 8.2: Running times of the accessor methods of an n-node general tree im-\nplemented with a linked structure. We let cpdenote the number of children of a\nposition p, and dpits depth. The space usage is O(n).\nwww.it-ebooks.info"
  },
  {
    "page": 352,
    "content": "334 Chapter 8. Trees\n8.4 Tree Traversal Algorithms\nAtraversal of a tree Tis a systematic way of accessing, or “visiting,” all the posi-\ntions of T. The speciﬁc action associated with the “visit” of a position pdepends\non the application of this traversal, and could involve anything from increment-\ning a counter to performing some complex computation for p. In this section, we\ndescribe several common traversal schemes for trees, implement them in the con-\ntext of our various tree classes, and discuss several common applications of tree\ntraversals.\n8.4.1 Preorder and Postorder Traversals of General Trees\nIn a preorder traversal of a tree T, the root of Tis visited ﬁrst and then the sub-\ntrees rooted at its children are traversed recursively. If the tree is ordered, then\nthe subtrees are traversed according to the order of the children. The pseudocode\nfor the preorder traversal of the subtree rooted at a position pis shown in Code\nFragment 8.12.\nAlgorithm preorder( p):\nperform the “visit” action for position p{this happens before any recursion }\nforeach child cinchildren (p)do\npreorder (c) {recursively traverse the subtree rooted at c}\nCode Fragment 8.12: Algorithm preorder for performing the preorder traversal of a\nsubtree rooted at position pof a tree.\nFigure 8.13 portrays the order in which positions of a sample tree are visited\nduring an application of the preorder traversal algorithm.\nPaper\nTitle Abstract § 1 References § 2 § 3\n§ 1.1 § 1.2 § 2.1 § 2.2 § 2.3 § 3.1 § 3.2\nFigure 8.13: Preorder traversal of an ordered tree, where the children of each posi-\ntion are ordered from left to right.\nwww.it-ebooks.info"
  },
  {
    "page": 353,
    "content": "8.4. Tree Traversal Algorithms 335\nPostorder Traversal\nAnother important tree traversal algorithm is the postorder traversal . In some\nsense, this algorithm can be viewed as the opposite of the preorder traversal, be-\ncause it recursively traverses the subtrees rooted at the children of the root ﬁrst, andthen visits the root (hence, the name “postorder”). Pseudocode for the postordertraversal is given in Code Fragment 8.13, and an example of a postorder traversalis portrayed in Figure 8.14.\nAlgorithm postorder( p):\nforeach child cinchildren (p)do\npostorder( c) {recursively traverse the subtree rooted at c}\nperform the “visit” action for position p{this happens after any recursion }\nCode Fragment 8.13: Algorithm postorder for performing the postorder traversal of\na subtree rooted at position pof a tree.\nPaper\nTitle Abstract § 1 References § 2 § 3\n§ 1.1 § 1.2 § 2.1 § 2.2 § 2.3 § 3.1 § 3.2\nFigure 8.14: Postorder traversal of the ordered tree of Figure 8.13.\nRunning-Time Analysis\nBoth preorder and postorder traversal algorithms are efﬁcient ways to access all the\npositions of a tree. The analysis of either of these traversal algorithms is similar\nto that of algorithm height , given in Code Fragment 8.5 of Section 8.1.3. At each\nposition p, the nonrecursive part of the traversal algorithm requires time O(cp+1),\nwhere cpis the number of children of p, under the assumption that the “visit” itself\ntakes O(1)time. By Proposition 8.4, the overall running time for the traversal of\ntreeTisO(n), where nis the number of positions in the tree. This running time is\nasymptotically optimal since the traversal must visit all npositions of the tree.\nwww.it-ebooks.info"
  },
  {
    "page": 354,
    "content": "336 Chapter 8. Trees\n8.4.2 Breadth-First Tree Traversal\nAlthough the preorder and postorder traversals are common ways of visiting the\npositions of a tree, another approach is to traverse a tree so that we visit all the\npositions at depth dbefore we visit the positions at depth d+1. Such an algorithm\nis known as a breadth-ﬁrst traversal .\nA breadth-ﬁrst traversal is a common approach used in software for playing\ngames. A game tree represents the possible choices of moves that might be made\nby a player (or computer) during a game, with the root of the tree being the initial\nconﬁguration for the game. For example, Figure 8.15 displays a partial game tree\nfor Tic-Tac-Toe.\nXX\nX\nOX\nOX\nOX\nOO X X\nOX\nOX\nOX O X O O\nXO\nXX\n163 2 41\n5 6 8 7 9 10 11 12 13 14 15\nFigure 8.15: Partial game tree for Tic-Tac-Toe when ignoring symmetries; annota-\ntions denote the order in which positions are visited in a breadth-ﬁrst tree traversal.\nA breadth-ﬁrst traversal of such a game tree is often performed because a computer\nmay be unable to explore a complete game tree in a limited amount of time. So the\ncomputer will consider all moves, then responses to those moves, going as deep as\ncomputational time allows.\nPseudocode for a breadth-ﬁrst traversal is given in Code Fragment 8.14. The\nprocess is not recursive, since we are not traversing entire subtrees at once. We use\na queue to produce a FIFO (i.e., ﬁrst-in ﬁrst-out) semantics for the order in which\nwe visit nodes. The overall running time is O(n), due to the ncalls toenqueue and\nncalls todequeue .\nAlgorithm breadthﬁrst() :\nInitialize queue Qto contain root()\nwhile Qnot empty do\np=Q.dequeue() {pis the oldest entry in the queue }\nperform the “visit” action for position p\nforeach child cinchildren (p)do\nQ.enqueue( c){addp’s children to the end of the queue for later visits }\nCode Fragment 8.14: Algorithm for performing a breadth-ﬁrst traversal of a tree.\nwww.it-ebooks.info"
  },
  {
    "page": 355,
    "content": "8.4. Tree Traversal Algorithms 337\n8.4.3 Inorder Traversal of a Binary Tree\nThe standard preorder, postorder, and breadth-ﬁrst traversals that were introduced\nfor general trees can be directly applied to binary trees. In this section, we will\nintroduce another common traversal algorithm speciﬁcally for a binary tree.\nDuring an inorder traversal , we visit a position between the recursive traver-\nsals of its left and right subtrees. The inorder traversal of a binary tree Tcan be\ninformally viewed as visiting the nodes of T“from left to right.” Indeed, for every\nposition p, the inorder traversal visits pafter all the positions in the left subtree of\npand before all the positions in the right subtree of p. Pseudocode for the inorder\ntraversal algorithm is given in Code Fragment 8.15, and an example of an inorder\ntraversal is portrayed in Figure 8.16.\nAlgorithm inorder( p):\nifphas a left child lcthen\ninorder (lc) {recursively traverse the left subtree of p}\nperform the “visit” action for position p\nifphas a right child rcthen\ninorder (rc) {recursively traverse the right subtree of p}\nCode Fragment 8.15: Algorithm inorder for performing an inorder traversal of a\nsubtree rooted at position pof a binary tree.\n3 1 9 5 4 7+ 3 2 − 3 −× + × 6/ +−\nFigure 8.16: Inorder traversal of a binary tree.\nThe inorder traversal algorithm has several important applications. When using\na binary tree to represent an arithmetic expression, as in Figure 8.16, the inorder\ntraversal visits positions in a consistent order with the standard representation of\nthe expression, as in 3 +1×3/9−5+2...(albeit without parentheses).\nwww.it-ebooks.info"
  },
  {
    "page": 356,
    "content": "338 Chapter 8. Trees\nBinary Search Trees\nAn important application of the inorder traversal algorithm arises when we store an\nordered sequence of elements in a binary tree, deﬁning a structure we call a binary\nsearch tree . Let Sbe a set whose unique elements have an order relation. For\nexample, Scould be a set of integers. A binary search tree for Sis a proper binary\ntreeTsuch that, for each internal position pofT:\n•Position pstores an element of S, denoted as e(p).\n•Elements stored in the left subtree of p(if any) are less than e(p).\n•Elements stored in the right subtree of p(if any) are greater than e(p).\nAn example of a binary search tree is shown in Figure 8.17. The above propertiesassure that an inorder traversal of a binary search tree Tvisits the elements in\nnondecreasing order.\n3658\n7562\n124231\n2590\nFigure 8.17: A binary search tree storing integers. The solid path is traversed when\nsearching (successfully) for 42. The dashed path is traversed when searching (un-successfully) for 70.\nWe can use a binary search tree Tfor set Sto ﬁnd whether a given search value\nvis in S, by traversing a path down the tree T, starting at the root. At each internal\nposition pencountered, we compare our search value vwith the element e(p)stored\natp. Ifv<e(p), then the search continues in the left subtree of p. Ifv=e(p), then\nthe search terminates successfully. If v>e(p), then the search continues in the\nright subtree of p. Finally, if we reach a leaf, the search terminates unsuccessfully.\nIn other words, a binary search tree can be viewed as a binary decision tree (recallExample 8.5), where the question asked at each internal node is whether the ele-ment at that node is less than, equal to, or larger than the element being searchedfor. We illustrate several examples of the search operation in Figure 8.17.\nNote that the running time of searching in a binary search tree Tis proportional\nto the height of T. Recall from Proposition 8.7 that the height of a binary tree with\nnnodes can be as small as log (n+1)−1 or as large as n−1. Thus, binary search\ntrees are most efﬁcient when they have small height. Chapter 11 is devoted to the\nstudy of search trees.\nwww.it-ebooks.info"
  },
  {
    "page": 357,
    "content": "8.4. Tree Traversal Algorithms 339\n8.4.4 Implementing Tree Traversals in Java\nWhen ﬁrst deﬁning the tree ADT in Section 8.1.2, we stated that tree Tmust include\nthe following supporting methods:\niterator() :Returns an iterator for all elements in the tree.\npositions() :Returns an iterable collection of all positions of the tree.\nAt that time, we did not make any assumption about the order in which these\niterations report their results. In this section, we will demonstrate how any of the\ntree traversal algorithms we have introduced can be used to produce these iterations\nas concrete implementations within the AbstractTree orAbstractBinaryTree base\nclasses.\nFirst, we note that an iteration of all elements of a tree can easily be produced\nif we have an iteration of all positions of that tree. Code Fragment 8.16 provides\nan implementation of the iterator() method by adapting an iteration produced by\nthepositions() method. In fact, this is the identical approach we used in Code\nFragment 7.14 of Section 7.4.2 for the LinkedPositionalList class.\n1//---------------- nested ElementIterator class ----------------\n2/∗This class adapts the iteration produced by positions() to return elements. ∗/\n3private class ElementIterator implements Iterator<E>{\n4Iterator<Position<E>>posIterator = positions().iterator();\n5public boolean hasNext(){returnposIterator.hasNext(); }\n6publicE next(){returnposIterator.next().getElement(); }// return element!\n7public void remove(){posIterator.remove(); }\n8}\n9\n10/∗∗Returns an iterator of the elements stored in the tree. ∗/\n11publicIterator<E>iterator(){return new ElementIterator(); }\nCode Fragment 8.16: Iterating all elements of an AbstractTree instance, based upon\nan iteration of the positions of the tree.\nTo implement the positions() method, we have a choice of tree traversal algo-\nrithms. Given that there are advantages to each of those traversal orders, we provide\npublic implementations of each strategy that can be called directly by a user of our\nclass. We can then trivially adapt one of those as a default order for the positions\nmethod of the AbstractTree class. For example, on the following page we will de-\nﬁne a public method, preorder() , that returns an iteration of the positions of a tree\nin preorder; Code Fragment 8.17 demonstrates how the positions() method can be\ntrivially deﬁned to rely on that order.\npublicIterable<Position<E>>positions(){returnpreorder();}\nCode Fragment 8.17: Deﬁning preorder as the default traversal algorithm for the\npublicpositions method of an abstract tree.\nwww.it-ebooks.info"
  },
  {
    "page": 358,
    "content": "340 Chapter 8. Trees\nPreorder Traversals\nWe begin by considering the preorder traversal algorithm. Our goal is to provide a\npublic method preorder() , as part of the AbstractTree class, which returns an iter-\nable container of the positions of the tree in preorder. For ease of implementation,\nwe choose to produce a snapshot iterator , as deﬁned in Section 7.4.2, returning\na list of all positions. (Exercise C-8.47 explores the goal of implementing a lazy\niterator that reports positions in preorder.)\nWe begin by deﬁning a private utility method, preorderSubtree , given in Code\nFragment 8.18, which allows us to parameterize the recursive process with a spe-\nciﬁc position of the tree that serves as the root of a subtree to traverse. (We alsopass a list as a parameter that serves as a buffer to which “visited” positions areadded.)\n1/∗∗Adds positions of the subtree rooted at Position p to the given snapshot. ∗/\n2private void preorderSubtree(Position <E>p, List<Position<E>>snapshot){\n3snapshot.add(p); // for preorder, we add position p before exploring subtrees\n4for(Position<E>c : children(p))\n5 preorderSubtree(c, snapshot);\n6}\nCode Fragment 8.18: A recursive subroutine for performing a preorder traversal of\nthe subtree rooted at position pof a tree. This code should be included within the\nbody of the AbstractTree class.\nThepreorderSubtree method follows the high-level algorithm originally de-\nscribed as pseudocode in Code Fragment 8.12. It has an implicit base case, as theforloop body never executes if a position has no children.\nThe public preorder method, shown in Code Fragment 8.19, has the respon-\nsibility of creating an empty list for the snapshot buffer, and invoking the recur-sive method at the root of the tree (assuming the tree is nonempty). We rely on ajava.util.ArrayList instance as an Iterable instance for the snapshot buffer.\n1/∗∗Returns an iterable collection of positions of the tree, reported in preorder. ∗/\n2publicIterable<Position<E>>preorder(){\n3List<Position<E>>snapshot = newArrayList <>();\n4if(!isEmpty())\n5 preorderSubtree(root(), snapshot); // ﬁll the snapshot recursively\n6returnsnapshot;\n7}\nCode Fragment 8.19: A public method that performs a preorder traversal of an entire\ntree. This code should be included within the body of the AbstractTree class.\nwww.it-ebooks.info"
  },
  {
    "page": 359,
    "content": "8.4. Tree Traversal Algorithms 341\nPostorder Traversal\nWe implement a postorder traversal using a similar design as we used for a pre-\norder traversal. The only difference is that a “visited” position is not added to a\npostorder snapshot until after all of its subtrees have been traversed. Both the re-\ncursive utility and the top-level public method are given in Code Fragment 8.20.\n1/∗∗Adds positions of the subtree rooted at Position p to the given snapshot. ∗/\n2private void postorderSubtree(Position <E>p, List<Position<E>>snapshot){\n3for(Position<E>c : children(p))\n4 postorderSubtree(c, snapshot);\n5snapshot.add(p); // for postorder, we add position p after exploring subtrees\n6}\n7/∗∗Returns an iterable collection of positions of the tree, reported in postorder. ∗/\n8publicIterable<Position<E>>postorder(){\n9List<Position<E>>snapshot = newArrayList <>();\n10if(!isEmpty())\n11 postorderSubtree(root(), snapshot); // ﬁll the snapshot recursively\n12returnsnapshot;\n13}\nCode Fragment 8.20: Support for performing a postorder traversal of a tree. This\ncode should be included within the body of the AbstractTree class.\nBreadth-First Traversal\nOn the following page, we will provide an implementation of the breadth-ﬁrsttraversal algorithm in the context of our AbstractTree class (Code Fragment 8.21).\nRecall that the breadth-ﬁrst traversal algorithm is not recursive; it relies on a queue\nof positions to manage the traversal process. We will use the LinkedQueue class\nfrom Section 6.2.3, although any implementation of the queue ADT would sufﬁce.\nInorder Traversal for Binary Trees\nThe preorder, postorder, and breadth-ﬁrst traversal algorithms are applicable to all\ntrees. The inorder traversal algorithm, because it explicitly relies on the notion of\na left and right child of a node, only applies to binary trees. We therefore includeits deﬁnition within the body of the AbstractBinaryTree class. We use a similar\ndesign to our preorder and postorder traversals, with a private recursive utility for\ntraversing subtrees. (See Code Fragment 8.22.)\nFor many applications of binary trees (for example, see Chapter 11), an inorder\ntraversal is the most natural order. Therefore, Code Fragment 8.22 makes it the\ndefault for the AbstractBinaryTree class by overriding the positions method that\nwas inherited from the AbstractTree class. Because the iterator() method relies on\npositions() , it will also use inorder when reporting the elements of a binary tree.\nwww.it-ebooks.info"
  },
  {
    "page": 360,
    "content": "342 Chapter 8. Trees\n1/∗∗Returns an iterable collection of positions of the tree in breadth-ﬁrst order. ∗/\n2publicIterable<Position<E>>breadthﬁrst(){\n3List<Position<E>>snapshot = newArrayList <>();\n4if(!isEmpty()){\n5 Queue<Position<E>>fringe = newLinkedQueue <>();\n6 fringe.enqueue(root()); // start with the root\n7 while(!fringe.isEmpty()) {\n8 Position<E>p = fringe.dequeue(); // remove from front of the queue\n9 snapshot.add(p); // report this position\n10 for(Position<E>c : children(p))\n11 fringe.enqueue(c); // add children to back of queue\n12}\n13}\n14returnsnapshot;\n15}\nCode Fragment 8.21: An implementation of a breadth-ﬁrst traversal of a tree. This\ncode should be included within the body of the AbstractTree class.\n1/∗∗Adds positions of the subtree rooted at Position p to the given snapshot. ∗/\n2private void inorderSubtree(Position <E>p, List<Position<E>>snapshot){\n3if(left(p) != null)\n4 inorderSubtree(left(p), snapshot);\n5snapshot.add(p);\n6if(right(p) != null)\n7 inorderSubtree(right(p), snapshot);\n8}\n9/∗∗Returns an iterable collection of positions of the tree, reported in inorder. ∗/\n10publicIterable<Position<E>>inorder(){\n11List<Position<E>>snapshot = newArrayList <>();\n12if(!isEmpty())\n13 inorderSubtree(root(), snapshot); // ﬁll the snapshot recursively\n14returnsnapshot;\n15}\n16/∗∗Overrides positions to make inorder the default order for binary trees. ∗/\n17publicIterable<Position<E>>positions(){\n18returninorder();\n19}\nCode Fragment 8.22: Support for performing an inorder traversal of a binary tree,\nand for making that order the default traversal for binary trees. This code should be\nincluded within the body of the AbstractBinaryTree class.\nwww.it-ebooks.info"
  },
  {
    "page": 361,
    "content": "8.4. Tree Traversal Algorithms 343\n8.4.5 Applications of Tree Traversals\nIn this section, we demonstrate several representative applications of tree traversals,\nincluding some customizations of the standard traversal algorithms.\nTable of Contents\nWhen using a tree to represent the hierarchical structure of a document, a preorder\ntraversal of the tree can be used to produce a table of contents for the document. For\nexample, the table of contents associated with the tree from Figure 8.13 is displayed\nin Figure 8.18. Part (a) of that ﬁgure gives a simple presentation with one element\nper line; part (b) shows a more attractive presentation, produced by indenting each\nelement based on its depth within the tree.\nPaper Paper\nTitle Title\nAbstract Abstract\n§1 §1\n§1.1 §1.1\n§1.2 §1.2\n§2 §2\n§2.1 §2.1\n... ...\n(a) (b)\nFigure 8.18: Table of contents for a document represented by the tree in Figure 8.13:\n(a) without indentation; (b) with indentation based on depth within the tree.\nThe unindented version of the table of contents can be produced with the fol-\nlowing code, given a tree Tsupporting the preorder() method:\nfor(Position<E>p : T.preorder())\nSystem.out.println(p.getElement());\nTo produce the presentation of Figure 8.18(b), we indent each element with\na number of spaces equal to twice the element’s depth in the tree (hence, the\nroot element was unindented). If we assume that method, spaces( n), produces a\nstring of nspaces, we could replace the body of the above loop with the statement\nSystem.out.println(spaces(2 ∗T.depth(p)) + p.getElement()) . Unfortunately, al-\nthough the work to produce the preorder traversal runs in O(n)time, based on the\nanalysis of Section 8.4.1, the calls to depth incur a hidden cost. Making a call to\ndepth from every position of the tree results in O(n2)worst-case time, as noted\nwhen analyzing the algorithm heightBad in Section 8.1.3.\nwww.it-ebooks.info"
  },
  {
    "page": 362,
    "content": "344 Chapter 8. Trees\nA preferred approach to producing an indented table of contents is to redesign\na top-down recursion that includes the current depth as an additional parameter.\nSuch an implementation is provided in Code Fragment 8.23. This implementationruns in worst-case O(n)time (except, technically, the time it takes to print strings\nof increasing lengths).\n1/∗∗Prints preorder representation of subtree of T rooted at p having depth d. ∗/\n2public static <E>voidprintPreorderIndent(Tree <E>T, Position <E>p,intd){\n3System.out.println(spaces(2 ∗d) + p.getElement()); // indent based on d\n4for(Position<E>c : T.children(p))\n5printPreorderIndent(T, c, d+1); // child depth is d+1\n6}\nCode Fragment 8.23: Efﬁcient recursion for printing indented version of a pre-\norder traversal. To print an entire tree T, the recursion should be started with form\nprintPreorderIndent(T, T.root(), 0) .\nIn the example of Figure 8.18, we were fortunate in that the numbering was\nembedded within the elements of the tree. More generally, we might be interested\nin using a preorder traversal to display the structure of a tree, with indentation andalso explicit numbering that was not present in the tree. For example, we mightdisplay the tree from Figure 8.2 beginning as:\nElectronics R’Us\n1 R&D2 Sales\n2.1 Domestic\n2.2 International\n2.2.1 Canada\n2.2.2 S. America\nThis is more challenging, because the numbers used as labels are implicit in\nthe structure of the tree. A label depends on the path from the root to the currentposition. To accomplish our goal, we add an additional parameter to the recursivesignature. We send a list of integers representing the labels leading to a particular\nposition. For example, when visiting the node Domestic above, we will send the\nlist of values{2,1}that comprise its label.\nAt the implementation level, we wish to avoid the inefﬁciency of duplicating\nsuch lists when sending a new parameter from one level of the recursion to the next.\nA standard solution is to pass the same list instance throughout the recursion. Atone level of the recursion, a new entry is temporarily added to the end of the listbefore making further recursive calls. In order to “leave no trace,” the extraneousentry must later be removed from the list by the same recursive call that added it.\nAn implementation based on this approach is given in Code Fragment 8.24.\nwww.it-ebooks.info"
  },
  {
    "page": 363,
    "content": "8.4. Tree Traversal Algorithms 345\n1/∗∗Prints labeled representation of subtree of T rooted at p having depth d. ∗/\n2public static <E>\n3voidprintPreorderLabeled(Tree <E>T, Position <E>p, ArrayList< Integer>path){\n4intd = path.size(); // depth equals the length of the path\n5System.out.print(spaces(2 ∗d)); // print indentation, then label\n6for(intj=0; j<d; j++) System.out.print(path.get(j) + (j == d −1 ?\" \":\".\"));\n7System.out.println(p.getElement());\n8path.add(1); // add path entry for ﬁrst child\n9for(Position<E>c : T.children(p)) {\n10printPreorderLabeled(T, c, path);\n11path.set(d, 1 + path.get(d)); // increment last entry of path\n12}\n13path.remove(d); // restore path to its incoming state\n14}\nCode Fragment 8.24: Efﬁcient recursion for printing an indented and labeled pre-\nsentation of a preorder traversal.\nComputing Disk Space\nIn Example 8.1, we considered the use of a tree as a model for a ﬁle-system struc-\nture, with internal positions representing directories and leaves representing ﬁles.\nIn fact, when introducing the use of recursion back in Chapter 5, we speciﬁcally\nexamined the topic of ﬁle systems (see Section 5.1.4). Although we did not explic-itly model it as a tree at that time, we gave an implementation of an algorithm forcomputing the disk usage (Code Fragment 5.5).\nThe recursive computation of disk space is emblematic of a postorder traversal,\nas we cannot effectively compute the total space used by a directory until after we\nknow the space that is used by its children directories. Unfortunately, the formalimplementation of postorder , as given in Code Fragment 8.20, does not sufﬁce for\nthis purpose. We would like to have a mechanism for children to return informationto the parent as part of the traversal process. A custom solution to the disk space\nproblem, with each level of recursion providing a return value to the (parent) caller,\nis provided in Code Fragment 8.25.\n1/∗∗Returns total disk space for subtree of T rooted at p. ∗/\n2public static int diskSpace(Tree <Integer>T, Position <Integer>p){\n3intsubtotal = p.getElement(); // we assume element represents space usage\n4for(Position<Integer>c : T.children(p))\n5subtotal += diskSpace(T, c);\n6returnsubtotal;\n7}\nCode Fragment 8.25: Recursive computation of disk space for a tree. We assume\nthat each tree element reports the local space used at that position.\nwww.it-ebooks.info"
  },
  {
    "page": 364,
    "content": "346 Chapter 8. Trees\nParenthetic Representations of a Tree\nIt is not possible to reconstruct a general tree, given only the preorder sequence of\nelements, as in Figure 8.18a. Some additional context is necessary for the structure\nof the tree to be well deﬁned. The use of indentation or numbered labels providessuch context, with a very human-friendly presentation. However, there are moreconcise string representations of trees that are computer-friendly.\nIn this section, we explore one such representation. The parenthetic string\nrepresentation P(T)of tree Tis recursively deﬁned. If Tconsists of a single\nposition p, then P(T)=p.getElement() . Otherwise, it is deﬁned recursively as,\nP(T)=p.getElement() +\"(\"+P(T\n1)+\", \"+···+\", \"+P(Tk)+\")\"\nwhere pis the root of TandT1,T2,..., Tkare the subtrees rooted at the children\nofp, which are given in order if Tis an ordered tree. We are using “ +” here to\ndenote string concatenation. As an example, the parenthetic representation of thetree of Figure 8.2 would appear as follows (line breaks are cosmetic):\nElectronics R’Us (R&D, Sales (Domestic, International (Canada,\nS. America, Overseas (Africa, Europe, Asia, Australia))),Purchasing, Manufacturing (TV, CD, Tuner))\nAlthough the parenthetic representation is essentially a preorder traversal, we\ncannot easily produce the additional punctuation using the formal implementationofpreorder . The opening parenthesis must be produced just before the loop over\na position’s children, the separating commas between children, and the closingparenthesis just after the loop completes. The Java method parenthesize , shown\nin Code Fragment 8.26, is a custom traversal that prints such a parenthetic stringrepresentation of a tree T.\n1/∗∗Prints parenthesized representation of subtree of T rooted at p. ∗/\n2public static <E>voidparenthesize(Tree <E>T, Position <E>p){\n3System.out.print(p.getElement());\n4if(T.isInternal(p)){\n5boolean ﬁrstTime = true;\n6for(Position<E>c : T.children(p)) {\n7 System.out.print( (ﬁrstTime ? \" (\":\", \") );// determine proper punctuation\n8 ﬁrstTime = false; // any future passes will get comma\n9 parenthesize(T, c); // recur on child\n10}\n11System.out.print( \")\");\n12}\n13}\nCode Fragment 8.26: Method that prints parenthetic string representation of a tree.\nwww.it-ebooks.info"
  },
  {
    "page": 365,
    "content": "8.4. Tree Traversal Algorithms 347\nUsing Inorder Traversal for Tree Drawing\nAn inorder traversal can be applied to the problem of computing a graphical layout\nof a binary tree, as shown in Figure 8.19. We assume the convention, common\nto computer graphics, that x-coordinates increase left to right and y-coordinates\nincrease top to bottom, so that the origin is in the upper left corner of the drawing.\n12\n32100 1 2 3 4 5 6 7 8 9 10 11\n4Figure 8.19: An inorder drawing of a binary tree.\nThe geometry is determined by an algorithm that assigns x- and y-coordinates\nto each position pof a binary tree Tusing the following two rules:\n•x(p)is the number of positions visited before pin an inorder traversal of T.\n•y(p)is the depth of pinT.\nCode Fragment 8.27 provides an implementation of a recursive method that\nassigns x- and y-coordinates to positions of a tree in this manner. Depth information\nis passed from one level of the recursion to another, as done in our earlier example\nfor indentation. To maintain an accurate value for the x-coordinate as the traversal\nproceeds, the method must be provided with the value of xthat should be assigned\nto the leftmost node of the current subtree, and it must return to its parent a revisedvalue of xthat is appropriate for the ﬁrst node drawn to the right of the subtree.\n1public static <E>intlayout(BinaryTree <E>T, Position <E>p,intd,intx){\n2if(T.left(p) != null)\n3 x = layout(T, T.left(p), d+1, x); // resulting x will be increased\n4p.getElement().setX(x++); // post-increment x\n5p.getElement().setY(d);\n6if(T.right(p) != null)\n7 x = layout(T, T.right(p), d+1, x); // resulting x will be increased\n8returnx;\n9}\nCode Fragment 8.27: Recursive method for computing coordinates at which to draw\npositions of a binary tree. We assume that the element type for the tree supportssetX andsetY methods. The initial call should be layout(T, T.root(), 0, 0) .\nwww.it-ebooks.info"
  },
  {
    "page": 366,
    "content": "348 Chapter 8. Trees\n8.4.6 Euler Tours\nThe various applications described in Section 8.4.5 demonstrate the great power of\nrecursive tree traversals, but they also show that not every application strictly ﬁts the\nmold of a preorder, postorder, or inorder traversal. We can unify the tree-traversal\nalgorithms into a single framework known as an Euler tour traversal . The Euler\ntour traversal of a tree Tcan be informally deﬁned as a “walk” around T, where\nwe start by going from the root toward its leftmost child, viewing the edges of Tas\nbeing “walls” that we always keep to our left. (See Figure 8.20.)\n3 1 9 5 4 7+ 3 2 − 3 −× + × 6/ +−\nFigure 8.20: Euler tour traversal of a tree.\nThe complexity of the walk is O(n), for a tree with nnodes, because it pro-\ngresses exactly two times along each of the n−1 edges of the tree—once going\ndownward along the edge, and later going upward along the edge. To unify the\nconcept of preorder and postorder traversals, we can view there being two notable\n“visits” to each position p:\n•A “pre visit” occurs when ﬁrst reaching the position, that is, when the walk\npasses immediately leftof the node in our visualization.\n•A “post visit” occurs when the walk later proceeds upward from that position,\nthat is, when the walk passes to the right of the node in our visualization.\nThe process of an Euler tour can be naturally viewed as recursive. In between\nthe “pre visit” and “post visit” of a given position will be a recursive tour of each\nof its subtrees. Looking at Figure 8.20 as an example, there is a contiguous portion\nof the entire tour that is itself an Euler tour of the subtree of the node with element\n“/”. That tour contains two contiguous subtours, one traversing that position’s left\nsubtree and another traversing the right subtree.\nIn the special case of a binary tree, we can designate the time when the walk\npasses immediately below a node as an “in visit” event. This will be just after the\ntour of its left subtree (if any), but before the tour of its right subtree (if any).\nwww.it-ebooks.info"
  },
  {
    "page": 367,
    "content": "8.4. Tree Traversal Algorithms 349\nThe pseudocode for an Euler tour traversal of a subtree rooted at a position pis\nshown in Code Fragment 8.28.\nAlgorithm eulerTour( T,p):\nperform the “pre visit” action for position p\nforeach child cinT.children (p)do\neulerTour( T,c) {recursively tour the subtree rooted at c}\nperform the “post visit” action for position p\nCode Fragment 8.28: Algorithm eulerTour for performing an Euler tour traversal of\na subtree rooted at position pof a tree.\nThe Euler tour traversal extends the preorder and postorder traversals, but it can\nalso perform other kinds of traversals. For example, suppose we wish to compute\nthe number of descendants of each position pin an n-node binary tree. We start an\nEuler tour by initializing a counter to 0, and then increment the counter during the“pre visit” for each position. To determine the number of descendants of a posi-tionp, we compute the difference between the values of the counter from when the\npre-visit occurs and when the post-visit occurs, and add 1 (for p). This simple rule\ngives us the number of descendants of p, because each node in the subtree rooted\natpis counted between p’s visit on the left and p’s visit on the right. Therefore, we\nhave an O(n)-time method for computing the number of descendants of each node.\nFor the case of a binary tree, we can customize the algorithm to include an\nexplicit “in visit” action, as shown in Code Fragment 8.29.\nAlgorithm eulerTourBinary( T,p):\nperform the “pre visit” action for position p\nifphas a left child lcthen\neulerTourBinary (T,lc){recursively tour the left subtree of p}\nperform the “in visit” action for position p\nifphas a right child rcthen\neulerTourBinary (T,rc){recursively tour the right subtree of p}\nperform the “post visit” action for position p\nCode Fragment 8.29: Algorithm eulerTourBinary for performing an Euler tour\ntraversal of a subtree rooted at position pof a binary tree.\nFor example, a binary Euler tour can produce a traditional parenthesized arith-\nmetic expression, such as \"((((3+1)x3)/((9-5)+2))-((3x(7-4))+6))\" for\nthe tree in Figure 8.20, as follows:\n•“Pre visit” action: if the position is internal, print “(”.\n•“In visit” action: print the value or operator stored at the position.\n•“Post visit” action: if the position is internal, print “)”.\nwww.it-ebooks.info"
  },
  {
    "page": 368,
    "content": "350 Chapter 8. Trees\n8.5 Exercises\nReinforcement\nR-8.1 The following questions refer to the tree of Figure 8.3.\na.Which node is the root?\nb.What are the internal nodes?\nc.How many descendants does node cs016/ have?\nd.How many ancestors does node cs016/ have?\ne.What are the siblings of node homeworks/ ?\nf.Which nodes are in the subtree rooted at node projects/ ?\ng.What is the depth of node papers/ ?\nh.What is the height of the tree?\nR-8.2 Show a tree achieving the worst-case running time for algorithm depth .\nR-8.3 Give a justiﬁcation of Proposition 8.3.\nR-8.4 What is the running time of a call to T.height( p)when called on a position p\ndistinct from the root of tree T? (See Code Fragment 8.5.)\nR-8.5 Describe an algorithm, relying only on the BinaryTree operations, that counts the\nnumber of leaves in a binary tree that are the leftchild of their respective parent.\nR-8.6 LetTbe an n-node binary tree that may be improper. Describe how to represent\nTby means of a proper binary tree T′with O(n)nodes.\nR-8.7 What are the minimum and maximum number of internal and external nodes in\nan improper binary tree with nnodes?\nR-8.8 Answer the following questions so as to justify Proposition 8.7.\na.What is the minimum number of external nodes for a proper binary tree\nwith height h? Justify your answer.\nb.What is the maximum number of external nodes for a proper binary tree\nwith height h? Justify your answer.\nc.LetTbe a proper binary tree with height handnnodes. Show that\nlog(n+1)−1≤h≤(n−1)/2.\nd.For which values of nandhcan the above lower and upper bounds on hbe\nattained with equality?\nR-8.9 Give a proof by induction of Proposition 8.8.\nR-8.10 Find the value of the arithmetic expression associated with each subtree of the\nbinary tree of Figure 8.6.\nR-8.11 Draw an arithmetic expression tree that has four external nodes, storing the num-\nbers 1, 5, 6, and 7 (with each number stored in a distinct external node, but not\nnecessarily in this order), and has three internal nodes, each storing an operator\nfrom the set{+,−,∗,/}, so that the value of the root is 21. The operators may\nreturn and act on fractions, and an operator may be used more than once.\nwww.it-ebooks.info"
  },
  {
    "page": 369,
    "content": "8.5. Exercises 351\nR-8.12 Draw the binary tree representation of the following arithmetic expression:\n“(((5+2)∗(2−1))/((2+9)+(( 7−2)−1))∗8)”.\nR-8.13 Justify Table 8.2, summarizing the running time of the methods of a tree repre-\nsented with a linked structure, by providing, for each method, a description of its\nimplementation, and an analysis of its running time.\nR-8.14 LetTbe a binary tree with nnodes, and let f()be the level numbering function\nof the positions of T, as given in Section 8.3.2.\na.Show that, for every position pofT,f(p)≤2n−2.\nb.Show an example of a binary tree with seven nodes that attains the above\nupper bound on f(p)for some position p.\nR-8.15 Show how to use an Euler tour traversal to compute the level number f(p), as\ndeﬁned in Section 8.3.2, of each position in a binary tree T.\nR-8.16 LetTbe a binary tree with npositions that is realized with an array representation\nA, and let f()be the level numbering function of the positions of T, as given in\nSection 8.3.2. Give pseudocode descriptions of each of the methods root,parent,\nleft,right,isExternal , andisRoot .\nR-8.17 Our deﬁnition of the level numbering function f(p), as given in Section 8.3.2, be-\ngins with the root having number 0. Some people prefer to use a level numbering\ng(p)in which the root is assigned number 1, because it simpliﬁes the arithmetic\nfor ﬁnding neighboring positions. Redo Exercise R-8.16, but assuming that we\nuse a level numbering g(p)in which the root is assigned number 1.\nR-8.18 In what order are positions visited during a preorder traversal of the tree of Fig-ure 8.6?\nR-8.19 In what order are positions visited during a postorder traversal of the tree of\nFigure 8.6?\nR-8.20 LetTbe an ordered tree with more than one node. Is it possible that the preorder\ntraversal of Tvisits the nodes in the same order as the postorder traversal of T?\nIf so, give an example; otherwise, explain why this cannot occur. Likewise, is it\npossible that the preorder traversal of Tvisits the nodes in the reverse order of\nthe postorder traversal of T? If so, give an example; otherwise, explain why this\ncannot occur.\nR-8.21 Answer the previous question for the case when Tis a proper binary tree with\nmore than one node.\nR-8.22 Draw a binary tree Tthat simultaneously satisﬁes the following:\n•Each internal node of Tstores a single character.\n•Apreorder traversal of TyieldsEXAMFUN .\n•Aninorder traversal of TyieldsMAFXUEN .\nR-8.23 Consider the example of a breadth-ﬁrst traversal given in Figure 8.15. Using theannotated numbers from that ﬁgure, describe the contents of the queue beforeeach pass of the while loop in Code Fragment 8.14. To get started, the queue has\ncontents{1}before the ﬁrst pass, and contents {2,3,4}before the second pass.\nwww.it-ebooks.info"
  },
  {
    "page": 370,
    "content": "352 Chapter 8. Trees\nR-8.24 Give the output of the method parenthesize(T, T.root()) , as described in Code\nFragment 8.26, when Tis the tree of Figure 8.6.\nR-8.25 Describe a modiﬁcation to parenthesize , from Code Fragment 8.26, that relies on\nthelength() method for the String class to output the parenthetic representation\nof a tree with line breaks added to display the tree in a text window that is 80\ncharacters wide.\nR-8.26 What is the running time of parenthesize(T, T.root()) , as given in Code Frag-\nment 8.26, for a tree Twith nnodes?\nCreativity\nC-8.27 Describe an efﬁcient algorithm for converting a fully balanced string of paren-\ntheses into an equivalent tree. The tree associated with such a string is deﬁned\nrecursively. The outermost pair of balanced parentheses is associated with the\nroot and each substring inside this pair, deﬁned by the substring between two\nbalanced parentheses, is associated with a subtree of this root.\nC-8.28 Thepath length of a tree Tis the sum of the depths of all positions in T. Describe\na linear-time method for computing the path length of a tree T.\nC-8.29 Deﬁne the internal path length ,I(T), of a tree Tto be the sum of the depths of\nall the internal positions in T. Likewise, deﬁne the external path length ,E(T),\nof a tree Tto be the sum of the depths of all the external positions in T. Show\nthat if Tis a proper binary tree with npositions, then E(T)=I(T)+n−1.\nC-8.30 LetTbe a (not necessarily proper) binary tree with nnodes, and let Dbe the sum\nof the depths of all the external nodes of T. Show that if Thas the minimum\nnumber of external nodes possible, then DisO(n)and if Thas the maximum\nnumber of external nodes possible, then DisO(nlogn).\nC-8.31 LetTbe a (possibly improper) binary tree with nnodes, and let Dbe the sum of\nthe depths of all the external nodes of T. Describe a conﬁguration for Tsuch that\nDisΩ(n2). Such a tree would be the worst case for the asymptotic running time\nof method heightBad (Code Fragment 8.4).\nC-8.32 For a tree T, letnIdenote the number of its internal nodes, and let nEdenote the\nnumber of its external nodes. Show that if every internal node in Thas exactly 3\nchildren, then nE=2nI+1.\nC-8.33 Two ordered trees T′andT′′are said to be isomorphic if one of the following\nholds:\n•Both T′andT′′are empty.\n•Both T′andT′′consist of a single node\n•The roots of T′andT′′have the same number k≥1 of subtrees, and the\nithsuch subtree of T′is isomorphic to the ithsuch subtree of T′′fori=\n1,..., k.\nDesign an algorithm that tests whether two given ordered trees are isomorphic.\nWhat is the running time of your algorithm?\nwww.it-ebooks.info"
  },
  {
    "page": 371,
    "content": "8.5. Exercises 353\nC-8.34 Show that there are more than 2nimproper binary trees with ninternal nodes\nsuch that no pair are isomorphic (see Exercise C-8.33).\nC-8.35 If we exclude isomorphic trees (see Exercise C-8.33), exactly how many proper\nbinary trees exist with exactly 4 leaves?\nC-8.36 Add support in LinkedBinaryTree for a method, pruneSubtree( p), that removes\nthe entire subtree rooted at position p, making sure to maintain an accurate count\nof the size of the tree. What is the running time of your implementation?\nC-8.37 Add support in LinkedBinaryTree for a method, swap( p,q), that has the effect of\nrestructuring the tree so that the node referenced by ptakes the place of the node\nreferenced by q, and vice versa. Make sure to properly handle the case when the\nnodes are adjacent.\nC-8.38 We can simplify parts of our LinkedBinaryTree implementation if we make use\nof of a single sentinel node, such that the sentinel is the parent of the real root ofthe tree, and the root is referenced as the left child of the sentinel. Furthermore,\nthe sentinel will take the place of nullas the value of the leftorright member for\na node without such a child. Give a new implementation of the update methods\nremove andattach , assuming such a representation.\nC-8.39 Describe how to clone a LinkedBinaryTree instance representing a proper binary\ntree, with use of the attach method.\nC-8.40 Describe how to clone a LinkedBinaryTree instance representing a (not necessar-\nily proper) binary tree, with use of the addLeft andaddRight methods.\nC-8.41 Modify the LinkedBinaryTree class to formally support the Cloneable interface,\nas described in Section 3.6.\nC-8.42 Give an efﬁcient algorithm that computes and prints, for every position pof a\ntreeT, the element of pfollowed by the height of p’s subtree.\nC-8.43 Give an O(n)-time algorithm for computing the depths of all positions of a tree\nT, where nis the number of nodes of T.\nC-8.44 The balance factor of an internal position pof a proper binary tree is the dif-\nference between the heights of the right and left subtrees of p. Show how to\nspecialize the Euler tour traversal of Section 8.4.6 to print the balance factors of\nall the internal nodes of a proper binary tree.\nC-8.45 Design algorithms for the following operations for a binary tree T:\n•preorderNext( p): Return the position visited after pin a preorder traversal\nofT(ornullifpis the last node visited).\n•inorderNext( p): Return the position visited after pin an inorder traversal\nofT(ornullifpis the last node visited).\n•postorderNext( p): Return the position visited after pin a postorder traver-\nsal of T(ornullifpis the last node visited).\nWhat are the worst-case running times of your algorithms?\nC-8.46 Describe, in pseudocode, a nonrecursive method for performing an inorder traver-sal of a binary tree in linear time.\nwww.it-ebooks.info"
  },
  {
    "page": 372,
    "content": "354 Chapter 8. Trees\nC-8.47 To implement the preorder method of the AbstractTree class, we relied on the\nconvenience of creating a snapshot. Reimplement a preorder method that creates\nalazy iterator . (See Section 7.4.2 for discussion of iterators.)\nC-8.48 Repeat Exercise C-8.47, implementing the postorder method of the AbstractTree\nclass.\nC-8.49 Repeat Exercise C-8.47, implementing the AbstractBinaryTree ’sinorder method.\nC-8.50 Algorithm preorderDraw draws a binary tree Tby assigning x- and y-coordinates\nto each position psuch that x(p)is the number of nodes preceding pin the\npreorder traversal of Tandy(p)is the depth of pinT.\na.Show that the drawing of Tproduced by preorderDraw has no pairs of\ncrossing edges.\nb.Redraw the binary tree of Figure 8.19 using preorderDraw .\nC-8.51 Redo the previous problem for the algorithm postorderDraw that is similar to\npreorderDraw except that it assigns x(p)to be the number of nodes preceding\nposition pin the postorder traversal.\nC-8.52 We can deﬁne a binary tree representation T′for an ordered general tree Tas\nfollows (see Figure 8.21):\n•For each position pofT, there is an associated position p′ofT′.\n•Ifpis a leaf of T, then p′inT′does not have a left child; otherwise the left\nchild of p′isq′, where qis the ﬁrst child of pinT.\n•Ifphas a sibling qordered immediately after it in T, then q′is the right\nchild of p′inT; otherwise p′does not have a right child.\nGiven such a representation T′of a general ordered tree T, answer each of the\nfollowing questions:\na.Is a preorder traversal of T′equivalent to a preorder traversal of T?\nb.Is a postorder traversal of T′equivalent to a postorder traversal of T?\nc.Is an inorder traversal of T′equivalent to one of the standard traversals\nofT? If so, which one?\nD\nF G ECA\nBA\nD F\nGE CB\n(a) (b)\nFigure 8.21: Representation of a tree with a binary tree: (a) tree T; (b) binary tree\nT′forT. The dashed edges connect nodes of T′that are siblings in T.\nwww.it-ebooks.info"
  },
  {
    "page": 373,
    "content": "8.5. Exercises 355\nC-8.53 Design an algorithm for drawing general trees, using a style similar to the inorder\ntraversal approach for drawing binary trees.\nC-8.54 Let the rank of a position pduring a traversal be deﬁned such that the ﬁrst ele-\nment visited has rank 1, the second element visited has rank 2, and so on. For\neach position pin a tree T, letpre(p)be the rank of pin a preorder traversal of\nT, letpost( p)be the rank of pin a postorder traversal of T, letdepth(p)be the\ndepth of p, and letdesc(p)be the number of descendants of p, including pitself.\nDerive a formula deﬁning post( p)in terms of desc(p),depth(p), andpre(p), for\neach node pinT.\nC-8.55 LetTbe a tree with npositions. Deﬁne the lowest common ancestor (LCA)\nbetween two positions pandqas the lowest position in Tthat has both pandq\nas descendants (where we allow a position to be a descendant of itself). Given\ntwo positions pandq, describe an efﬁcient algorithm for ﬁnding the LCA of p\nandq. What is the running time of your algorithm?\nC-8.56 Suppose each position pof a binary tree Tis labeled with its value f(p)in a\nlevel numbering of T. Design a fast method for determining f(a)for the lowest\ncommon ancestor (LCA), a, of two positions pandqinT, given f(p)andf(q).\nYou do not need to ﬁnd position a, just value f(a).\nC-8.57 LetTbe a binary tree with npositions, and, for any position pinT, let dp\ndenote the depth of pinT. The distance between two positions pandqinTis\ndp+dq−2da, where ais the lowest common ancestor (LCA) of pandq. The\ndiameter ofTis the maximum distance between two positions in T. Describe\nan efﬁcient algorithm for ﬁnding the diameter of T. What is the running time of\nyour algorithm?\nC-8.58 Theindented parenthetic representation of a tree Tis a variation of the paren-\nthetic representation of T(see Code Fragment 8.26) that uses indentation and\nline breaks as illustrated in Figure 8.22. Give an algorithm that prints this repre-\nsentation of a tree.\nEurope Asia Africa AustraliaCanada Overseas S. AmericaDomestic InternationalSalesSales (\nDomesticInternational (\nCanadaS. AmericaOverseas (\nAfricaEurope\nAsia\nAustralia\n)\n)\n)\n(a) (b)\nFigure 8.22: (a) Tree T; (b) indented parenthetic representation of T.\nwww.it-ebooks.info"
  },
  {
    "page": 374,
    "content": "356 Chapter 8. Trees\nC-8.59 As mentioned in Exercise C-6.19, postﬁx notation is an unambiguous way of\nwriting an arithmetic expression without parentheses. It is deﬁned so that if\n“(exp1)op(exp2)” is a normal (inﬁx) fully parenthesized expression with opera-\ntionop, then its postﬁx equivalent is “ pexp1pexp2op”, where pexp1is the postﬁx\nversion of exp1andpexp2is the postﬁx version of exp2. The postﬁx version of\na single number or variable is just that number or variable. So, for example, the\npostﬁx version of the inﬁx expression “ ((5+2)∗(8−3))/4” is “5 2 +8 3−∗\n4/”. Give an efﬁcient algorithm for converting an inﬁx arithmetic expression to\nits equivalent postﬁx notation. (Hint: First convert the inﬁx expression into its\nequivalent binary tree representation.)\nC-8.60 LetTbe a binary tree with npositions. Deﬁne a Roman position to be a position\npinT, such that the number of descendants in p’s left subtree differ from the\nnumber of descendants in p’s right subtree by at most 5. Describe a linear-time\nmethod for ﬁnding each position pofT, such that pis not a Roman position, but\nall of p’s descendants are Roman.\nProjects\nP-8.61 Implement the binary tree ADT using the array-based representation described\nin Section 8.3.2.\nP-8.62 Implement the tree ADT using a linked structure as described in Section 8.3.3.\nProvide a reasonable set of update methods for your tree.\nP-8.63 Implement the tree ADT using the binary tree representation described in Exer-\ncise C-8.52. You may adapt the LinkedBinaryTree implementation.\nP-8.64 The memory usage for the LinkedBinaryTree class can be streamlined by remov-\ning the parent reference from each node, and instead implementing a Position as\nan object that keeps a list of nodes representing the entire path from the root to\nthat position. Reimplement the LinkedBinaryTree class using this strategy.\nP-8.65 Write a program that takes as input a fully parenthesized, arithmetic expression\nand converts it to a binary expression tree. Your program should display the tree\nin some way and also print the value associated with the root. For an additional\nchallenge, allow the leaves to store variables of the form x1,x2,x3, and so on,\nwhich are initially 0 and which can be updated interactively by your program,\nwith the corresponding update in the printed value of the root of the expression\ntree.\nP-8.66 Aslicing ﬂoor plan divides a rectangle with horizontal and vertical sides using\nhorizontal and vertical cuts. (See Figure 8.23a.) A slicing ﬂoor plan can be\nrepresented by a proper binary tree, called a slicing tree , whose internal nodes\nrepresent the cuts, and whose external nodes represent the basic rectangles into\nwhich the ﬂoor plan is decomposed by the cuts. (See Figure 8.23b.) The com-\npaction problem for a slicing ﬂoor plan is deﬁned as follows. Assume that each\nbasic rectangle of a slicing ﬂoor plan is assigned a minimum width wand a min-\nimum height h. The compaction problem is to ﬁnd the smallest possible height\nwww.it-ebooks.info"
  },
  {
    "page": 375,
    "content": "8.5. Exercises 357\nA\nBC DE F\nDE F\nB\nCA\n(a) (b)\nFigure 8.23: (a) Slicing ﬂoor plan; (b) slicing tree associated with the ﬂoor plan.\nand width for each rectangle of the slicing ﬂoor plan that is compatible with the\nminimum dimensions of the basic rectangles. Namely, this problem requires the\nassignment of values h(p)andw(p)to each position pof the slicing tree such\nthat:\nw(p)=\n\nwifpis a leaf whose basic rectangle has mini-\nmum width w\nmax(w(ℓ),w(r))ifpis an internal position, associated with\na horizontal cut, with left child ℓand right\nchild r\nw(ℓ)+w(r)ifpis an internal position, associated with a\nvertical cut, with left child ℓand right child r\nh(p)=\n\nhifpis a leaf node whose basic rectangle has\nminimum height h\nh(ℓ)+h(r)ifpis an internal position, associated with\na horizontal cut, with left child ℓand right\nchild r\nmax(h(ℓ),h(r))ifpis an internal position, associated with a\nvertical cut, with left child ℓand right child r\nDesign a data structure for slicing ﬂoor plans that supports the operations:\n•Create a ﬂoor plan consisting of a single basic rectangle.\n•Decompose a basic rectangle by means of a horizontal cut.\n•Decompose a basic rectangle by means of a vertical cut.\n•Assign minimum height and width to a basic rectangle.\n•Draw the slicing tree associated with the ﬂoor plan.\n•Compact and draw the ﬂoor plan.\nwww.it-ebooks.info"
  },
  {
    "page": 376,
    "content": "358 Chapter 8. Trees\nP-8.67 Write a program that can play Tic-Tac-Toe effectively. (See Section 3.1.5.) To do\nthis, you will need to create a game tree T, which is a tree where each position\ncorresponds to a game conﬁguration , which, in this case, is a representation of\nthe Tic-Tac-Toe board. (See Section 8.4.2.) The root corresponds to the initial\nconﬁguration. For each internal position pinT, the children of pcorrespond\nto the game states we can reach from p’s game state in a single legal move for\nthe appropriate player, A(the ﬁrst player) or B(the second player). Positions at\neven depths correspond to moves for Aand positions at odd depths correspond to\nmoves for B. Leaves are either ﬁnal game states or are at a depth beyond which\nwe do not want to explore. We score each leaf with a value that indicates how\ngood this state is for player A. In large games, like chess, we have to use a heuris-\ntic scoring function, but for small games, like Tic-Tac-Toe, we can construct the\nentire game tree and score leaves as +1, 0,−1, indicating whether player Ahas\na win, draw, or lose in that conﬁguration. A good algorithm for choosing moves\nisminimax . In this algorithm, we assign a score to each internal position pin\nT, such that if prepresents A’s turn, we compute p’s score as the maximum of\nthe scores of p’s children (which corresponds to A’s optimal play from p). If an\ninternal node prepresents B’s turn, then we compute p’s score as the minimum\nof the scores of p’s children (which corresponds to B’s optimal play from p).\nP-8.68 Write a program that takes as input a general tree Tand a position pofTand\nconverts Tto another tree with the same set of position adjacencies, but now with\npas its root.\nP-8.69 Write a program that draws a binary tree.\nP-8.70 Write a program that draws a general tree.\nP-8.71 Write a program that can input and display a person’s family tree.\nP-8.72 Write a program that visualizes an Euler tour traversal of a proper binary tree,\nincluding the movements from node to node and the actions associated with visits\non the left, from below, and on the right. Illustrate your program by having it\ncompute and display preorder labels, inorder labels, postorder labels, ancestor\ncounts, and descendant counts for each node in the tree (not necessarily all at the\nsame time).\nChapter Notes\nDiscussions of the classic preorder, inorder, and postorder tree traversal methods can be\nfound in Knuth’s Fundamental Algorithms book [60]. The Euler tour traversal technique\ncomes from the parallel algorithms community; it is introduced by Tarjan and Vishkin [86]\nand is discussed by J´ aJ´ a [50] and by Karp and Ramachandran [55]. The algorithm for\ndrawing a tree is generally considered to be a part of the “folklore” of graph-drawing al-\ngorithms. The reader interested in graph drawing is referred to the book by Di Battista,\nEades, Tamassia, and Tollis [29] and the survey by Tamassia and Liotta [85]. The puzzle\nin Exercise R-8.11 was communicated by Micha Sharir.\nwww.it-ebooks.info"
  },
  {
    "page": 377,
    "content": "Chapter\n9Priority Queues\nContents\n9.1 The Priority Queue Abstract Data Type . . . . . . . . . . . 360\n9.1.1 Priorities . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\n9.1.2 The Priority Queue ADT . . . . . . . . . . . . . . . . . . 361\n9.2 Implementing a Priority Queue . . . . . . . . . . . . . . . . 362\n9.2.1 The Entry Composite . . . . . . . . . . . . . . . . . . . . 362\n9.2.2 Comparing Keys with Total Orders . . . . . . . . . . . . . 363\n9.2.3 The AbstractPriorityQueue Base Class . . . . . . . . . . . 364\n9.2.4 Implementing a Priority Queue with an Unsorted List . . . 366\n9.2.5 Implementing a Priority Queue with a Sorted List . . . . . 368\n9.3 Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n9.3.1 The Heap Data Structure . . . . . . . . . . . . . . . . . . 370\n9.3.2 Implementing a Priority Queue with a Heap . . . . . . . . 372\n9.3.3 Analysis of a Heap-Based Priority Queue . . . . . . . . . . 379\n9.3.4 Bottom-Up Heap Construction ⋆. . . . . . . . . . . . . 380\n9.3.5 Using the java.util.PriorityQueue Class . . . . . . . . . . . 384\n9.4 Sorting with a Priority Queue . . . . . . . . . . . . . . . . . 385\n9.4.1 Selection-Sort and Insertion-Sort . . . . . . . . . . . . . . 386\n9.4.2 Heap-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n9.5 Adaptable Priority Queues . . . . . . . . . . . . . . . . . . 390\n9.5.1 Location-Aware Entries . . . . . . . . . . . . . . . . . . . 391\n9.5.2 Implementing an Adaptable Priority Queue . . . . . . . . 392\n9.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\nwww.it-ebooks.info"
  },
  {
    "page": 378,
    "content": "360 Chapter 9. Priority Queues\n9.1 The Priority Queue Abstract Data Type\n9.1.1 Priorities\nIn Chapter 6, we introduced the queue ADT as a collection of objects that are\nadded and removed according to the ﬁrst-in, ﬁrst-out (FIFO ) principle. A com-\npany’s customer call center embodies such a model in which waiting customers are\ntold “calls will be answered in the order that they were received.” In that setting, a\nnew call is added to the back of the queue, and each time a customer service rep-\nresentative becomes available, he or she is connected with the call that is removed\nfrom the front of the call queue.\nIn practice, there are many applications in which a queue-like structure is used\nto manage objects that must be processed in some way, but for which the ﬁrst-in,\nﬁrst-out policy does not sufﬁce. Consider, for example, an air-trafﬁc control center\nthat has to decide which ﬂight to clear for landing from among many approaching\nthe airport. This choice may be inﬂuenced by factors such as each plane’s distance\nfrom the runway, time spent waiting in a holding pattern, or amount of remaining\nfuel. It is unlikely that the landing decisions are based purely on a FIFO policy.\nThere are other situations in which a “ﬁrst come, ﬁrst serve” policy might seem\nreasonable, yet for which other priorities come into play. To use another airline\nanalogy, suppose a certain ﬂight is fully booked an hour prior to departure. Be-\ncause of the possibility of cancellations, the airline maintains a queue of standby\npassengers hoping to get a seat. Although the priority of a standby passenger is\ninﬂuenced by the check-in time of that passenger, other considerations include the\nfare paid and frequent-ﬂyer status. So it may be that an available seat is given to\na passenger who has arrived later than another, if such a passenger is assigned a\nbetter priority by the airline agent.\nIn this chapter, we introduce a new abstract data type known as a priority queue .\nThis is a collection of prioritized elements that allows arbitrary element insertion,\nand allows the removal of the element that has ﬁrst priority. When an element is\nadded to a priority queue, the user designates its priority by providing an associ-\nated key. The element with the minimal key will be the next to be removed from\nthe queue (thus, an element with key 1 will be given priority over an element with\nkey 2). Although it is quite common for priorities to be expressed numerically, any\nJava object may be used as a key, as long as there exists means to compare any two\ninstances aandb, in a way that deﬁnes a natural order of the keys. With such gen-\nerality, applications may develop their own notion of priority for each element. For\nexample, different ﬁnancial analysts may assign different ratings (i.e., priorities) to\na particular asset, such as a share of stock.\nwww.it-ebooks.info"
  },
  {
    "page": 379,
    "content": "9.1. The Priority Queue Abstract Data Type 361\n9.1.2 The Priority Queue ADT\nWe model an element and its priority as a key-value composite known as an entry .\n(However, we defer until Section 9.2.1 the technical deﬁnition of the Entry type.)\nWe deﬁne the priority queue ADT to support the following methods:\ninsert( k,v):Creates an entry with key kand value vin the priority queue.\nmin() :Returns (but does not remove) a priority queue entry ( k,v)\nhaving minimal key; returns nullif the priority queue is empty.\nremoveMin() :Removes and returns an entry ( k,v) having minimal key from\nthe priority queue; returns nullif the priority queue is empty.\nsize() :Returns the number of entries in the priority queue.\nisEmpty() :Returns a boolean indicating whether the priority queue is\nempty.\nA priority queue may have multiple entries with equivalent keys, in which case\nmethods min andremoveMin may report an arbitrary choice among those entry\nhaving minimal key. Values may be any type of object.\nIn our initial model for a priority queue, we assume that an element’s key re-\nmains ﬁxed once it has been added to a priority queue. In Section 9.5, we consider\nan extension that allows a user to update an element’s key within the priority queue.\nExample 9.1: The following table shows a series of operations and their effects\non an initially empty priority queue. The “Priority Queue Contents” column is\nsomewhat deceiving since it shows the entries sorted by key. Such an internal\nrepresentation is not required of a priority queue.\nMethod Return Value Priority Queue Contents\ninsert(5,A) {(5,A)}\ninsert(9,C) {(5,A), (9,C)}\ninsert(3,B) {(3,B), (5,A), (9,C) }\nmin() (3,B){(3,B), (5,A), (9,C) }\nremoveMin() (3,B){(5,A), (9,C)}\ninsert(7,D) {(5,A), (7,D), (9,C) }\nremoveMin() (5,A){(7,D), (9,C)}\nremoveMin() (7,D) {(9,C)}\nremoveMin() (9,C) { }\nremoveMin() null { }\nisEmpty() true { }\nwww.it-ebooks.info"
  },
  {
    "page": 380,
    "content": "362 Chapter 9. Priority Queues\n9.2 Implementing a Priority Queue\nIn this section, we discuss several technical issues involving the implementation of\nthe priority queue ADT in Java, and we deﬁne an abstract base class that provides\nfunctionality that is shared by all priority queue implementations in this chapter. We\nthen provide two concrete priority queue implementations using a positional list L\n(see Section 7.3) for storage. They differ in whether or not entries are maintained\nin sorted order according to their keys.\n9.2.1 The Entry Composite\nOne challenge in implementing a priority queue is that we must keep track of both\nan element and its key, even as entries are relocated within a data structure. This\nis reminiscent of a case study from Section 7.7 in which we maintain a list of\nelements with access frequencies. In that setting, we introduced the composition\ndesign pattern , deﬁning an Item class that paired each element with its associated\ncount in our primary data structure. For priority queues, we use composition to\npair a key kand a value vas a single object. To formalize this, we deﬁne the public\ninterface, Entry , shown in Code Fragment 9.1.\n1/∗∗Interface for a key-value pair. ∗/\n2public interface Entry<K,V>{\n3K getKey(); // returns the key stored in this entry\n4V getValue(); // returns the value stored in this entry\n5}\nCode Fragment 9.1: Java interface for an entry storing a key-value pair.\nWe then use the Entry type in the formal interface for the priority queue, shown\nin Code Fragment 9.2. This allows us to return both a key and value as a single\nobject from methods such as minandremoveMin . We also deﬁne the insert method\nto return an entry; in a more advanced adaptable priority queue (see Section 9.5),\nthat entry can be subsequently updated or removed.\n1/∗∗Interface for the priority queue ADT. ∗/\n2public interface PriorityQueue <K,V>{\n3intsize();\n4boolean isEmpty();\n5Entry<K,V>insert(K key, V value) throwsIllegalArgumentException;\n6Entry<K,V>min();\n7Entry<K,V>removeMin();\n8}\nCode Fragment 9.2: Java interface for the priority queue ADT.\nwww.it-ebooks.info"
  },
  {
    "page": 381,
    "content": "9.2. Implementing a Priority Queue 363\n9.2.2 Comparing Keys with Total Orders\nIn deﬁning the priority queue ADT, we can allow any type of object to serve as a\nkey, but we must be able to compare keys to each other in a meaningful way. More\nso, the results of the comparisons must not be contradictory. For a comparison rule,\nwhich we denote by ≤, to be self-consistent, it must deﬁne a total order relation,\nwhich is to say that it satisﬁes the following properties for any keys k1,k2, and k3:\n•Comparability property: k1≤k2ork2≤k1.\n•Antisymmetric property : ifk1≤k2andk2≤k1, then k1=k2.\n•Transitive property : ifk1≤k2andk2≤k3, then k1≤k3.\nThe comparability property states that comparison rule is deﬁned for every pair of\nkeys. Note that this property implies the following one:\n•Reﬂexive property :k≤k.\nA comparison rule, ≤, that deﬁnes a total order relation will never lead to a con-\ntradiction. Such a rule deﬁnes a linear ordering among a set of keys; hence, if a\n(ﬁnite) set of elements has a total order deﬁned for it, then the notion of a minimal\nkey,kmin, is well deﬁned, as a key in which kmin≤k, for any other key kin our set.\nThe Comparable Interface\nJava provides two means for deﬁning comparisons between object types. The ﬁrst\nof these is that a class may deﬁne what is known as the natural ordering of its\ninstances by formally implementing the java.lang.Comparable interface, which in-\ncludes a single method, compareTo . The syntax a.compareTo( b)must return an\ninteger iwith the following meaning:\n•i<0 designates that a<b.\n•i=0 designates that a=b.\n•i>0 designates that a>b.\nFor example, the compareTo method of the String class deﬁnes the natural\nordering of strings to be lexicographic , which is a case-sensitive extension of the\nalphabetic ordering to Unicode.\nThe Comparator Interface\nIn some applications, we may want to compare objects according to some notion\nother than their natural ordering. For example, we might be interested in which\nof two strings is the shortest, or in deﬁning our own complex rules for judging\nwhich of two stocks is more promising. To support generality, Java deﬁnes the\njava.util.Comparator interface. A comparator is an object that is external to the\nclass of the keys it compares. It provides a method with the signature compare( a,b)\nthat returns an integer with similar meaning to the compareTo method described\nabove.\nwww.it-ebooks.info"
  },
  {
    "page": 382,
    "content": "364 Chapter 9. Priority Queues\nAs a concrete example, Code Fragment 9.3 deﬁnes a comparator that evaluates\nstrings based on their length (rather than their natural lexicographic order).\n1public class StringLengthComparator implements Comparator <String>{\n2/∗∗Compares two strings according to their lengths. ∗/\n3public int compare(String a, String b) {\n4if(a.length() <b.length()) return−1;\n5else if(a.length() == b.length()) return0;\n6else return 1;\n7}\n8}\nCode Fragment 9.3: A comparator that evaluates strings based on their lengths.\nComparators and the Priority Queue ADT\nFor a general and reusable form of a priority queue, we allow a user to choose\nany key type and to send an appropriate comparator instance as a parameter to the\npriority queue constructor. The priority queue will use that comparator anytime it\nneeds to compare two keys to each other.\nFor convenience, we also allow a default priority queue to instead rely on the\nnatural ordering for the given keys (assuming those keys come from a comparable\nclass). In that case, we build our own instance of a DefaultComparator class, shown\nin Code Fragment 9.4.\n1public class DefaultComparator <E>implements Comparator <E>{\n2public int compare(E a, E b) throwsClassCastException {\n3return((Comparable <E>) a).compareTo(b);\n4}\n5}\nCode Fragment 9.4: ADefaultComparator class that implements a comparator\nbased upon the natural ordering of its element type.\n9.2.3 The AbstractPriorityQueue Base Class\nTo manage technical issues common to all our priority queue implementations, we\ndeﬁne an abstract base class named AbstractPriorityQueue in Code Fragment 9.5.\n(See Section 2.3.3 for a discussion of abstract base classes.) This includes a nested\nPQEntry class that implements the public Entry interface.\nOur abstract class also declares and initializes an instance variable, comp , that\nstores the comparator being used for the priority queue. We then provide a protected\nmethod,compare , that invokes the comparator on the keys of two given entries.\nwww.it-ebooks.info"
  },
  {
    "page": 383,
    "content": "9.2. Implementing a Priority Queue 365\n1/∗∗An abstract base class to assist implementations of the PriorityQueue interface. ∗/\n2public abstract class AbstractPriorityQueue <K,V>\n3 implements PriorityQueue <K,V>{\n4//---------------- nested PQEntry class ----------------\n5protected static class PQEntry<K,V>implements Entry<K,V>{\n6privateK k;// key\n7privateV v;// value\n8publicPQEntry(K key, V value) {\n9 k = key;\n10 v = value;\n11}\n12// methods of the Entry interface\n13publicK getKey(){returnk;}\n14publicV getValue(){returnv;}\n15// utilities not exposed as part of the Entry interface\n16protected void setKey(K key){k = key;}\n17protected void setValue(V value) {v = value;}\n18}//----------- end of nested PQEntry class -----------\n19\n20// instance variable for an AbstractPriorityQueue\n21/∗∗The comparator deﬁning the ordering of keys in the priority queue. ∗/\n22privateComparator <K>comp;\n23/∗∗Creates an empty priority queue using the given comparator to order keys. ∗/\n24protected AbstractPriorityQueue(Comparator <K>c){comp = c;}\n25/∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/\n26protected AbstractPriorityQueue() {this(newDefaultComparator <K>());}\n27/∗∗Method for comparing two entries according to key ∗/\n28protected int compare(Entry <K,V>a, Entry<K,V>b){\n29returncomp.compare(a.getKey(), b.getKey());\n30}\n31/∗∗Determines whether a key is valid. ∗/\n32protected boolean checkKey(K key) throwsIllegalArgumentException {\n33try{\n34 return(comp.compare(key,key) == 0); // see if key can be compared to itself\n35}catch(ClassCastException e) {\n36 throw new IllegalArgumentException( \"Incompatible key\" );\n37}\n38}\n39/∗∗Tests whether the priority queue is empty. ∗/\n40public boolean isEmpty(){returnsize() == 0;}\n41}\nCode Fragment 9.5: TheAbstractPriorityQueue class. This provides a nested\nPQEntry class that composes a key and a value into a single object, and support\nfor managing a comparator. For convenience, we also provide an implementation\nofisEmpty based on a presumed sizemethod.\nwww.it-ebooks.info"
  },
  {
    "page": 384,
    "content": "366 Chapter 9. Priority Queues\n9.2.4 Implementing a Priority Queue with an Unsorted List\nIn our ﬁrst concrete implementation of a priority queue, we store entries within an\nunsorted linked list. Code Fragment 9.6 presents our UnsortedPriorityQueue class\nas a subclass of the AbstractPriorityQueue class (from Code Fragment 9.5). For\ninternal storage, key-value pairs are represented as composites, using instances of\nthe inherited PQEntry class. These entries are stored within a PositionalList that\nis an instance variable. We assume that the positional list is implemented with a\ndoubly linked list, as in Section 7.3, so that all operations of that ADT execute in\nO(1)time.\nWe begin with an empty list when a new priority queue is constructed. At all\ntimes, the size of the list equals the number of key-value pairs currently stored in\nthe priority queue. For this reason, our priority queue sizemethod simply returns\nthe length of the internal list. By the design of our AbstractPriorityQueue class,\nwe inherit a concrete implementation of the isEmpty method that relies on a call to\noursizemethod.\nEach time a key-value pair is added to the priority queue, via the insert method,\nwe create a new PQEntry composite for the given key and value, and add that entry\nto the end of the list. Such an implementation takes O(1)time.\nThe remaining challenge is that when min orremoveMin is called, we must\nlocate the entry with minimal key. Because the entries are not sorted, we must\ninspect all entries to ﬁnd one with a minimal key. For convenience, we deﬁne\na private ﬁndMin utility that returns the position of an entry with minimal key.\nKnowledge of the position allows the removeMin method to invoke the remove\nmethod on the positional list. The minmethod simply uses the position to retrieve\nthe entry when preparing a key-value tuple to return. Due to the loop for ﬁnding\nthe minimal key, both minandremoveMin methods run in O(n)time, where nis\nthe number of entries in the priority queue.\nA summary of the running times for the UnsortedPriorityQueue class is given\nin Table 9.1.\nMethod Running Time\nsize O(1)\nisEmpty O(1)\ninsert O(1)\nmin O(n)\nremoveMin O(n)\nTable 9.1: Worst-case running times of the methods of a priority queue of size\nn, realized by means of an unsorted, doubly linked list. The space requirement\nisO(n).\nwww.it-ebooks.info"
  },
  {
    "page": 385,
    "content": "9.2. Implementing a Priority Queue 367\n1/∗∗An implementation of a priority queue with an unsorted list. ∗/\n2public class UnsortedPriorityQueue <K,V>extends AbstractPriorityQueue <K,V>{\n3/∗∗primary collection of priority queue entries ∗/\n4privatePositionalList <Entry<K,V>>list =newLinkedPositionalList <>();\n5\n6/∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/\n7publicUnsortedPriorityQueue() {super();}\n8/∗∗Creates an empty priority queue using the given comparator to order keys. ∗/\n9publicUnsortedPriorityQueue(Comparator <K>comp){super(comp);}\n10\n11/∗∗Returns the Position of an entry having minimal key. ∗/\n12privatePosition<Entry<K,V>>ﬁndMin(){// only called when nonempty\n13Position<Entry<K,V>>small = list.ﬁrst();\n14for(Position<Entry<K,V>>walk : list.positions())\n15 if(compare(walk.getElement(), small.getElement()) <0)\n16 small = walk; // found an even smaller key\n17returnsmall;\n18}\n19\n20/∗∗Inserts a key-value pair and returns the entry created. ∗/\n21publicEntry<K,V>insert(K key, V value) throwsIllegalArgumentException {\n22checkKey(key); // auxiliary key-checking method (could throw exception)\n23Entry<K,V>newest = newPQEntry<>(key, value);\n24list.addLast(newest);\n25returnnewest;\n26}\n2728/∗∗Returns (but does not remove) an entry with minimal key. ∗/\n29publicEntry<K,V>min(){\n30if(list.isEmpty()) return null ;\n31returnﬁndMin().getElement();\n32}\n33\n34/∗∗Removes and returns an entry with minimal key. ∗/\n35publicEntry<K,V>removeMin(){\n36if(list.isEmpty()) return null ;\n37returnlist.remove(ﬁndMin());\n38}\n3940/∗∗Returns the number of items in the priority queue. ∗/\n41public int size(){returnlist.size();}\n42}\nCode Fragment 9.6:\nAn implementation of a priority queue using an unsorted list.\nThe parent class AbstractPriorityQueue is given in Code Fragment 9.5, and the\nLinkedPositionalList class is from Section 7.3.\nwww.it-ebooks.info"
  },
  {
    "page": 386,
    "content": "368 Chapter 9. Priority Queues\n9.2.5 Implementing a Priority Queue with a Sorted List\nOur next implementation of a priority queue also uses a positional list, yet maintains\nentries sorted by nondecreasing keys. This ensures that the ﬁrst element of the list\nis an entry with the smallest key.\nOurSortedPriorityQueue class is given in Code Fragment 9.7. The implemen-\ntation ofminandremoveMin are rather straightforward given knowledge that the\nﬁrst element of a list has a minimal key. We rely on the ﬁrst method of the posi-\ntional list to ﬁnd the position of the ﬁrst entry, and the remove method to remove\nthe entry from the list. Assuming that the list is implemented with a doubly linked\nlist, operations minandremoveMin take O(1)time.\nThis beneﬁt comes at a cost, however, for method insert now requires that we\nscan the list to ﬁnd the appropriate position to insert the new entry. Our implemen-\ntation starts at the end of the list, walking backward until the new key is smaller\nthan that of an existing entry; in the worst case, it progresses until reaching the\nfront of the list. Therefore, the insert method takes O(n)worst-case time, where n\nis the number of entries in the priority queue at the time the method is executed. In\nsummary, when using a sorted list to implement a priority queue, insertion runs in\nlinear time, whereas ﬁnding and removing the minimum can be done in constant\ntime.\nComparing the Two List-Based Implementations\nTable 9.2 compares the running times of the methods of a priority queue realized\nby means of a sorted and unsorted list, respectively. We see an interesting trade-\noff when we use a list to implement the priority queue ADT. An unsorted list\nsupports fast insertions but slow queries and deletions, whereas a sorted list allows\nfast queries and deletions, but slow insertions.\nMethod Unsorted List Sorted List\nsize O(1) O(1)\nisEmpty O(1) O(1)\ninsert O(1) O(n)\nmin O(n) O(1)\nremoveMin O(n) O(1)\nTable 9.2: Worst-case running times of the methods of a priority queue of size n,\nrealized by means of an unsorted or sorted list, respectively. We assume that the\nlist is implemented by a doubly linked list. The space requirement is O(n).\nwww.it-ebooks.info"
  },
  {
    "page": 387,
    "content": "9.2. Implementing a Priority Queue 369\n1/∗∗An implementation of a priority queue with a sorted list. ∗/\n2public class SortedPriorityQueue <K,V>extends AbstractPriorityQueue <K,V>{\n3/∗∗primary collection of priority queue entries ∗/\n4privatePositionalList <Entry<K,V>>list =newLinkedPositionalList <>();\n5\n6/∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/\n7publicSortedPriorityQueue() {super();}\n8/∗∗Creates an empty priority queue using the given comparator to order keys. ∗/\n9publicSortedPriorityQueue(Comparator <K>comp){super(comp);}\n1011/∗∗Inserts a key-value pair and returns the entry created. ∗/\n12publicEntry<K,V>insert(K key, V value) throwsIllegalArgumentException {\n13checkKey(key); // auxiliary key-checking method (could throw exception)\n14Entry<K,V>newest = newPQEntry<>(key, value);\n15Position<Entry<K,V>>walk = list.last();\n16// walk backward, looking for smaller key\n17while(walk != null&& compare(newest, walk.getElement()) <0)\n18 walk = list.before(walk);\n19if(walk == null)\n20 list.addFirst(newest); // new key is smallest\n21else\n22 list.addAfter(walk, newest); // newest goes after walk\n23returnnewest;\n24}\n25\n26/∗∗Returns (but does not remove) an entry with minimal key. ∗/\n27publicEntry<K,V>min(){\n28if(list.isEmpty()) return null ;\n29returnlist.ﬁrst().getElement();\n30}\n3132/∗∗Removes and returns an entry with minimal key. ∗/\n33publicEntry<K,V>removeMin(){\n34if(list.isEmpty()) return null ;\n35returnlist.remove(list.ﬁrst());\n36}\n37\n38/∗∗Returns the number of items in the priority queue. ∗/\n39public int size(){returnlist.size();}\n40}\nCode Fragment 9.7:\nAn implementation of a priority queue using a sorted list.\nThe parent class AbstractPriorityQueue is given in Code Fragment 9.5, and the\nLinkedPositionalList class is from Section 7.3.\nwww.it-ebooks.info"
  },
  {
    "page": 388,
    "content": "370 Chapter 9. Priority Queues\n9.3 Heaps\nThe two strategies for implementing a priority queue ADT in the previous section\ndemonstrate an interesting trade-off. When using an unsorted list to store entries,\nwe can perform insertions in O(1)time, but ﬁnding or removing an element with\nminimal key requires an O(n)-time loop through the entire collection. In contrast,\nif using a sorted list, we can trivially ﬁnd or remove the minimal element in O(1)\ntime, but adding a new element to the queue may require O(n)time to restore the\nsorted order.\nIn this section, we provide a more efﬁcient realization of a priority queue using\na data structure called a binary heap . This data structure allows us to perform both\ninsertions and removals in logarithmic time, which is a signiﬁcant improvement\nover the list-based implementations discussed in Section 9.2. The fundamental\nway the heap achieves this improvement is to use the structure of a binary tree to\nﬁnd a compromise between elements being entirely unsorted and perfectly sorted.\n9.3.1 The Heap Data Structure\nA heap (see Figure 9.1) is a binary tree Tthat stores entries at its positions, and\nthat satisﬁes two additional properties: a relational property deﬁned in terms of the\nway keys are stored in Tand a structural property deﬁned in terms of the shape of\nTitself. The relational property is the following:\nHeap-Order Property :In a heap T, for every position pother than the root, the\nkey stored at pis greater than or equal to the key stored at p’s parent.\nAs a consequence of the heap-order property, the keys encountered on a path from\nthe root to a leaf of Tare in nondecreasing order. Also, a minimal key is always\nstored at the root of T. This makes it easy to locate such an entry when min or\nremoveMin is called, as it is informally said to be “at the top of the heap” (hence,\nthe name “heap” for the data structure). By the way, the heap data structure deﬁned\nhere has nothing to do with the memory heap (Section 15.1.2) used in the runtime\nenvironment supporting a programming language like Java.\nFor the sake of efﬁciency, as will become clear later, we want the heap Tto have\nas small a height as possible. We enforce this requirement by insisting that the heap\nTsatisfy an additional structural property; it must be what we term complete .\nComplete Binary Tree Property: A heap Twith height his acomplete binary tree\nif levels 0 ,1,2,..., h−1 of Thave the maximal number of nodes possible\n(namely, level ihas 2inodes, for 0≤i≤h−1) and the remaining nodes at\nlevel hreside in the leftmost possible positions at that level.\nwww.it-ebooks.info"
  },
  {
    "page": 389,
    "content": "9.3. Heaps 371\n(14,E)(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H)(4,C)\nFigure 9.1: Example of a heap storing 13 entries with integer keys. The last position\nis the one storing entry (13,W).\nThe tree in Figure 9.1 is complete because levels 0, 1, and 2 are full, and the six\nnodes in level 3 are in the six leftmost possible positions at that level. In formalizing\nwhat we mean by the leftmost possible positions, we refer to the discussion of level\nnumbering from Section 8.3.2, in the context of an array-based representation of a\nbinary tree. (In fact, in Section 9.3.2 we will discuss the use of an array to represent\na heap.) A complete binary tree with nelements is one that has positions with level\nnumbering 0 through n−1. For example, in an array-based representation of the\nabove tree, its 13 entries would be stored consecutively from A[0]toA[12].\nThe Height of a Heap\nLethdenote the height of T. Insisting that Tbe complete also has an important\nconsequence, as shown in Proposition 9.2.\nProposition 9.2: A heap Tstoring nentries has height h=⌊logn⌋.\nJustiﬁcation: From the fact that Tis complete, we know that the number of\nnodes in levels 0 through h−1 ofTis precisely 1 +2+4+···+2h−1=2h−1, and\nthat the number of nodes in level his at least 1 and at most 2h. Therefore\nn≥2h−1+1=2hand n≤2h−1+2h=2h+1−1.\nBy taking the logarithm of both sides of inequality n≥2h, we see that height\nh≤logn. By rearranging terms and taking the logarithm of both sides of inequality\nn≤2h+1−1, we see that h≥log(n+1)−1. Since his an integer, these two\ninequalities imply that h=⌊logn⌋.\nwww.it-ebooks.info"
  },
  {
    "page": 390,
    "content": "372 Chapter 9. Priority Queues\n9.3.2 Implementing a Priority Queue with a Heap\nProposition 9.2 has an important consequence, for it implies that if we can perform\nupdate operations on a heap in time proportional to its height, then those opera-\ntions will run in logarithmic time. Let us therefore turn to the problem of how to\nefﬁciently perform various priority queue methods using a heap.\nWe will use the composition pattern from Section 9.2.1 to store key-value pairs\nas entries in the heap. The sizeandisEmpty methods can be implemented based\non examination of the tree, and the min operation is equally trivial because the\nheap property assures that the element at the root of the tree has a minimal key.\nThe interesting algorithms are those for implementing the insert andremoveMin\nmethods.\nAdding an Entry to the Heap\nLet us consider how to perform insert( k,v)on a priority queue implemented with\na heap T. We store the pair (k,v)as an entry at a new node of the tree. To maintain\nthecomplete binary tree property , that new node should be placed at a position p\njust beyond the rightmost node at the bottom level of the tree, or as the leftmost\nposition of a new level, if the bottom level is already full (or if the heap is empty).\nUp-Heap Bubbling After an Insertion\nAfter this action, the tree Tis complete, but it may violate the heap-order property .\nHence, unless position pis the root of T(that is, the priority queue was empty\nbefore the insertion), we compare the key at position pto that of p’s parent, which\nwe denote as q. If key kp≥kq, the heap-order property is satisﬁed and the algorithm\nterminates. If instead kp<kq, then we need to restore the heap-order property,\nwhich can be locally achieved by swapping the entries stored at positions pandq.\n(See Figure 9.2c and d.) This swap causes the new entry to move up one level.\nAgain, the heap-order property may be violated, so we repeat the process, going up\ninTuntil no violation of the heap-order property occurs. (See Figure 9.2e and h.)\nThe upward movement of the newly inserted entry by means of swaps is con-\nventionally called up-heap bubbling . A swap either resolves the violation of the\nheap-order property or propagates it one level up in the heap. In the worst case, up-\nheap bubbling causes the new entry to move all the way up to the root of heap T.\nThus, in the worst case, the number of swaps performed in the execution of method\ninsert is equal to the height of T. By Proposition 9.2, that bound is ⌊logn⌋.\nwww.it-ebooks.info"
  },
  {
    "page": 391,
    "content": "9.3. Heaps 373\n(14,E)(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H)(4,C)\n(2,T)(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E)(4,C)\n(a) (b)\n(20,B)(5,A) (6,Z)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E)(2,T)(4,C)\n(2,T)(5,A) (6,Z)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(4,C)\n(c) (d)\n(2,T)(5,A)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(6,Z)(4,C)\n(6,Z)(5,A)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(2,T)(4,C)\n(e) (f)\n(4,C)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(6,Z)(2,T)\n(5,A)\n(6,Z) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(2,T)\n(4,C) (5,A)\n(g) (h)\nFigure 9.2: Insertion of a new entry with key 2 into the heap of Figure 9.1: (a) initial\nheap; (b) after adding a new node; (c and d) swap to locally restore the partial order\nproperty; (e and f) another swap; (g and h) ﬁnal swap.\nwww.it-ebooks.info"
  },
  {
    "page": 392,
    "content": "374 Chapter 9. Priority Queues\nRemoving the Entry with Minimal Key\nLet us now turn to method removeMin of the priority queue ADT. We know that an\nentry with the smallest key is stored at the root rofT(even if there is more than\none entry with smallest key). However, in general we cannot simply delete node r,\nbecause this would leave two disconnected subtrees.\nInstead, we ensure that the shape of the heap respects the complete binary tree\nproperty by deleting the leaf at the lastposition pofT, deﬁned as the rightmost\nposition at the bottommost level of the tree. To preserve the entry from the last\nposition p, we copy it to the root r(in place of the entry with minimal key that is\nbeing removed by the operation). Figure 9.3a and b illustrates an example of thesesteps, with minimal entry (4,C)being removed from the root and replaced by entry\n(13,W)from the last position. The node at the last position is removed from the\ntree.\nDown-Heap Bubbling After a Removal\nWe are not yet done, however, for even though Tis now complete, it likely violates\nthe heap-order property. If Thas only one node (the root), then the heap-order\nproperty is trivially satisﬁed and the algorithm terminates. Otherwise, we distin-guish two cases, where pinitially denotes the root of T:\n•Ifphas no right child, let cbe the left child of p.\n•Otherwise ( phas both children), let cbe a child of pwith minimal key.\nIf key k\np≤kc, the heap-order property is satisﬁed and the algorithm terminates. If\ninstead kp>kc, then we need to restore the heap-order property. This can be locally\nachieved by swapping the entries stored at pandc. (See Figure 9.3c and d.) It is\nworth noting that when phas two children, we intentionally consider the smaller\nkey of the two children. Not only is the key of csmaller than that of p, it is at\nleast as small as the key at c’s sibling. This ensures that the heap-order property is\nlocally restored when that smaller key is promoted above the key that had been at\npand that at c’s sibling.\nHaving restored the heap-order property for node prelative to its children, there\nmay be a violation of this property at c; hence, we may have to continue swapping\ndown Tuntil no violation of the heap-order property occurs. (See Figure 9.3e–h.)\nThis downward swapping process is called down-heap bubbling . A swap either\nresolves the violation of the heap-order property or propagates it one level down inthe heap. In the worst case, an entry moves all the way down to the bottom level.(See Figure 9.3.) Thus, the number of swaps performed in the execution of methodremoveMin is, in the worst case, equal to the height of heap T, that is, it is⌊logn⌋\nby Proposition 9.2.\nwww.it-ebooks.info"
  },
  {
    "page": 393,
    "content": "9.3. Heaps 375\n(13,W)\n(6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (12,H) (14,E)(4,C)\n(5,A)(13,W)\n(14,E) (12,H) (25,J) (16,X) (11,S)(15,K) (9,F) (7,Q) (20,B)(6,Z) (5,A)\n(a) (b)\n(13,W)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (12,H) (14,E)(5,A) (6,Z) (13,W)\n(14,E) (12,H) (25,J) (16,X) (11,S)(15,K) (9,F) (7,Q) (20,B)(6,Z)(5,A)\n(c) (d)\n(9,F)\n(20,B) (7,Q) (15,K)\n(11,S) (16,X) (25,J) (12,H) (14,E)(5,A)\n(13,W)(6,Z)\n(13,W)\n(14,E) (12,H) (25,J) (16,X) (11,S)(15,K) (7,Q) (20,B)(6,Z)(5,A)\n(9,F)\n(e) (f)\n(13,W)(20,B) (7,Q) (15,K)(5,A)\n(9,F)\n(11,S) (14,E) (25,J) (16,X)(12,H)(6,Z)\n(13,W)(20,B) (7,Q) (15,K)(5,A)\n(9,F)\n(12,H)\n(11,S) (14,E) (25,J) (16,X)(6,Z)\n(g) (h)\nFigure 9.3: Removal of the entry with the smallest key from a heap: (a and b)\ndeletion of the last node, whose entry gets stored into the root; (c and d) swap to\nlocally restore the heap-order property; (e and f) another swap; (g and h) ﬁnal swap.\nwww.it-ebooks.info"
  },
  {
    "page": 394,
    "content": "376 Chapter 9. Priority Queues\nArray-Based Representation of a Complete Binary Tree\nThe array-based representation of a binary tree (Section 8.3.2) is especially suitable\nfor a complete binary tree. We recall that in this implementation, the elements of\nthe tree are stored in an array-based list Asuch that the element at position pis\nstored in Awith index equal to the level number f(p)ofp, deﬁned as follows:\n•Ifpis the root, then f(p)=0.\n•Ifpis the left child of position q, then f(p)=2f(q)+1.\n•Ifpis the right child of position q, then f(p)=2f(q)+2.\nFor a tree with of size n, the elements have contiguous indices in the range [0,n−1]\nand the last position of is always at index n−1. (See Figure 9.4.)\n(12,H)(4,C)\n(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (14,E)70\n1 2\n3 4 5 6\n8 9 10 11 12\n0 1 2 3 4 5 12 11 10 9 8 7 6(5,A) (13,W)(11,S)(12,H)(14,E)(25,J)(16,X)(20,B)(7,Q)(9,F) (15,K)(6,Z) (4,C)\nFigure 9.4: Array-based representation of a heap.\nThe array-based heap representation avoids some complexities of a linked tree\nstructure. Speciﬁcally, methods insert andremoveMin depend on locating the last\nposition of a heap. With the array-based representation of a heap of size n, the last\nposition is simply at index n−1. Locating the last position in a heap implemented\nwith a linked tree structure requires more effort. (See Exercise C-9.33.)\nIf the size of a priority queue is not known in advance, use of an array-based\nrepresentation does introduce the need to dynamically resize the array on occasion,\nas is done with a Java ArrayList . The space usage of such an array-based repre-\nsentation of a complete binary tree with nnodes is O(n), and the time bounds of\nmethods for adding or removing elements become amortized . (See Section 7.2.2.)\nJava Heap Implementation\nIn Code Fragments 9.8 and 9.9, we provide a Java implementation of a heap-based\npriority queue. Although we think of our heap as a binary tree, we do not formally\nwww.it-ebooks.info"
  },
  {
    "page": 395,
    "content": "9.3. Heaps 377\nuse the binary tree ADT. We prefer to use the more efﬁcient array-based represen-\ntation of a tree, maintaining a Java ArrayList of entry composites. To allow us to\nformalize our algorithms using tree-like terminology of parent ,left, and right , the\nclass includes protected utility methods that compute the level numbering of a par-\nent or child of another position (lines 10–14 of Code Fragment 9.8). However, the“positions” in this representation are simply integer indices into the array-list.\nOur class also has protected utilities swap ,upheap , anddownheap for the low-\nlevel movement of entries within the array-list. A new entry is added the end ofthe array-list, and then repositioned as needed with upheap . To remove the entry\nwith minimal key (which resides at index 0), we move the last entry of the array-list\nfrom index n−1 to index 0, and then invoke downheap to reposition it.\n1/∗∗An implementation of a priority queue using an array-based heap. ∗/\n2public class HeapPriorityQueue <K,V>extends AbstractPriorityQueue <K,V>{\n3/∗∗primary collection of priority queue entries ∗/\n4protected ArrayList <Entry<K,V>>heap =newArrayList <>();\n5/∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/\n6publicHeapPriorityQueue() {super();}\n7/∗∗Creates an empty priority queue using the given comparator to order keys. ∗/\n8publicHeapPriorityQueue(Comparator <K>comp){super(comp);}\n9// protected utilities\n10protected int parent(intj){return(j−1) / 2;} // truncating division\n11protected int left(intj){return2∗j + 1;}\n12protected int right(intj){return2∗j + 2;}\n13protected boolean hasLeft(intj){returnleft(j)<heap.size();}\n14protected boolean hasRight( intj){returnright(j)<heap.size();}\n15/∗∗Exchanges the entries at indices i and j of the array list. ∗/\n16protected void swap(inti,intj){\n17Entry<K,V>temp = heap.get(i);\n18heap.set(i, heap.get(j));\n19heap.set(j, temp);\n20}\n21/∗∗Moves the entry at index j higher, if necessary, to restore the heap property. ∗/\n22protected void upheap(intj){\n23while(j>0){ // continue until reaching root (or break statement)\n24 intp = parent(j);\n25 if(compare(heap.get(j), heap.get(p)) >= 0)break; // heap property veriﬁed\n26 swap(j, p);\n27 j = p; // continue from the parent 's location\n28}\n29}\nCode Fragment 9.8: Priority queue that uses an array-based heap and extends\nAbstractPriorityQueue (Code Fragment 9.5). (Continues in Code Fragment 9.9.)\nwww.it-ebooks.info"
  },
  {
    "page": 396,
    "content": "378 Chapter 9. Priority Queues\n30/∗∗Moves the entry at index j lower, if necessary, to restore the heap property. ∗/\n31protected void downheap( intj){\n32while(hasLeft(j)){ // continue to bottom (or break statement)\n33 intleftIndex = left(j);\n34 intsmallChildIndex = leftIndex; // although right may be smaller\n35 if(hasRight(j)){\n36 intrightIndex = right(j);\n37 if(compare(heap.get(leftIndex), heap.get(rightIndex)) >0)\n38 smallChildIndex = rightIndex; // right child is smaller\n39}\n40 if(compare(heap.get(smallChildIndex), heap.get(j)) >= 0)\n41 break; // heap property has been restored\n42 swap(j, smallChildIndex);\n43 j = smallChildIndex; // continue at position of the child\n44}\n45}\n46\n47// public methods\n48/∗∗Returns the number of items in the priority queue. ∗/\n49public int size(){returnheap.size();}\n50/∗∗Returns (but does not remove) an entry with minimal key (if any). ∗/\n51publicEntry<K,V>min(){\n52if(heap.isEmpty()) return null ;\n53returnheap.get(0);\n54}\n55/∗∗Inserts a key-value pair and returns the entry created. ∗/\n56publicEntry<K,V>insert(K key, V value) throwsIllegalArgumentException {\n57checkKey(key); // auxiliary key-checking method (could throw exception)\n58Entry<K,V>newest = newPQEntry<>(key, value);\n59heap.add(newest); // add to the end of the list\n60upheap(heap.size() −1); // upheap newly added entry\n61returnnewest;\n62}\n63/∗∗Removes and returns an entry with minimal key (if any). ∗/\n64publicEntry<K,V>removeMin(){\n65if(heap.isEmpty()) return null ;\n66Entry<K,V>answer = heap.get(0);\n67swap(0, heap.size() −1); // put minimum item at the end\n68heap.remove(heap.size() −1); // and remove it from the list;\n69downheap(0); // then ﬁx new root\n70returnanswer;\n71}\n72}\nCode Fragment 9.9: Priority queue implemented with an array-based heap (contin-\nued from Code Fragment 9.8).\nwww.it-ebooks.info"
  },
  {
    "page": 397,
    "content": "9.3. Heaps 379\n9.3.3 Analysis of a Heap-Based Priority Queue\nTable 9.3 shows the running time of the priority queue ADT methods for the heap\nimplementation of a priority queue, assuming that two keys can be compared in\nO(1)time and that the heap Tis implemented with an array-based or linked-based\ntree representation.\nIn short, each of the priority queue ADT methods can be performed in O(1)or\ninO(logn)time, where nis the number of entries at the time the method is exe-\ncuted. The analysis of the running time of the methods is based on the following:\n•The heap Thasnnodes, each storing a reference to a key-value entry.\n•The height of heap TisO(logn), since Tis complete (Proposition 9.2).\n•Theminoperation runs in O(1)because the root of the tree contains such an\nelement.\n•Locating the last position of a heap, as required for insert andremoveMin ,\ncan be performed in O(1)time for an array-based representation, or O(logn)\ntime for a linked-tree representation. (See Exercise C-9.33.)\n•In the worst case, up-heap and down-heap bubbling perform a number of\nswaps equal to the height of T.\nMethod Running Time\nsize,isEmpty O(1)\nmin O(1)\ninsert O(logn)∗\nremoveMin O(logn)∗\n∗amortized, if using dynamic array\nTable 9.3: Performance of a priority queue realized by means of a heap. We let n\ndenote the number of entries in the priority queue at the time an operation is ex-\necuted. The space requirement is O(n). The running time of operations minand\nremoveMin are amortized for an array-based representation, due to occasional re-\nsizing of a dynamic array; those bounds are worst case with a linked tree structure.\nWe conclude that the heap data structure is a very efﬁcient realization of the\npriority queue ADT, independent of whether the heap is implemented with a linked\nstructure or an array. The heap-based implementation achieves fast running times\nfor both insertion and removal, unlike the implementations that were based on using\nan unsorted or sorted list.\nwww.it-ebooks.info"
  },
  {
    "page": 398,
    "content": "380 Chapter 9. Priority Queues\n9.3.4 Bottom-Up Heap Construction ⋆\nIf we start with an initially empty heap, nsuccessive calls to the insert operation\nwill run in O(nlogn)time in the worst case. However, if all nkey-value pairs to\nbe stored in the heap are given in advance, such as during the ﬁrst phase of the\nheap-sort algorithm (introduced in Section 9.4.2), there is an alternative bottom-up\nconstruction method that runs in O(n)time.\nIn this section, we describe the bottom-up heap construction, and provide an\nimplementation that can be used by the constructor of a heap-based priority queue.\nFor simplicity of exposition, we describe this bottom-up heap construction as-\nsuming the number of keys, n, is an integer such that n=2h+1−1. That is,\nthe heap is a complete binary tree with every level being full, so the heap has\nheight h=log(n+1)−1. Viewed nonrecursively, bottom-up heap construction\nconsists of the following h+1=log(n+1)steps:\n1.In the ﬁrst step (see Figure 9.5b), we construct (n+1)/2 elementary heaps\nstoring one entry each.\n2.In the second step (see Figure 9.5c–d), we form (n+1)/4 heaps, each storing\nthree entries, by joining pairs of elementary heaps and adding a new entry.\nThe new entry is placed at the root and may have to be swapped with the\nentry stored at a child to preserve the heap-order property.\n3.In the third step (see Figure 9.5e–f), we form (n+1)/8 heaps, each storing\n7 entries, by joining pairs of 3-entry heaps (constructed in the previous step)\nand adding a new entry. The new entry is placed initially at the root, but may\nhave to move down with a down-heap bubbling to preserve the heap-order\nproperty.\n...\ni.In the generic ithstep, 2≤i≤h, we form (n+1)/2iheaps, each storing 2i−1\nentries, by joining pairs of heaps storing (2i−1−1)entries (constructed in the\nprevious step) and adding a new entry. The new entry is placed initially at\nthe root, but may have to move down with a down-heap bubbling to preserve\nthe heap-order property.\n...\nh+1.In the last step (see Figure 9.5g–h), we form the ﬁnal heap, storing all the\nnentries, by joining two heaps storing (n−1)/2 entries (constructed in the\nprevious step) and adding a new entry. The new entry is placed initially at\nthe root, but may have to move down with a down-heap bubbling to preserve\nthe heap-order property.\nWe illustrate bottom-up heap construction in Figure 9.5 for h=3.\nwww.it-ebooks.info"
  },
  {
    "page": 399,
    "content": "9.3. Heaps 381\n4 15 12 6 7 23 20 16\n(a) (b)\n4 16 159\n12 6 711\n2317\n2025\n20 16 25 94\n12 11 76\n2317 15\n(c) (d)\n25 12 11 23 2017 15\n168\n4\n95\n6\n7 25 12 11 23 2017 15\n16 85\n94 6\n7\n(e) (f)\n25 12 11 8 23 2017 7 156\n16514\n4\n9 25 12 11 8 23 2017 7 156\n16 144\n5\n9\n(g) (h)\nFigure 9.5: Bottom-up construction of a heap with 15 entries: (a and b) we begin by\nconstructing 1-entry heaps on the bottom level; (c and d) we combine these heaps\ninto 3-entry heaps; (e and f) we build 7-entry heaps; (g and h) we create the ﬁnalheap. The paths of the down-heap bubblings are highlighted in (d, f, and h). For\nsimplicity, we only show the key within each node instead of the entire entry.\nwww.it-ebooks.info"
  },
  {
    "page": 400,
    "content": "382 Chapter 9. Priority Queues\nJava Implementation of a Bottom-Up Heap Construction\nImplementing a bottom-up heap construction is quite easy, given the existence of\na “down-heap” utility method. The “merging” of two equally sized heaps that are\nsubtrees of a common position p, as described in the opening of this section, can\nbe accomplished simply by down-heaping p’s entry. For example, that is what\nhappened to the key 14 in going from Figure 9.5(f) to (g).\nWith our array-based representation of a heap, if we initially store all nentries\nin arbitrary order within the array, we can implement the bottom-up heap construc-\ntion process with a single loop that makes a call to downheap from each position\nof the tree, as long as those calls are ordered starting with the deepest level and\nending with the root of the tree. In fact, that loop can start with the deepest internal\nposition, since there is no effect when down-heap is called at an external position.\nIn Code Fragment 9.10, we augment the original HeapPriorityQueue class from\nSection 9.3.2 to provide support for the bottom-up construction of an initial collec-tion. We introduce a nonpublic utility method, heapify , that calls downheap on\neach nonleaf position, beginning with the deepest and concluding with a call at the\nroot of the tree.\nWe introduce an additional constructor for the class that accepts an initial se-\nquence of keys and values, parameterized as two coordinate arrays that are pre-sumed to have the same length. We create new entries, pairing the ﬁrst key withthe ﬁrst value, the second key with the second value, and so on. We then call the\nheapify utility to establish the heap ordering. For brevity, we omit a similar con-\nstructor that accepts a nondefault comparator for the priority queue.\n/∗∗Creates a priority queue initialized with the given key-value pairs. ∗/\npublicHeapPriorityQueue(K[ ] keys, V[ ] values) {\nsuper();\nfor(intj=0; j<Math.min(keys.length, values.length); j++)\nheap.add( newPQEntry<>(keys[j], values[j]));\nheapify();\n}\n/∗∗Performs a bottom-up construction of the heap in linear time. ∗/\nprotected void heapify(){\nintstartIndex = parent(size() −1); // start at PARENT of last entry\nfor(intj=startIndex; j >= 0; j−−) // loop until processing the root\ndownheap(j);\n}\nCode Fragment 9.10: Revision to the HeapPriorityQueue class of Code Frag-\nments 9.8 and 9.9, supporting linear-time construction given an initial collection\nof key-value pairs.\nwww.it-ebooks.info"
  },
  {
    "page": 410,
    "content": "392 Chapter 9. Priority Queues\n9.5.2 Implementing an Adaptable Priority Queue\nCode Fragments 9.12 and 9.13 present a Java implementation of an adaptable pri-\nority queue, as a subclass of the HeapPriorityQueue class from Section 9.3.2. We\nbegin by deﬁning a nested AdaptablePQEntry class (lines 5–15) that extends the\ninherited PQEntry class, augmenting it with an additional index ﬁeld. The inher-\nitedinsert method is overridden, so that we create and initialize an instance of the\nAdaptablePQEntry class (not the original PQEntry class).\nAn important aspect of our design is that the original HeapPriorityQueue class\nrelies exclusively on a protected swap method for all low-level data movement dur-\ning up-heap or down-heap operations. The AdaptablePriorityQueue class overrides\nthat utility in order to update the stored indices of our location-aware entries when\nthey are relocated (as discussed on the previous page).\nWhen an entry is sent as a parameter to remove ,replaceKey , orreplaceValue ,\nwe rely on the new index ﬁeld of that entry to designate where the element resides\nin the heap (a fact that is easily validated). When a key of an existing entry is\nreplaced, that new key may violate the heap-order property by being either too\nbig or too small. We provide a new bubble utility that determines whether an up-\nheap or down-heap bubbling step is warranted. When removing an arbitrary entry,\nwe replace it with the last entry in the heap (to maintain the complete binary tree\nproperty) and perform the bubbling step, since the displaced element may have a\nkey that is too large or too small for its new location.\nPerformance of Adaptable Priority Queue Implementations\nThe performance of an adaptable priority queue by means of our location-aware\nheap structure is summarized in Table 9.5. The new class provides the same asymp-\ntotic efﬁciency and space usage as the nonadaptive version, and provides logarith-\nmic performance for the new locator-based remove andreplaceKey methods, and\nconstant-time performance for the new replaceValue method.\nMethod Running Time\nsize,isEmpty ,min O(1)\ninsert O(logn)\nremove O(logn)\nremoveMin O(logn)\nreplaceKey O(logn)\nreplaceValue O(1)\nTable 9.5: Running times of the methods of an adaptable priority queue with size n,\nrealized by means of our array-based heap representation. The space requirement\nisO(n).\nwww.it-ebooks.info"
  },
  {
    "page": 411,
    "content": "9.5. Adaptable Priority Queues 393\n1/∗∗An implementation of an adaptable priority queue using an array-based heap. ∗/\n2public class HeapAdaptablePriorityQueue <K,V>extends HeapPriorityQueue <K,V>\n3 implements AdaptablePriorityQueue <K,V>{\n4\n5//---------------- nested AdaptablePQEntry class ----------------\n6/∗∗Extension of the PQEntry to include location information. ∗/\n7protected static class AdaptablePQEntry <K,V>extends PQEntry<K,V>{\n8private int index; // entry’s current index within the heap\n9publicAdaptablePQEntry(K key, V value, intj){\n10 super(key, value); // this sets the key and value\n11 index = j; // this sets the new ﬁeld\n12}\n13public int getIndex(){returnindex;}\n14public void setIndex( intj){index = j;}\n15}//----------- end of nested AdaptablePQEntry class -----------\n16\n17/∗∗Creates an empty adaptable priority queue using natural ordering of keys. ∗/\n18publicHeapAdaptablePriorityQueue() {super();}\n19/∗∗Creates an empty adaptable priority queue using the given comparator. ∗/\n20publicHeapAdaptablePriorityQueue(Comparator <K>comp){super(comp);}\n2122// protected utilites\n23/∗∗Validates an entry to ensure it is location-aware. ∗/\n24protected AdaptablePQEntry <K,V>validate(Entry <K,V>entry)\n25 throwsIllegalArgumentException {\n26if(!(entryinstanceof AdaptablePQEntry))\n27 throw new IllegalArgumentException( \"Invalid entry\" );\n28AdaptablePQEntry <K,V>locator = (AdaptablePQEntry <K,V>) entry; // safe\n29intj = locator.getIndex();\n30if(j>= heap.size()||heap.get(j) != locator)\n31 throw new IllegalArgumentException( \"Invalid entry\" );\n32returnlocator;\n33}\n3435/∗∗Exchanges the entries at indices i and j of the array list. ∗/\n36protected void swap(inti,intj){\n37super.swap(i,j); // perform the swap\n38((AdaptablePQEntry <K,V>) heap.get(i)).setIndex(i); // reset entry 's index\n39((AdaptablePQEntry <K,V>) heap.get(j)).setIndex(j); // reset entry 's index\n40}\nCode Fragment 9.12:\nAn implementation of an adaptable priority queue. (Contin-\nues in Code Fragment 9.13.) This extends the HeapPriorityQueue class of Code\nFragments 9.8 and 9.9.\nwww.it-ebooks.info"
  },
  {
    "page": 412,
    "content": "394 Chapter 9. Priority Queues\n41/∗∗Restores the heap property by moving the entry at index j upward/downward.∗ /\n42protected void bubble(intj){\n43if(j>0 && compare(heap.get(j), heap.get(parent(j))) <0)\n44 upheap(j);\n45else\n46 downheap(j); // although it might not need to move\n47}\n48\n49/∗∗Inserts a key-value pair and returns the entry created. ∗/\n50publicEntry<K,V>insert(K key, V value) throwsIllegalArgumentException {\n51checkKey(key); // might throw an exception\n52Entry<K,V>newest = newAdaptablePQEntry <>(key, value, heap.size());\n53heap.add(newest); // add to the end of the list\n54upheap(heap.size() −1); // upheap newly added entry\n55returnnewest;\n56}\n5758/∗∗Removes the given entry from the priority queue. ∗/\n59public void remove(Entry <K,V>entry)throwsIllegalArgumentException {\n60AdaptablePQEntry <K,V>locator = validate(entry);\n61intj = locator.getIndex();\n62if(j == heap.size() −1) // entry is at last position\n63 heap.remove(heap.size() −1); // so just remove it\n64else{\n65 swap(j, heap.size() −1); // swap entry to last position\n66 heap.remove(heap.size() −1); // then remove it\n67 bubble(j); // and ﬁx entry displaced by the swap\n68}\n69}\n7071/∗∗Replaces the key of an entry. ∗/\n72public void replaceKey(Entry <K,V>entry, K key)\n73 throwsIllegalArgumentException {\n74AdaptablePQEntry <K,V>locator = validate(entry);\n75checkKey(key); // might throw an exception\n76locator.setKey(key); // method inherited from PQEntry\n77bubble(locator.getIndex()); // with new key, may need to move entry\n78}\n79\n80/∗∗Replaces the value of an entry. ∗/\n81public void replaceValue(Entry <K,V>entry, V value)\n82 throwsIllegalArgumentException {\n83AdaptablePQEntry <K,V>locator = validate(entry);\n84locator.setValue(value); // method inherited from PQEntry\n85}\nCode Fragment 9.13:\nAn implementation of an adaptable priority queue (continued\nfrom Code Fragment 9.12).\nwww.it-ebooks.info"
  },
  {
    "page": 413,
    "content": "9.6. Exercises 395\n9.6 Exercises\nReinforcement\nR-9.1 How long would it take to remove the ⌈logn⌉smallest elements from a heap that\ncontains nentries, using the removeMin operation?\nR-9.2 Suppose you set the key for each position pof a binary tree Tequal to its preorder\nrank. Under what circumstances is Ta heap?\nR-9.3 What does each removeMin call return within the following sequence of priority\nqueue ADT operations: insert( 5,A),insert( 4,B),insert( 7,F),insert( 1,D),\nremoveMin() ,insert( 3,J),insert( 6,L),removeMin() ,\nremoveMin() ,insert( 8,G),removeMin() ,insert( 2,H),removeMin() ,\nremoveMin() ?\nR-9.4 An airport is developing a computer simulation of air-trafﬁc control that handles\nevents such as landings and takeoffs. Each event has a time stamp that denotes\nthe time when the event will occur. The simulation program needs to efﬁciently\nperform the following two fundamental operations:\n•Insert an event with a given time stamp (that is, add a future event).\n•Extract the event with smallest time stamp (that is, determine the next event\nto process).\nWhich data structure should be used for the above operations? Why?\nR-9.5 Theminmethod for the UnsortedPriorityQueue class executes in O(n)time, as\nanalyzed in Table 9.2. Give a simple modiﬁcation to the class so that minruns in\nO(1)time. Explain any necessary modiﬁcations to other methods of the class.\nR-9.6 Can you adapt your solution to the previous problem to make removeMin run in\nO(1)time for the UnsortedPriorityQueue class? Explain your answer.\nR-9.7 Illustrate the execution of the selection-sort algorithm on the following input se-\nquence:(22, 15, 36, 44, 10, 3, 9, 13, 29, 25) .\nR-9.8 Illustrate the execution of the insertion-sort algorithm on the input sequence of\nthe previous problem.\nR-9.9 Give an example of a worst-case sequence with nelements for insertion-sort, and\nshow that insertion-sort runs in Ω(n2)time on such a sequence.\nR-9.10 At which positions of a heap might the third smallest key be stored?\nR-9.11 At which positions of a heap might the largest key be stored?\nR-9.12 Consider a situation in which a user has numeric keys and wishes to have a pri-\nority queue that is maximum-oriented . How could a standard (min-oriented) pri-\nority queue be used for such a purpose?\nR-9.13 Illustrate the execution of the in-place heap-sort algorithm on the following input\nsequence: (2, 5, 16, 4, 10, 23, 39, 18, 26, 15) .\nwww.it-ebooks.info"
  },
  {
    "page": 414,
    "content": "396 Chapter 9. Priority Queues\nR-9.14 LetTbe a complete binary tree such that position pstores an element with key\nf(p), where f(p)is the level number of p(see Section 8.3.2). Is tree Ta heap?\nWhy or why not?\nR-9.15 Explain why the description of down-heap bubbling does not consider the case\nin which position phas a right child but not a left child.\nR-9.16 Is there a heap Hstoring seven entries with distinct keys such that a preorder\ntraversal of Hyields the entries of Hin increasing or decreasing order by key?\nHow about an inorder traversal? How about a postorder traversal? If so, give an\nexample; if not, say why.\nR-9.17 LetHbe a heap storing 15 entries using the array-based representation of a com-\nplete binary tree. What is the sequence of indices of the array that are visited in\na preorder traversal of H? What about an inorder traversal of H? What about a\npostorder traversal of H?\nR-9.18 Show that the sumn\n∑\ni=1logi, appearing in the analysis of heap-sort, is Ω(nlogn).\nR-9.19 Bill claims that a preorder traversal of a heap will list its keys in nondecreasing\norder. Draw an example of a heap that proves him wrong.\nR-9.20 Hillary claims that a postorder traversal of a heap will list its keys in nonincreas-\ning order. Draw an example of a heap that proves her wrong.\nR-9.21 Illustrate all the steps of the adaptable priority queue call remove( e)for entry e\nstoring(16,X) in the heap of Figure 9.1.\nR-9.22 Illustrate all the steps of the adaptable priority queue call replaceKey( e,18)for\nentry estoring(5, A) in the heap of Figure 9.1.\nR-9.23 Draw an example of a heap whose keys are all the odd numbers from 1 to 59\n(with no repeats), such that the insertion of an entry with key 32 would cause\nup-heap bubbling to proceed all the way up to a child of the root (replacing that\nchild’s key with 32).\nR-9.24 Describe a sequence of ninsertions in a heap that requires Ω(nlogn)time to\nprocess.\nCreativity\nC-9.25 Show how to implement the stack ADT using only a priority queue and one\nadditional integer instance variable.\nC-9.26 Show how to implement the FIFO queue ADT using only a priority queue and\none additional integer instance variable.\nC-9.27 Professor Idle suggests the following solution to the previous problem. Whenever\nan entry is inserted into the queue, it is assigned a key that is equal to the current\nsize of the queue. Does such a strategy result in FIFO semantics? Prove that it is\nso or provide a counterexample.\nwww.it-ebooks.info"
  },
  {
    "page": 415,
    "content": "9.6. Exercises 397\nC-9.28 Reimplement the SortedPriorityQueue using a Java array. Make sure to maintain\nremoveMin ’sO(1)performance.\nC-9.29 Give an alternative implementation of the HeapPriorityQueue ’supheap method\nthat uses recursion (and no loop).\nC-9.30 Give an implementation of the HeapPriorityQueue ’sdownheap method that uses\nrecursion (and no loop).\nC-9.31 Assume that we are using a linked representation of a complete binary tree T, and\nan extra reference to the last node of that tree. Show how to update the reference\nto the last node after operations insert orremove inO(logn)time, where nis the\ncurrent number of nodes of T. Be sure to handle all possible cases, as illustrated\nin Figure 9.12.\nC-9.32 When using a linked-tree representation for a heap, an alternative method for\nﬁnding the last node during an insertion in a heap Tis to store, in the last node\nand each leaf node of T, a reference to the leaf node immediately to its right\n(wrapping to the ﬁrst node in the next lower level for the rightmost leaf node).Show how to maintain such references in O(1)time per operation of the priority\nqueue ADT assuming that Tis implemented with a linked structure.\nC-9.33 We can represent a path from the root to a given node of a binary tree by means ofa binary string, where 0 means “go to the left child” and 1 means “go to the rightchild.” For example, the path from the root to the node storing (8,W)in the heap\nof Figure 9.12a is represented by “101.” Design an O(logn)-time algorithm for\nﬁnding the last node of a complete binary tree with nnodes, based on the above\nrepresentation. Show how this algorithm can be used in the implementation of a\ncomplete binary tree by means of a linked structure that does not keep an explicit\nreference to the last node instance.\nC-9.34 Given a heap Hand a key k, give an algorithm to compute all the entries in H\nhaving a key less than or equal to k. For example, given the heap of Figure 9.12a\nand query k=7, the algorithm should report the entries with keys 2, 4, 5, 6, and 7\n(but not necessarily in this order). Your algorithm should run in time proportionalto the number of entries returned, and should notmodify the heap.\n(11,S)(2,B)\n(5,A) (4,C)\n(6,Z) (9,F) (15,K)\n(25,J) (12,H) (14,E) (16,X)(7,Q)\n(8,W) (10,L) (20,B)\nzw(5,A) (6,Z)\n(20,B) (9,F) (15,K)\n(25,J) (14,E) (16,X)(7,Q)\n(12,H)(4,C)\nw z\n(a) (b)\nFigure 9.12: Two cases of updating the last node in a complete binary tree after\noperation insert orremove . Node wis the last node before operation insert or after\noperation remove . Node zis the last node after operation insert or before operation\nremove .\nwww.it-ebooks.info"
  },
  {
    "page": 416,
    "content": "398 Chapter 9. Priority Queues\nC-9.35 Provide a justiﬁcation of the time bounds in Table 9.5.\nC-9.36 Give an alternative analysis of bottom-up heap construction by showing the fol-\nlowing summation is O(1), for any positive integer h:\nh\n∑\ni=1/parenleftbig\ni/2i/parenrightbig\n.\nC-9.37 Suppose two binary trees, T1andT2, hold entries satisfying the heap-order prop-\nerty (but not necessarily the complete binary tree property). Describe a methodfor combining T\n1andT2into a binary tree T, whose nodes hold the union of\nthe entries in T1andT2and also satisfy the heap-order property. Your algorithm\nshould run in time O(h1+h2)where h1andh2are the respective heights of T1\nandT2.\nC-9.38 Tamarindo Airlines wants to give a ﬁrst-class upgrade coupon to their top log n\nfrequent ﬂyers, based on the number of miles accumulated, where nis the total\nnumber of the airlines’ frequent ﬂyers. The algorithm they currently use, which\nruns in O(nlogn)time, sorts the ﬂyers by the number of miles ﬂown and then\nscans the sorted list to pick the top log nﬂyers. Describe an algorithm that iden-\ntiﬁes the top log nﬂyers in O(n)time.\nC-9.39 Explain how the klargest elements from an unordered collection of size ncan be\nfound in time O(n+klogn)using a maximum-oriented heap.\nC-9.40 Explain how the klargest elements from an unordered collection of size ncan be\nfound in time O(nlogk)using O(k)auxiliary space.\nC-9.41 Write a comparator for nonnegative integers that determines order based on the\nnumber of 1’s in each integer’s binary expansion, so that i<jif the number of\n1’s in the binary representation of iis less than the number of 1’s in the binary\nrepresentation of j.\nC-9.42 Implement the binarySearch algorithm (see Section 5.1.3) using a Comparator\nfor an array with elements of generic type E.\nC-9.43 Given a class, MinPriorityQueue , that implements the minimum-oriented pri-\nority queue ADT, provide an implementation of a MaxPriorityQueue class that\nadapts to provide a maximum-oriented abstraction with methods insert,max, and\nremoveMax . Your implementation should not make any assumption about the in-\nternal workings of the original MinPriorityQueue class, nor the type of keys that\nmight be used.\nC-9.44 Describe an in-place version of the selection-sort algorithm for an array that usesonly O(1)space for instance variables in addition to the array.\nC-9.45 Assuming the input to the sorting problem is given in an array A, describe how\nto implement the insertion-sort algorithm using only the array Aand at most six\nadditional (base-type) variables.\nC-9.46 Give an alternate description of the in-place heap-sort algorithm using the stan-\ndard minimum-oriented priority queue (instead of a maximum-oriented one).\nwww.it-ebooks.info"
  },
  {
    "page": 417,
    "content": "9.6. Exercises 399\nC-9.47 A group of children want to play a game, called Unmonopoly , where in each turn\nthe player with the most money must give half of his/her money to the player\nwith the least amount of money. What data structure(s) should be used to play\nthis game efﬁciently? Why?\nC-9.48 An online computer system for trading stocks needs to process orders of the form\n“buy 100 shares at $ xeach” or “sell 100 shares at $ yeach.” A buy order for $ x\ncan only be processed if there is an existing sell order with price $ ysuch that\ny≤x. Likewise, a sell order for $ ycan only be processed if there is an existing\nbuy order with price $ xsuch that y≤x. If a buy or sell order is entered but\ncannot be processed, it must wait for a future order that allows it to be processed.\nDescribe a scheme that allows buy and sell orders to be entered in O(logn)time,\nindependent of whether or not they can be immediately processed.\nC-9.49 Extend a solution to the previous problem so that users are allowed to update the\nprices for their buy or sell orders that have yet to be processed.\nProjects\nP-9.50 Implement the in-place heap-sort algorithm. Experimentally compare its running\ntime with that of the standard heap-sort that is not in-place.\nP-9.51 Use the approach of either Exercise C-9.39 or C-9.40 to reimplement the method\ngetFavorites of theFavoritesListMTF class from Section 7.7.2. Make sure that\nresults are generated from largest to smallest.\nP-9.52 Develop a Java implementation of an adaptable priority queue that is based on an\nunsorted list and supports location-aware entries.\nP-9.53 Write an applet or stand-alone graphical program that animates a heap. Your\nprogram should support all the priority queue operations and should visualize\nthe swaps in the up-heap and down-heap bubblings. (Extra: Visualize bottom-up\nheap construction as well.)\nP-9.54 Write a program that can process a sequence of stock buy and sell orders as\ndescribed in Exercise C-9.48.\nP-9.55 One of the main applications of priority queues is in operating systems—for\nscheduling jobs on a CPU. In this project you are to build a program that sched-\nules simulated CPU jobs. Your program should run in a loop, each iteration of\nwhich corresponds to a time slice for the CPU. Each job is assigned a priority,\nwhich is an integer between −20 (highest priority) and 19 (lowest priority), inclu-\nsive. From among all jobs waiting to be processed in a time slice, the CPU must\nwork on a job with highest priority. In this simulation, each job will also come\nwith a length value, which is an integer between 1 and 100, inclusive, indicating\nthe number of time slices that are needed to process this job. For simplicity, you\nmay assume jobs cannot be interrupted—once it is scheduled on the CPU, a job\nruns for a number of time slices equal to its length. Your simulator must output\nthe name of the job running on the CPU in each time slice and must process a\nsequence of commands, one per time slice, each of which is of the form “add job\nname with length nand priority p” or “no new job this slice”.\nwww.it-ebooks.info"
  },
  {
    "page": 418,
    "content": "400 Chapter 9. Priority Queues\nP-9.56 LetSbe a set of npoints in the plane with distinct integer x- and y-coordinates.\nLetTbe a complete binary tree storing the points from Sat its external nodes,\nsuch that the points are ordered left to right by increasing x-coordinates. For\neach node vinT, letS(v)denote the subset of Sconsisting of points stored in the\nsubtree rooted at v. For the root rofT, deﬁne top(r)to be the point in S=S(r)\nwith maximal y-coordinate. For every other node v, deﬁne top(r)to be the point\ninSwith highest y-coordinate in S(v)that is not also the highest y-coordinate in\nS(u), where uis the parent of vinT(if such a point exists). Such labeling turns\nTinto a priority search tree . Describe a linear-time algorithm for turning Tinto\na priority search tree. Implement this approach.\nChapter Notes\nKnuth’s book on sorting and searching [61] describes the motivation and history for the\nselection-sort, insertion-sort, and heap-sort algorithms. The heap-sort algorithm is due\nto Williams [95], and the linear-time heap construction algorithm is due to Floyd [35].\nAdditional algorithms and analyses for heaps and heap-sort variations can be found in\npapers by Bentley [14], Carlsson [21], Gonnet and Munro [39], McDiarmid and Reed [69],\nand Schaffer and Sedgewick [82].\nwww.it-ebooks.info"
  },
  {
    "page": 419,
    "content": "Chapter\n10Maps, Hash Tables, and Skip Lists\nContents\n10.1 Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n10.1.1 The Map ADT . . . . . . . . . . . . . . . . . . . . . . . 403\n10.1.2 Application: Counting Word Frequencies . . . . . . . . . . 405\n10.1.3 An AbstractMap Base Class . . . . . . . . . . . . . . . . 406\n10.1.4 A Simple Unsorted Map Implementation . . . . . . . . . . 408\n10.2 Hash Tables . . . . . . . . . . . . . . . . . . . . . . . . . . 410\n10.2.1 Hash Functions . . . . . . . . . . . . . . . . . . . . . . . 411\n10.2.2 Collision-Handling Schemes . . . . . . . . . . . . . . . . . 417\n10.2.3 Load Factors, Rehashing, and Eﬃciency . . . . . . . . . . 420\n10.2.4 Java Hash Table Implementation . . . . . . . . . . . . . . 422\n10.3 Sorted Maps . . . . . . . . . . . . . . . . . . . . . . . . . . 428\n10.3.1 Sorted Search Tables . . . . . . . . . . . . . . . . . . . . 429\n10.3.2 Two Applications of Sorted Maps . . . . . . . . . . . . . 433\n10.4 Skip Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436\n10.4.1 Search and Update Operations in a Skip List . . . . . . . 438\n10.4.2 Probabilistic Analysis of Skip Lists ⋆. . . . . . . . . . . . 442\n10.5 Sets, Multisets, and Multimaps . . . . . . . . . . . . . . . 445\n10.5.1 The Set ADT . . . . . . . . . . . . . . . . . . . . . . . . 445\n10.5.2 The Multiset ADT . . . . . . . . . . . . . . . . . . . . . 447\n10.5.3 The Multimap ADT . . . . . . . . . . . . . . . . . . . . . 448\n10.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\nwww.it-ebooks.info"
  },
  {
    "page": 420,
    "content": "402 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.1 Maps\nAmap is an abstract data type designed to efﬁciently store and retrieve values based\nupon a uniquely identifying search key for each. Speciﬁcally, a map stores key-\nvalue pairs (k,v), which we call entries , where kis the key and vis its corresponding\nvalue. Keys are required to be unique, so that the association of keys to values\ndeﬁnes a mapping. Figure 10.1 provides a conceptual illustration of a map using\nthe ﬁle-cabinet metaphor. For a more modern metaphor, think about the web as\nbeing a map whose entries are the web pages. The key of a page is its URL (e.g.,\nhttp://datastructures.net/ ) and its value is the page content.\nFigure 10.1: A conceptual illustration of the map ADT. Keys (labels) are assigned\nto values (folders) by a user. The resulting entries (labeled folders) are inserted into\nthe map (ﬁle cabinet). The keys can be used later to retrieve or remove values.\nMaps are also known as associative arrays , because the entry’s key serves\nsomewhat like an index into the map, in that it assists the map in efﬁciently lo-\ncating the associated entry. However, unlike a standard array, a key of a map need\nnot be numeric, and is does not directly designate a position within the structure.\nCommon applications of maps include the following:\n•A university’s information system relies on some form of a student ID as a\nkey that is mapped to that student’s associated record (such as the student’s\nname, address, and course grades) serving as the value.\n•The domain-name system (DNS) maps a host name, such as www.wiley.com ,\nto an Internet-Protocol (IP) address, such as 208.215.179.146 .\n•A social media site typically relies on a (nonnumeric) username as a key that\ncan be efﬁciently mapped to a particular user’s associated information.\n•A company’s customer base may be stored as a map, with a customer’s ac-\ncount number or unique user ID as a key, and a record with the customer’s\ninformation as a value. The map would allow a service representative to\nquickly access a customer’s record, given the key.\n•A computer graphics system may map a color name, such as 'turquoise ',\nto the triple of numbers that describes the color’s RGB (red-green-blue) rep-\nresentation, such as (64, 224, 208) .\nwww.it-ebooks.info"
  },
  {
    "page": 421,
    "content": "10.1. Maps 403\n10.1.1 The Map ADT\nSince a map stores a collection of objects, it should be viewed as a collection of\nkey-value pairs. As an ADT, a map Msupports the following methods:\nsize() :Returns the number of entries in M.\nisEmpty() :Returns a boolean indicating whether Mis empty.\nget(k):Returns the value vassociated with key k, if such an entry exists;\notherwise returns null.\nput(k,v):IfMdoes not have an entry with key equal to k, then adds entry\n(k,v)toMand returns null; else, replaces with vthe existing\nvalue of the entry with key equal to kand returns the old value.\nremove( k):Removes from Mthe entry with key equal to k, and returns its\nvalue; if Mhas no such entry, then returns null.\nkeySet() :Returns an iterable collection containing all the keys stored in M.\nvalues() :Returns an iterable collection containing all the values of entries\nstored in M(with repetition if multiple keys map to the same\nvalue).\nentrySet() :Returns an iterable collection containing all the key-value en-\ntries in M.\nMaps in the java.util Package\nOur deﬁnition of the map ADT is a simpliﬁed version of the java.util.Map interface.\nFor the elements of the iteration returned by entrySet , we will rely on the composite\nEntry interface introduced in Section 9.2.1 (the java.util.Map relies on the nested\njava.util.Map.Entry interface).\nNotice that each of the operations get(k),put(k,v), andremove( k)returns the\nexisting value associated with key k, if the map has such an entry, and otherwise\nreturnsnull. This introduces ambiguity in an application for which nullis allowed\nas a natural value associated with a key k. That is, if an entry ( k,null) exists in a\nmap, then the operation get(k)will return null, not because it couldn’t ﬁnd the key,\nbut because it found the key and is returning its associated value.\nSome implementations of the java.util.Map interface explicitly forbid use of\nanullvalue (and nullkeys, for that matter). However, to resolve the ambiguity\nwhennullis allowable, the interface contains a boolean method, containsKey( k)to\ndeﬁnitively check whether kexists as a key. (We leave implementation of such a\nmethod as an exercise.)\nwww.it-ebooks.info"
  },
  {
    "page": 422,
    "content": "404 Chapter 10. Maps, Hash Tables, and Skip Lists\nExample 10.1: In the following, we show the effect of a series of operations on\nan initially empty map storing entries with integer keys and single-character values.\nMethod Return Value Map\nisEmpty() true {}\nput(5,A) null {(5,A)}\nput(7,B) null {(5,A),(7,B)}\nput(2,C) null{(5,A),(7,B),(2,C)}\nput(8,D) null{(5,A),(7,B),(2,C),(8,D)}\nput(2,E) C{(5,A),(7,B),(2,E),(8,D)}\nget(7) B{(5,A),(7,B),(2,E),(8,D)}\nget(4) null{(5,A),(7,B),(2,E),(8,D)}\nget(2) E{(5,A),(7,B),(2,E),(8,D)}\nsize() 4{(5,A),(7,B),(2,E),(8,D)}\nremove(5) A{(7,B),(2,E),(8,D)}\nremove(2) E{(7,B),(8,D)}\nget(2) null {(7,B),(8,D)}\nremove(2) null {(7,B),(8,D)}\nisEmpty() false {(7,B),(8,D)}\nentrySet(){(7,B),(8,D)}{(7,B),(8,D)}\nkeySet(){7,8}{(7,B),(8,D)}\nvalues(){B,D}{(7,B),(8,D)}\nA Java Interface for the Map ADT\nA formal deﬁnition of a Java interface for our version of the map ADT is given\nin Code Fragment 10.1. It uses the generics framework (Section 2.5.2), with K\ndesignating the key type and Vdesignating the value type.\n1public interface Map<K,V>{\n2intsize();\n3boolean isEmpty();\n4V get(K key);\n5V put(K key, V value);\n6V remove(K key);\n7Iterable<K>keySet();\n8Iterable<V>values();\n9Iterable<Entry<K,V>>entrySet();\n10}\nCode Fragment 10.1: Java interface for our simpliﬁed version of the map ADT.\nwww.it-ebooks.info"
  },
  {
    "page": 423,
    "content": "10.1. Maps 405\n10.1.2 Application: Counting Word Frequencies\nAs a case study for using a map, consider the problem of counting the number\nof occurrences of words in a document. This is a standard task when performing a\nstatistical analysis of a document, for example, when categorizing an email or news\narticle. A map is an ideal data structure to use here, for we can use words as keys\nand word counts as values. We show such an application in Code Fragment 10.2.\nWe begin with an empty map, mapping words to their integer frequencies. (We\nrely on the ChainHashMap class that will be introduced in Section 10.2.4.) We\nﬁrst scan through the input, considering adjacent alphabetic characters to be words,\nwhich we then convert to lowercase. For each word found, we attempt to retrieve\nits current frequency from the map using the getmethod, with a yet unseen word\nhaving frequency zero. We then (re)set its frequency to be one more to reﬂect the\ncurrent occurrence of the word. After processing the entire input, we loop through\ntheentrySet() of the map to determine which word has the most occurrences.\n1/∗∗A program that counts words in a document, printing the most frequent. ∗/\n2public class WordCount{\n3public static void main(String[ ] args) {\n4Map<String,Integer >freq =newChainHashMap <>();// or any concrete map\n5// scan input for words, using all nonletters as delimiters\n6Scanner doc = newScanner(System.in).useDelimiter( \"[^a-zA-Z]+\" );\n7while(doc.hasNext()){\n8 String word = doc.next().toLowerCase(); // convert next word to lowercase\n9 Integer count = freq.get(word); // get the previous count for this word\n10 if(count == null)\n11 count = 0; // if not in map, previous count is zero\n12 freq.put(word, 1 + count); // (re)assign new count for this word\n13}\n14intmaxCount = 0;\n15String maxWord = \"no word\" ;\n16for(Entry<String,Integer >ent : freq.entrySet()) // ﬁnd max-count word\n17 if(ent.getValue() >maxCount){\n18 maxWord = ent.getKey();\n19 maxCount = ent.getValue();\n20}\n21System.out.print( \"The most frequent word is '\"+ maxWord);\n22System.out.println( \"'with \"+ maxCount + \" occurrences.\" );\n23}\n24}\nCode Fragment 10.2: A program for counting word frequencies in a document,\nprinting the most frequent word. The document is parsed using the Scanner class,\nfor which we change the delimiter for separating tokens from whitespace to any\nnonletter. We also convert words to lowercase.\nwww.it-ebooks.info"
  },
  {
    "page": 424,
    "content": "406 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.1.3 An AbstractMap Base Class\nIn the remainder of this chapter (and the next), we will be providing many different\nimplementations of the map ADT using a variety of data structures, each with its\nown trade-off of advantages and disadvantages. As we have done in earlier chap-\nters, we rely on a combination of abstract and concrete classes in the interest of\ngreater code reuse. Figure 10.2 provides a preview of those classes.\n(Section 10.1.1)\nSortedTableMap\n(Chapter 11)TreeMap\n(Section 10.2.4)AbstractHashMap\n(Section 10.2.4)ChainHashMap\n(Section 10.2.4)ProbeHashMap(Section 10.1.4)UnsortedTableMap(Section 10.3)AbstractSortedMap\n(Section 10.1.3)AbstractMapMap\n(Section 10.3.1)≪interface≫\nSortedMap\n(Section 10.3)≪interface≫\n(additional subclasses)\nFigure 10.2: Our hierarchy of map types (with references to where they are deﬁned).\nWe begin, in this section, by designing an AbstractMap base class that provides\nfunctionality that is shared by all of our map implementations. More speciﬁcally,\nthe base class (given in Code Fragment 10.3) provides the following support:\n•An implementation of the isEmpty method, based upon the presumed imple-\nmentation of the sizemethod.\n•A nested MapEntry class that implements the public Entry interface, while\nproviding a composite for storing key-value entries in a map data structure.\n•Concrete implementations of the keySet andvalues methods, based upon an\nadaption to the entrySet method. In this way, concrete map classes need only\nimplement the entrySet method to provide all three forms of iteration.\nWe implement the iterations using the technique introduced in Section 7.4.2\n(at that time providing an iteration of all elements of a positional list given\nan iteration of all positions of the list).\nwww.it-ebooks.info"
  },
  {
    "page": 425,
    "content": "10.1. Maps 407\n1public abstract class AbstractMap <K,V>implements Map<K,V>{\n2public boolean isEmpty(){returnsize() == 0;}\n3//---------------- nested MapEntry class ----------------\n4protected static class MapEntry <K,V>implements Entry<K,V>{\n5privateK k;// key\n6privateV v;// value\n7publicMapEntry(K key, V value) {\n8 k = key;\n9 v = value;\n10}\n11// public methods of the Entry interface\n12publicK getKey(){returnk;}\n13publicV getValue(){returnv;}\n14// utilities not exposed as part of the Entry interface\n15protected void setKey(K key){k = key;}\n16protected V setValue(V value) {\n17 V old = v;\n18 v = value;\n19 returnold;\n20}\n21}//----------- end of nested MapEntry class -----------\n22\n23// Support for public keySet method...\n24private class KeyIterator implements Iterator<K>{\n25privateIterator<Entry<K,V>>entries = entrySet().iterator(); // reuse entrySet\n26public boolean hasNext(){returnentries.hasNext(); }\n27publicK next(){returnentries.next().getKey(); } // return key!\n28public void remove(){throw new UnsupportedOperationException(); }\n29}\n30private class KeyIterable implements Iterable<K>{\n31publicIterator<K>iterator(){return new KeyIterator();}\n32}\n33publicIterable<K>keySet(){return new KeyIterable();}\n34\n35// Support for public values method...\n36private class ValueIterator implements Iterator<V>{\n37privateIterator<Entry<K,V>>entries = entrySet().iterator(); // reuse entrySet\n38public boolean hasNext(){returnentries.hasNext(); }\n39publicV next(){returnentries.next().getValue(); } // return value!\n40public void remove(){throw new UnsupportedOperationException(); }\n41}\n42private class ValueIterable implements Iterable<V>{\n43publicIterator<V>iterator(){return new ValueIterator();}\n44}\n45publicIterable<V>values(){return new ValueIterable();}\n46}\nCode Fragment 10.3: Implementation of the AbstractMap base class.\nwww.it-ebooks.info"
  },
  {
    "page": 426,
    "content": "408 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.1.4 A Simple Unsorted Map Implementation\nWe demonstrate the use of the AbstractMap class with a very simple concrete\nimplementation of the map ADT that relies on storing key-value pairs in arbitrary\norder within a Java ArrayList . The presentation of such an UnsortedTableMap class\nis given in Code Fragments 10.4 and 10.5.\nEach of the fundamental methods get(k),put(k,v), andremove( k)requires an\ninitial scan of the array to determine whether an entry with key equal to kexists.\nFor this reason, we provide a nonpublic utility, ﬁndIndex(key) , that returns the\nindex at which such an entry is found, or −1 if no such entry is found. (See Code\nFragment 10.4.)\nThe rest of the implementation is rather simple. One subtlety worth mentioning\nis the way in which we remove an entry from the array list. Although we could use\ntheremove method of the ArrayList class, that would result in an unnecessary loop\nto shift all subsequent entries to the left. Because the map is unordered, we prefer\nto ﬁll the vacated cell of the array by relocating the last entry to that location. Such\nan update step runs in constant time.\nUnfortunately, the UnsortedTableMap class on the whole is not very efﬁcient.\nOn a map with nentries, each of the fundamental methods takes O(n)time in the\nworst case because of the need to scan through the entire list when searching for an\nexisting entry. Fortunately, as we discuss in the next section, there is a much faster\nstrategy for implementing the map ADT.\n1public class UnsortedTableMap <K,V>extends AbstractMap <K,V>{\n2/∗∗Underlying storage for the map of entries. ∗/\n3privateArrayList <MapEntry <K,V>>table =newArrayList <>();\n4\n5/∗∗Constructs an initially empty map. ∗/\n6publicUnsortedTableMap() {}\n7\n8// private utility\n9/∗∗Returns the index of an entry with equal key, or −1 if none found. ∗/\n10private int ﬁndIndex(K key){\n11intn = table.size();\n12for(intj=0; j<n; j++)\n13 if(table.get(j).getKey().equals(key))\n14 returnj;\n15return−1; // special value denotes that key was not found\n16}\nCode Fragment 10.4: An implementation of a map using a Java ArrayList as an\nunsorted table. (Continues in Code Fragment 10.5.) The parent class AbstractMap\nis given in Code Fragment 10.3.\nwww.it-ebooks.info"
  },
  {
    "page": 427,
    "content": "10.1. Maps 409\n17/∗∗Returns the number of entries in the map. ∗/\n18public int size(){returntable.size();}\n19/∗∗Returns the value associated with the speciﬁed key (or else null). ∗/\n20publicV get(K key){\n21intj = ﬁndIndex(key);\n22if(j ==−1)return null; // not found\n23returntable.get(j).getValue();\n24}\n25/∗∗Associates given value with given key, replacing a previous value (if any). ∗/\n26publicV put(K key, V value) {\n27intj = ﬁndIndex(key);\n28if(j ==−1){\n29 table.add( newMapEntry <>(key, value)); // add new entry\n30 return null;\n31}else // key already exists\n32 returntable.get(j).setValue(value); // replaced value is returned\n33}\n34/∗∗Removes the entry with the speciﬁed key (if any) and returns its value. ∗/\n35publicV remove(K key){\n36intj = ﬁndIndex(key);\n37intn = size();\n38if(j ==−1)return null; // not found\n39V answer = table.get(j).getValue();\n40if(j != n−1)\n41 table.set(j, table.get(n −1));// relocate last entry to ’hole’ created by removal\n42table.remove(n−1); // remove last entry of table\n43returnanswer;\n44}\n45// Support for public entrySet method...\n46private class EntryIterator implements Iterator<Entry<K,V>>{\n47private int j=0;\n48public boolean hasNext(){returnj<table.size();}\n49publicEntry<K,V>next(){\n50 if(j == table.size()) throw new NoSuchElementException();\n51 returntable.get(j++);\n52}\n53public void remove(){throw new UnsupportedOperationException(); }\n54}\n55private class EntryIterable implements Iterable<Entry<K,V>>{\n56publicIterator<Entry<K,V>>iterator(){return new EntryIterator();}\n57}\n58/∗∗Returns an iterable collection of all key-value entries of the map. ∗/\n59publicIterable<Entry<K,V>>entrySet(){return new EntryIterable();}\n60}\nCode Fragment 10.5: An implementation of a map using a Java ArrayList as an\nunsorted table (continued from Code Fragment 10.4).\nwww.it-ebooks.info"
  },
  {
    "page": 428,
    "content": "410 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.2 Hash Tables\nIn this section, we introduce one of the most efﬁcient data structures for imple-\nmenting a map, and the one that is used most in practice. This structure is known\nas ahash table .\nIntuitively, a map Msupports the abstraction of using keys as “addresses” that\nhelp locate an entry. As a mental warm-up, consider a restricted setting in which\na map with nentries uses keys that are known to be integers in a range from 0 to\nN−1 for some N≥n. In this case, we can represent the map using a lookup table\nof length N, as diagrammed in Figure 10.3.\n0 1 2 3 4 5 6 7 8 9 10\nD Z C Q\nFigure 10.3: A lookup table with length 11 for a map containing entries (1,D), (3,Z),\n(6,C), and (7,Q).\nIn this representation, we store the value associated with key kat index kof the\ntable (presuming that we have a distinct way to represent an empty slot). Basic map\noperations get,put, andremove can be implemented in O(1)worst-case time.\nThere are two challenges in extending this framework to the more general set-\nting of a map. First, we may not wish to devote an array of length Nif it is the case\nthatN≫n. Second, we do not in general require that a map’s keys be integers.\nThe novel concept for a hash table is the use of a hash function to map general\nkeys to corresponding indices in a table. Ideally, keys will be well distributed in the\nrange from 0 to N−1 by a hash function, but in practice there may be two or more\ndistinct keys that get mapped to the same index. As a result, we will conceptualize\nour table as a bucket array , as shown in Figure 10.4, in which each bucket may\nmanage a collection of entries that are sent to a speciﬁc index by the hash function.\n(To save space, an empty bucket may be replaced by a nullreference.)\n0 1 2 3 4 5 6 7 8 9 10\n(1,D) (25,C)\n(3,F)\n(14,Z)(39,C)(6,A) (7,Q)\nFigure 10.4: A bucket array of capacity 11 with entries (1,D), (25,C), (3,F), (14,Z),\n(6,A), (39,C), and (7,Q), using a simple hash function.\nwww.it-ebooks.info"
  },
  {
    "page": 429,
    "content": "10.2. Hash Tables 411\n10.2.1 Hash Functions\nThe goal of a hash function ,h, is to map each key kto an integer in the range\n[0,N−1], where Nis the capacity of the bucket array for a hash table. Equipped\nwith such a hash function, h, the main idea of this approach is to use the hash\nfunction value, h(k), as an index into our bucket array, A, instead of the key k\n(which may not be appropriate for direct use as an index). That is, we store the\nentry(k,v)in the bucket A[h(k)].\nIf there are two or more keys with the same hash value, then two different\nentries will be mapped to the same bucket in A. In this case, we say that a collision\nhas occurred. To be sure, there are ways of dealing with collisions, which we will\ndiscuss later, but the best strategy is to try to avoid them in the ﬁrst place. We say\nthat a hash function is “good” if it maps the keys in our map so as to sufﬁciently\nminimize collisions. For practical reasons, we also would like a hash function to\nbe fast and easy to compute.\nIt is common to view the evaluation of a hash function, h(k), as consisting of\ntwo portions—a hash code that maps a key kto an integer, and a compression\nfunction that maps the hash code to an integer within a range of indices, [0,N−1],\nfor a bucket array. (See Figure 10.5.)\n−2hash code\ncompression functionArbitrary Objects\n0\n1 0 2 N−1···−1··· ··· 2 1\nFigure 10.5: Two parts of a hash function: a hash code and a compression function.\nThe advantage of separating the hash function into two such components is that\nthe hash code portion of that computation is independent of a speciﬁc hash table\nsize. This allows the development of a general hash code for each object that can\nbe used for a hash table of any size; only the compression function depends upon\nthe table size. This is particularly convenient, because the underlying bucket array\nfor a hash table may be dynamically resized, depending on the number of entries\ncurrently stored in the map. (See Section 10.2.3.)\nwww.it-ebooks.info"
  },
  {
    "page": 430,
    "content": "412 Chapter 10. Maps, Hash Tables, and Skip Lists\nHash Codes\nThe ﬁrst action that a hash function performs is to take an arbitrary key kin our\nmap and compute an integer that is called the hash code fork; this integer need not\nbe in the range [0,N−1], and may even be negative. We desire that the set of hash\ncodes assigned to our keys should avoid collisions as much as possible. For if the\nhash codes of our keys cause collisions, then there is no hope for our compression\nfunction to avoid them. In this subsection, we begin by discussing the theory of\nhash codes. Following that, we discuss practical implementations of hash codes in\nJava.\nTreating the Bit Representation as an Integer\nTo begin, we note that, for any data type Xthat is represented using at most as many\nbits as our integer hash codes, we can simply take as a hash code for Xan integer\ninterpretation of its bits. Java relies on 32-bit hash codes, so for base types byte ,\nshort ,int, andchar , we can achieve a good hash code simply by casting a value\ntoint. Likewise, for a variable xof base type ﬂoat , we can convert xto an integer\nusing a call to Float.ﬂoatToIntBits( x), and then use this integer as x’s hash code.\nFor a type whose bit representation is longer than a desired hash code (such as\nJava’slong anddouble types), the above scheme is not immediately applicable.\nOne possibility is to use only the high-order 32 bits (or the low-order 32 bits). This\nhash code, of course, ignores half of the information present in the original key,\nand if many of the keys in our map only differ in these bits, then they will collide\nusing this simple hash code.\nA better approach is to combine in some way the high-order and low-order por-\ntions of a 64-bit key to form a 32-bit hash code, which takes all the original bits\ninto consideration. A simple implementation is to add the two components as 32-\nbit numbers (ignoring overﬂow), or to take the exclusive-or of the two components.\nThese approaches of combining components can be extended to any object xwhose\nbinary representation can be viewed as an n-tuple(x0,x1,..., xn−1)of 32-bit inte-\ngers, for example, by forming a hash code for xas∑n−1\ni=0xi, or as x0⊕x1⊕···⊕ xn−1,\nwhere the⊕symbol represents the bitwise exclusive-or operation (which is the\nˆoperator in Java).\nPolynomial Hash Codes\nThe summation and exclusive-or hash codes, described above, are not good choices\nfor character strings or other variable-length objects that can be viewed as tuples of\nthe form (x0,x1,..., xn−1), where the order of the xi’s is signiﬁcant. For example,\nconsider a 16-bit hash code for a character string sthat sums the Unicode values\nof the characters in s. This hash code unfortunately produces lots of unwanted\nwww.it-ebooks.info"
  },
  {
    "page": 431,
    "content": "10.2. Hash Tables 413\ncollisions for common groups of strings. In particular, \"temp01\" and\"temp10\"\ncollide using this function, as do \"stop\" ,\"tops\" ,\"pots\" , and\"spot\" . A better\nhash code should somehow take into consideration the positions of the xi’s. An\nalternative hash code, which does exactly this, is to choose a nonzero constant,\na/n⌉}ationslash=1, and use as a hash code the value\nx0an−1+x1an−2+···+xn−2a+xn−1.\nMathematically speaking, this is simply a polynomial in athat takes the compo-\nnents(x0,x1,..., xn−1)of an object xas its coefﬁcients. This hash code is therefore\ncalled a polynomial hash code . By Horner’s rule (see Exercise C-4.54), this poly-\nnomial can be computed as\nxn−1+a(xn−2+a(xn−3+···+a(x2+a(x1+ax0))···)).\nIntuitively, a polynomial hash code uses multiplication by different powers as a\nway to spread out the inﬂuence of each component across the resulting hash code.\nOf course, on a typical computer, evaluating a polynomial will be done using\nthe ﬁnite bit representation for a hash code; hence, the value will periodically over-\nﬂow the bits used for an integer. Since we are more interested in a good spread of\nthe object xwith respect to other keys, we simply ignore such overﬂows. Still, we\nshould be mindful that such overﬂows are occurring and choose the constant aso\nthat it has some nonzero, low-order bits, which will serve to preserve some of the\ninformation content even as we are in an overﬂow situation.\nWe have done some experimental studies that suggest that 33, 37, 39, and 41\nare particularly good choices for awhen working with character strings that are\nEnglish words. In fact, in a list of over 50,000 English words formed as the union\nof the word lists provided in two variants of Unix, we found that taking ato be 33,\n37, 39, or 41 produced fewer than 7 collisions in each case!\nCyclic-Shift Hash Codes\nA variant of the polynomial hash code replaces multiplication by awith a cyclic\nshift of a partial sum by a certain number of bits. For example, a 5-bit cyclic shift\nof the 32-bit value 00111 101100101101010100010101000 is achieved by taking\nthe leftmost ﬁve bits and placing those on the rightmost side of the representation,\nresulting in 10110010110101010001010100000111 . While this operation has little\nnatural meaning in terms of arithmetic, it accomplishes the goal of varying the bits\nof the calculation. In Java, a cyclic shift of bits can be accomplished through careful\nuse of the bitwise shift operators.\nwww.it-ebooks.info"
  },
  {
    "page": 432,
    "content": "414 Chapter 10. Maps, Hash Tables, and Skip Lists\nAn implementation of a cyclic-shift hash code computation for a character\nstring in Java appears as follows:\nstatic int hashCode(String s) {\ninth=0;\nfor(inti=0; i<s.length(); i++){\nh = (h<<5)|(h>>>27); // 5-bit cyclic shift of the running sum\nh += (int) s.charAt(i); // add in next character\n}\nreturnh;\n}\nAs with the traditional polynomial hash code, ﬁne-tuning is required when using a\ncyclic-shift hash code, as we must wisely choose the amount to shift by for each\nnew character. Our choice of a 5-bit shift is justiﬁed by experiments run on a list of\njust over 230,000 English words, comparing the number of collisions for various\nshift amounts (see Table 10.1).\nCollisions\nShift Total Max\n0234735 623\n1165076 43\n2 38471 13\n3 7174 5\n4 1379 3\n5 190 3\n6 502 2\n7 560 2\n8 5546 4\n9 393 3\n10 5194 5\n11 11559 5\n12 822 2\n13 900 4\n14 2001 4\n15 19251 8\n16 211781 37\nTable 10.1: Comparison of collision behavior for the cyclic-shift hash code as ap-\nplied to a list of 230,000 English words. The “Total” column records the total num-\nber of words that collide with at least one other, and the “Max” column records the\nmaximum number of words colliding at any one hash code. Note that with a cyclic\nshift of 0, this hash code reverts to the one that simply sums all the characters.\nwww.it-ebooks.info"
  },
  {
    "page": 433,
    "content": "10.2. Hash Tables 415\nHash Codes in Java\nThe notion of hash codes are an integral part of the Java language. The Object\nclass, which serves as an ancestor of all object types, includes a default hashCode()\nmethod that returns a 32-bit integer of type int, which serves as an object’s hash\ncode. The default version of hashCode() provided by the Object class is often just\nan integer representation derived from the object’s memory address.\nHowever, we must be careful if relying on the default version of hashCode()\nwhen authoring a class. For hashing schemes to be reliable, it is imperative that\nany two objects that are viewed as “equal” to each other have the same hash code.\nThis is important because if an entry is inserted into a map, and a later search isperformed on a key that is considered equivalent to that entry’s key, the map mustrecognize this as a match. (See, for example, the UnsortedTableMap.ﬁndIndex\nmethod in Code Fragment 10.4.) Therefore, when using a hash table to implementa map, we want equivalent keys to have the same hash code so that they are guar-anteed to map to the same bucket. More formally, if a class deﬁnes equivalencethrough the equals method (see Section 3.5), then that class should also provide a\nconsistent implementation of the hashCode method, such that if x.equals(y) then\nx.hashCode() == y.hashCode() .\nAs an example, Java’s String class deﬁnes the equals method so that two in-\nstances are equivalent if they have precisely the same sequence of characters. Thatclass also overrides the hashCode method to provide consistent behavior. In fact,\nthe implementation of hash codes for the String class is excellent. If we repeat\nthe experiment from the previous page using Java’s implementation of hash codes,there are only 12 collisions among more than 230,000 words. Java’s primitivewrapper classes also deﬁne hashCode , using techniques described on page 412.\nAs an example of how to properly implement hashCode for a user-deﬁned class,\nwe will revisit the SinglyLinkedList class from Chapter 3. We deﬁned the equals\nmethod for that class, in Section 3.5.2, so that two lists are equivalent if they rep-resent equal-length sequences of elements that are pairwise equivalent. We cancompute a robust hash code for a list by taking the exclusive-or of its elements’hash codes, while performing a cyclic shift. (See Code Fragment 10.6.)\n1public int hashCode(){\n2inth = 0;\n3for(Node walk=head; walk != null; walk = walk.getNext()) {\n4h ˆ= walk.getElement().hashCode(); // bitwise exclusive-or with element’s code\n5h = (h<<5)|(h>>>27); // 5-bit cyclic shift of composite code\n6}\n7returnh;\n8}\nCode Fragment 10.6: A robust implementation of the hashCode method for the\nSinglyLinkedList class from Chapter 3.\nwww.it-ebooks.info"
  },
  {
    "page": 434,
    "content": "416 Chapter 10. Maps, Hash Tables, and Skip Lists\nCompression Functions\nThe hash code for a key kwill typically not be suitable for immediate use with a\nbucket array, because the integer hash code may be negative or may exceed the ca-\npacity of the bucket array. Thus, once we have determined an integer hash code for\na key object k, there is still the issue of mapping that integer into the range [0,N−1].\nThis computation, known as a compression function , is the second action per-\nformed as part of an overall hash function. A good compression function is one\nthat minimizes the number of collisions for a given set of distinct hash codes.\nThe Division Method\nA simple compression function is the division method , which maps an integer ito\nimod N,\nwhere N, the size of the bucket array, is a ﬁxed positive integer. Additionally, if we\ntake Nto be a prime number, then this compression function helps “spread out” the\ndistribution of hashed values. Indeed, if Nis not prime, then there is greater risk\nthat patterns in the distribution of hash codes will be repeated in the distribution of\nhash values, thereby causing collisions. For example, if we insert keys with hash\ncodes{200,205,210,215,220,..., 600}into a bucket array of size 100, then each\nhash code will collide with three others. But if we use a bucket array of size 101,\nthen there will be no collisions. If a hash function is chosen well, it should ensure\nthat the probability of two different keys getting hashed to the same bucket is 1 /N.\nChoosing Nto be a prime number is not always enough, however, for if there is\na repeated pattern of hash codes of the form pN+qfor several different p’s, then\nthere will still be collisions.\nThe MAD Method\nA more sophisticated compression function, which helps eliminate repeated pat-\nterns in a set of integer keys, is the Multiply-Add-and-Divide (or “MAD”) method.\nThis method maps an integer ito\n[(ai+b)mod p]mod N,\nwhere Nis the size of the bucket array, pis a prime number larger than N, and a\nandbare integers chosen at random from the interval [0,p−1], with a>0. This\ncompression function is chosen in order to eliminate repeated patterns in the set of\nhash codes and get us closer to having a “good” hash function, that is, one such that\nthe probability any two different keys collide is 1 /N. This good behavior would be\nthe same as we would have if these keys were “thrown” into Auniformly at random.\nwww.it-ebooks.info"
  },
  {
    "page": 435,
    "content": "10.2. Hash Tables 417\n10.2.2 Collision-Handling Schemes\nThe main idea of a hash table is to take a bucket array, A, and a hash function, h, and\nuse them to implement a map by storing each entry (k,v)in the “bucket” A[h(k)].\nThis simple idea is challenged, however, when we have two distinct keys, k1andk2,\nsuch that h(k1)=h(k2). The existence of such collisions prevents us from simply\ninserting a new entry (k,v)directly into the bucket A[h(k)]. It also complicates our\nprocedure for performing insertion, search, and deletion operations.\nSeparate Chaining\nA simple and efﬁcient way for dealing with collisions is to have each bucket A[j]\nstore its own secondary container, holding all entries (k,v)such that h(k) =j. A\nnatural choice for the secondary container is a small map instance implemented\nusing an unordered list, as described in Section 10.1.4. This collision resolution\nrule is known as separate chaining , and is illustrated in Figure 10.6.\nA1 2 3 4 5 6 7 8 9 10 0 11 12\n123825\n9054\n28\n413618 10\nFigure 10.6: A hash table of size 13, storing 10 entries with integer keys, with\ncollisions resolved by separate chaining. The compression function is h(k)=kmod\n13. For simplicity, we do not show the values associated with the keys.\nIn the worst case, operations on an individual bucket take time proportional to\nthe size of the bucket. Assuming we use a good hash function to index the nentries\nof our map in a bucket array of capacity N, the expected size of a bucket is n/N.\nTherefore, if given a good hash function, the core map operations run in O(⌈n/N⌉).\nThe ratio λ=n/N, called the load factor of the hash table, should be bounded by\na small constant, preferably below 1. As long as λisO(1), the core operations on\nthe hash table run in O(1)expected time.\nwww.it-ebooks.info"
  },
  {
    "page": 436,
    "content": "418 Chapter 10. Maps, Hash Tables, and Skip Lists\nOpen Addressing\nThe separate chaining rule has many nice properties, such as affording simple im-\nplementations of map operations, but it nevertheless has one slight disadvantage: It\nrequires the use of an auxiliary data structure to hold entries with colliding keys. If\nspace is at a premium (for example, if we are writing a program for a small hand-\nheld device), then we can use the alternative approach of storing each entry directly\nin a table slot. This approach saves space because no auxiliary structures are em-\nployed, but it requires a bit more complexity to properly handle collisions. There\nare several variants of this approach, collectively referred to as open addressing\nschemes, which we discuss next. Open addressing requires that the load factor is\nalways at most 1 and that entries are stored directly in the cells of the bucket array\nitself.\nLinear Probing and Its Variants\nA simple method for collision handling with open addressing is linear probing .\nWith this approach, if we try to insert an entry (k,v)into a bucket A[j]that is already\noccupied, where j=h(k), then we next try A[(j+1)mod N]. IfA[(j+1)mod N]\nis also occupied, then we try A[(j+2)mod N], and so on, until we ﬁnd an empty\nbucket that can accept the new entry. Once this bucket is located, we simply insert\nthe entry there. Of course, this collision resolution strategy requires that we change\nthe implementation when searching for an existing key—the ﬁrst step of all get,\nput, orremove operations. In particular, to attempt to locate an entry with key equal\ntok, we must examine consecutive slots, starting from A[h(k)], until we either ﬁnd\nan entry with an equal key or we ﬁnd an empty bucket. (See Figure 10.7.) The\nname “linear probing” comes from the fact that accessing a cell of the bucket array\ncan be viewed as a “probe,” and that consecutive probes occur in neighboring cells\n(when viewed circularly).\n5 6 7 8 9 10 0New element with\nkey = 15 to be insertedMust probe 4 times\nbefore ﬁnding empty slot\n5 37 16 21 13 261 2 3 4\nFigure 10.7: Insertion into a hash table with integer keys using linear probing. The\nhash function is h(k)=kmod 11. Values associated with keys are not shown.\nwww.it-ebooks.info"
  },
  {
    "page": 437,
    "content": "10.2. Hash Tables 419\nTo implement a deletion, we cannot simply remove a found entry from its slot\nin the array. For example, after the insertion of key 15 portrayed in Figure 10.7,\nif the entry with key 37 were trivially deleted, a subsequent search for 15 wouldfail because that search would start by probing at index 4, then index 5, and thenindex 6, at which an empty cell is found. A typical way to get around this difﬁcultyis to replace a deleted entry with a special “defunct” sentinel object. With this\nspecial marker possibly occupying spaces in our hash table, we modify our search\nalgorithm so that the search for a key kwill skip over cells containing the defunct\nsentinel and continue probing until reaching the desired entry or an empty bucket(or returning back to where we started from). Additionally, our algorithm for put\nshould remember a defunct location encountered during the search for k, since this\nis a valid place to put a new entry (k,v), if no existing entry is found beyond it.\nAlthough use of an open addressing scheme can save space, linear probing\nsuffers from an additional disadvantage. It tends to cluster the entries of a map into\ncontiguous runs, which may even overlap (particularly if more than half of the cells\nin the hash table are occupied). Such contiguous runs of occupied hash cells causesearches to slow down considerably.\nAnother open addressing strategy, known as quadratic probing , iteratively tries\nthe buckets A[(h(k)+f(i))mod N], fori=0,1,2,..., where f(i)=i\n2, until ﬁnding\nan empty bucket. As with linear probing, the quadratic probing strategy compli-cates the removal operation, but it does avoid the kinds of clustering patterns thatoccur with linear probing. Nevertheless, it creates its own kind of clustering, called\nsecondary clustering , where the set of ﬁlled array cells still has a nonuniform pat-\ntern, even if we assume that the original hash codes are distributed uniformly. When\nNis prime and the bucket array is less than half full, the quadratic probing strategy\nis guaranteed to ﬁnd an empty slot. However, this guarantee is not valid once thetable becomes at least half full, or if Nis not chosen as a prime number; we explore\nthe cause of this type of clustering in an exercise (C-10.42).\nAn open addressing strategy that does not cause clustering of the kind produced\nby linear probing or the kind produced by quadratic probing is the double hashing\nstrategy. In this approach, we choose a secondary hash function, h\n′, and if hmaps\nsome key kto a bucket A[h(k)]that is already occupied, then we iteratively try\nthe buckets A[(h(k)+f(i))mod N]next, for i=1,2,3,..., where f(i) =i·h′(k).\nIn this scheme, the secondary hash function is not allowed to evaluate to zero; acommon choice is h\n′(k)=q−(kmod q), for some prime number q<N. Also, N\nshould be a prime.\nAnother approach to avoid clustering with open addressing is to iteratively try\nbuckets A[(h(k)+f(i))mod N]where f(i)is based on a pseudorandom number\ngenerator, providing a repeatable, but somewhat arbitrary, sequence of subsequent\nprobes that depends upon bits of the original hash code.\nwww.it-ebooks.info"
  },
  {
    "page": 438,
    "content": "420 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.2.3 Load Factors, Rehashing, and Eﬃciency\nIn the hash table schemes described thus far, it is important that the load factor,\nλ=n/N, be kept below 1. With separate chaining, as λgets very close to 1, the\nprobability of a collision greatly increases, which adds overhead to our operations,\nsince we must revert to linear-time list-based methods in buckets that have col-\nlisions. Experiments and average-case analyses suggest that we should maintain\nλ<0.9 for hash tables with separate chaining. (By default, Java’s implementation\nuses separate chaining with λ<0.75.)\nWith open addressing, on the other hand, as the load factor λgrows beyond 0 .5\nand starts approaching 1, clusters of entries in the bucket array start to grow as well.\nThese clusters cause the probing strategies to “bounce around” the bucket array\nfor a considerable amount of time before they ﬁnd an empty slot. In Exercise C-\n10.42, we explore the degradation of quadratic probing when λ≥0.5. Experiments\nsuggest that we should maintain λ<0.5 for an open addressing scheme with linear\nprobing, and perhaps only a bit higher for other open addressing schemes.\nIf an insertion causes the load factor of a hash table to go above the speciﬁed\nthreshold, then it is common to resize the table (to regain the speciﬁed load factor)\nand to reinsert all objects into this new table. Although we need not deﬁne a new\nhash code for each object, we do need to reapply a new compression function that\ntakes into consideration the size of the new table. Rehashing will generally scatter\nthe entries throughout the new bucket array. When rehashing to a new table, it is\na good requirement for the new array’s size to be a prime number approximately\ndouble the previous size (see Exercise C-10.32). In that way, the cost of rehashing\nall the entires in the table can be amortized against the time used to insert them in\nthe ﬁrst place (as with dynamic arrays; see Section 7.2.1).\nEﬃciency of Hash Tables\nAlthough the details of the average-case analysis of hashing are beyond the scope\nof this book, its probabilistic basis is quite intuitive. If our hash function is good,\nthen we expect the entries to be uniformly distributed in the Ncells of the bucket\narray. Thus, to store nentries, the expected number of keys in a bucket would\nbe⌈n/N⌉, which is O(1)ifnisO(N).\nThe costs associated with a periodic rehashing (when resizing a table after oc-\ncasional insertions or deletions) can be accounted for separately, leading to an ad-\nditional O(1)amortized cost for putandremove .\nIn the worst case, a poor hash function could map every entry to the same\nbucket. This would result in linear-time performance for the core map operations\nwith separate chaining, or with any open addressing model in which the secondary\nsequence of probes depends only on the hash code. A summary of these costs is\ngiven in Table 10.2.\nwww.it-ebooks.info"
  },
  {
    "page": 439,
    "content": "10.2. Hash Tables 421\nMethodUnsorted Hash Table\nList expected worst case\nget O(n) O(1) O(n)\nput O(n) O(1) O(n)\nremove O(n) O(1) O(n)\nsize,isEmpty O(1) O(1) O(1)\nentrySet ,keySet ,values O(n) O(n) O(n)\nTable 10.2: Comparison of the running times of the methods of a map realized by\nmeans of an unsorted list (as in Section 10.1.4) or a hash table. We let ndenote\nthe number of entries in the map, and we assume that the bucket array supporting\nthe hash table is maintained such that its capacity is proportional to the number of\nentries in the map.\nAn Anecdote About Hashing and Computer Security\nIn a 2003 academic paper, researchers discuss the possibility of exploiting a hash\ntable’s worst-case performance to cause a denial-of-service (DoS) attack of Internet\ntechnologies. Since many published algorithms compute hash codes with a deter-\nministic function, an attacker could precompute a very large number of moderate-\nlength strings that all hash to the identical 32-bit hash code. (Recall that by any\nof the hashing schemes we describe, other than double hashing, if two keys are\nmapped to the same hash code, they will be inseparable in the collision resolution.)\nThis concern was brought to the attention of the Java development team, and that\nof many other programming languages, but deemed an insigniﬁcant risk at the time\nby most. (Kudos to the Perl team for implementing a ﬁx in 2003.)\nIn late 2011, another team of researchers demonstrated an implementation of\njust such an attack. Web servers allow a series of key-value parameters to be em-\nbedded in a URL using a syntax such as ?key1=val1&key2=val2&key3=val3 .\nThose key-value pairs are strings and a typical Web server immediately stores them\nin a hash-map. Servers already place a limit on the length and number of such\nparameters, to avoid overload, but they presume that the total insertion time in the\nmap will be linear in the number of entries, given the expected constant-time oper-\nations. However, if all keys were to collide, the insertions into the map will require\nquadratic time, causing the server to perform an inordinate amount of work.\nIn 2012, the OpenJDK team announced the following resolution: they dis-\ntributed a security patch that includes an alternative hash function that introduces\nrandomization into the computation of hash codes, making it less tractable to re-\nverse engineer a set of colliding strings. However, to avoid breaking existing code,\nthe new feature is disabled by default in Java SE 7 and, when enabled, is only used\nfor hashing strings and only when a table size grows beyond a certain threshold.\nEnhanced hashing will be enabled in Java SE 8 for all types and uses.\nwww.it-ebooks.info"
  },
  {
    "page": 440,
    "content": "422 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.2.4 Java Hash Table Implementation\nIn this section, we develop two implementations of a hash table, one using sep-\narate chaining and the other using open addressing with linear probing. While\nthese approaches to collision resolution are quite different, there are many higher-\nlevel commonalities to the two hashing algorithms. For that reason, we extend the\nAbstractMap class (from Code Fragment 10.3) to deﬁne a new AbstractHashMap\nclass (see Code Fragment 10.7), which provides much of the functionality common\nto our two hash table implementations.\nWe will begin by discussing what this abstract class does notdo—it does not\nprovide any concrete representation of a table of “buckets.” With separate chaining,\neach bucket will be a secondary map. With open addressing, however, there is no\ntangible container for each bucket; the “buckets” are effectively interleaved due to\nthe probing sequences. In our design, the AbstractHashMap class presumes the\nfollowing to be abstract methods—to be implemented by each concrete subclass:\ncreateTable() :This method should create an initially empty table having\nsize equal to a designated capacity instance variable.\nbucketGet( h,k):This method should mimic the semantics of the public get\nmethod, but for a key kthat is known to hash to bucket h.\nbucketPut( h,k,v):This method should mimic the semantics of the public put\nmethod, but for a key kthat is known to hash to bucket h.\nbucketRemove( h,k):This method should mimic the semantics of the public\nremove method, but for a key kknown to hash to bucket h.\nentrySet() :This standard map method iterates through allentries of the\nmap. We do not delegate this on a per-bucket basis because\n“buckets” in open addressing are not inherently disjoint.\nWhat the AbstractHashMap class does provide is mathematical support in\nthe form of a hash compression function using a randomized Multiply-Add-and-\nDivide (MAD) formula, and support for automatically resizing the underlying hash\ntable when the load factor reaches a certain threshold.\nThehashValue method relies on an original key’s hash code, as returned by its\nhashCode() method, followed by MAD compression based on a prime number and\nthescale andshift parameters that are randomly chosen in the constructor.\nTo manage the load factor, the AbstractHashMap class declares a protected\nmember,n, which should equal the current number of entries in the map; however,\nit must rely on the subclasses to update this ﬁeld from within methods bucketPut\nandbucketRemove . If the load factor of the table increases beyond 0 .5, we request\na bigger table (using the createTable method) and reinsert all entries into the new\ntable. (For simplicity, this implementation uses tables of size 2k+1, even though\nthese are not generally prime.)\nwww.it-ebooks.info"
  },
  {
    "page": 441,
    "content": "10.2. Hash Tables 423\n1public abstract class AbstractHashMap <K,V>extends AbstractMap <K,V>{\n2protected int n = 0; // number of entries in the dictionary\n3protected int capacity; // length of the table\n4private int prime; // prime factor\n5private long scale, shift; // the shift and scaling factors\n6publicAbstractHashMap( intcap,intp){\n7prime = p;\n8capacity = cap;\n9Random rand = newRandom();\n10scale = rand.nextInt(prime −1) + 1;\n11shift = rand.nextInt(prime);\n12createTable();\n13}\n14publicAbstractHashMap( intcap){this(cap, 109345121); }// default prime\n15publicAbstractHashMap() {this(17);} // default capacity\n16// public methods\n17public int size(){returnn;}\n18publicV get(K key){returnbucketGet(hashValue(key), key); }\n19publicV remove(K key){returnbucketRemove(hashValue(key), key); }\n20publicV put(K key, V value) {\n21V answer = bucketPut(hashValue(key), key, value);\n22if(n>capacity / 2) // keep load factor <= 0.5\n23 resize(2 ∗capacity−1); // (or ﬁnd a nearby prime)\n24returnanswer;\n25}\n26// private utilities\n27private int hashValue(K key) {\n28return(int) ((Math.abs(key.hashCode() ∗scale + shift) % prime) % capacity);\n29}\n30private void resize(intnewCap){\n31ArrayList< Entry<K,V>>buﬀer = newArrayList <>(n);\n32for(Entry<K,V>e : entrySet())\n33 buﬀer.add(e);\n34capacity = newCap;\n35createTable(); // based on updated capacity\n36n = 0; // will be recomputed while reinserting entries\n37for(Entry<K,V>e : buﬀer)\n38 put(e.getKey(), e.getValue());\n39}\n40// protected abstract methods to be implemented by subclasses\n41protected abstract void createTable();\n42protected abstract V bucketGet( inth, K k);\n43protected abstract V bucketPut( inth, K k, V v);\n44protected abstract V bucketRemove( inth, K k);\n45}\nCode Fragment 10.7: A base class for our hash table implementations, extending\ntheAbstractMap class from Code Fragment 10.3.\nwww.it-ebooks.info"
  },
  {
    "page": 442,
    "content": "424 Chapter 10. Maps, Hash Tables, and Skip Lists\nSeparate Chaining\nTo represent each bucket for separate chaining, we use an instance of the simpler\nUnsortedTableMap class from Section 10.1.4. This technique, in which we use a\nsimple solution to a problem to create a new, more advanced solution, is known asbootstrapping . The advantage of using a map for each bucket is that it becomes\neasy to delegate responsibilities for top-level map operations to the appropriatebucket.\nThe entire hash table is then represented as a ﬁxed-capacity array Aof the\nsecondary maps. Each cell, A[h], is initially a null reference; we only create a\nsecondary map when an entry is ﬁrst hashed to a particular bucket.\nAs a general rule, we implement bucketGet( h,k)by calling A[h].get(k), we\nimplement bucketPut( h,k,v)by calling A[h].put(k,v), andbucketRemove( h,k)\nby calling A[h].remove( k). However, care is needed for two reasons.\nFirst, because we choose to leave table cells as nulluntil a secondary map is\nneeded, each of these fundamental operations must begin by checking to see if\nA[h]isnull. In the case of bucketGet andbucketRemove , if the bucket does not\nyet exist, we can simply return nullas there can not be any entry matching key k.\nIn the case of bucketPut , a new entry must be inserted, so we instantiate a new\nUnsortedTableMap forA[h]before continuing.\nThe second issue is that, in our AbstractHashMap framework, the subclass\nhas the responsibility to properly maintain the instance variable nwhen an entry is\nnewly inserted or deleted. Remember that when put(k,v)is called on a map, the\nsize of the map only increases if key kis new to the map (otherwise, the value of an\nexisting entry is reassigned). Similarly, a call to remove( k)only decreases the size\nof the map when an entry with key equal to kis found. In our implementation, we\ndetermine the change in the overall size of the map, by determining if there is any\nchange in the size of the relevant secondary map before and after an operation.\nCode Fragment 10.8 provides a complete deﬁnition for our ChainHashMap\nclass, which implements a hash table with separate chaining. If we assume that thehash function performs well, a map with nentries and a table of capacity Nwill\nhave an expected bucket size of n/N(recall, this is its load factor ). So even though\nthe individual buckets, implemented as UnsortedTableMap instances, are not par-\nticularly efﬁcient, each bucket has expected O(1)size, provided that nisO(N),\nas in our implementation. Therefore, the expected running time of operations get,\nput, andremove for this map is O(1). TheentrySet method (and thus the related\nkeySet andvalues ) runs in O(n+N)time, as it loops through the length of the table\n(with length N) and through all buckets (which have cumulative lengths n).\nwww.it-ebooks.info"
  },
  {
    "page": 443,
    "content": "10.2. Hash Tables 425\n1public class ChainHashMap <K,V>extends AbstractHashMap <K,V>{\n2// a ﬁxed capacity array of UnsortedTableMap that serve as buckets\n3privateUnsortedTableMap <K,V>[ ] table; // initialized within createTable\n4publicChainHashMap() {super();}\n5publicChainHashMap( intcap){super(cap);}\n6publicChainHashMap( intcap,intp){super(cap, p);}\n7/∗∗Creates an empty table having length equal to current capacity. ∗/\n8protected void createTable(){\n9table = (UnsortedTableMap <K,V>[ ])newUnsortedTableMap[capacity];\n10}\n11/∗∗Returns value associated with key k in bucket with hash value h, or else null. ∗/\n12protected V bucketGet( inth, K k){\n13UnsortedTableMap <K,V>bucket = table[h];\n14if(bucket == null)return null;\n15returnbucket.get(k);\n16}\n17/∗∗Associates key k with value v in bucket with hash value h; returns old value. ∗/\n18protected V bucketPut( inth, K k, V v){\n19UnsortedTableMap <K,V>bucket = table[h];\n20if(bucket == null)\n21 bucket = table[h] = newUnsortedTableMap <>();\n22intoldSize = bucket.size();\n23V answer = bucket.put(k,v);\n24n += (bucket.size() −oldSize); // size may have increased\n25returnanswer;\n26}\n27/∗∗Removes entry having key k from bucket with hash value h (if any). ∗/\n28protected V bucketRemove( inth, K k){\n29UnsortedTableMap <K,V>bucket = table[h];\n30if(bucket == null)return null;\n31intoldSize = bucket.size();\n32V answer = bucket.remove(k);\n33n−= (oldSize−bucket.size()); // size may have decreased\n34returnanswer;\n35}\n36/∗∗Returns an iterable collection of all key-value entries of the map. ∗/\n37publicIterable<Entry<K,V>>entrySet(){\n38ArrayList< Entry<K,V>>buﬀer = newArrayList <>();\n39for(inth=0; h<capacity; h++)\n40 if(table[h] != null)\n41 for(Entry<K,V>entry : table[h].entrySet())\n42 buﬀer.add(entry);\n43returnbuﬀer;\n44}\n45}\nCode Fragment 10.8: A concrete hash map implementation using separate chaining.\nwww.it-ebooks.info"
  },
  {
    "page": 444,
    "content": "426 Chapter 10. Maps, Hash Tables, and Skip Lists\nLinear Probing\nOur implementation of a ProbeHashMap class, using open addressing with linear\nprobing, is given in Code Fragments 10.9 and 10.10. In order to support deletions,\nwe use a technique described in Section 10.2.2 in which we place a special marker\nin a table location at which an entry has been deleted, so that we can distinguish\nbetween it and a location that has always been empty. To this end, we create aﬁxed entry instance, DEFUNCT , as a sentinel (disregarding any key or value stored\nwithin), and use references to that instance to mark vacated cells.\nThe most challenging aspect of open addressing is to properly trace the series\nof probes when collisions occur during a search for an existing entry, or placementof a new entry. To this end, the three primary map operations each rely on a utility,ﬁndSlot , that searches for an entry with key kin “bucket” h(that is, where his\nthe index returned by the hash function for key k). When attempting to retrieve\nthe value associated with a given key, we must continue probing until we ﬁnd thekey, or until we reach a table slot with a nullreference. We cannot stop the search\nupon reaching an DEFUNCT sentinel, because it represents a location that may\nhave been ﬁlled at the time the desired entry was once inserted.\nWhen a key-value pair is being placed in the map, we must ﬁrst attempt to ﬁnd\nan existing entry with the given key, so that we might overwrite its value. Therefore,we must search beyond any occurrences of the DEFUNCT sentinel when inserting.\nHowever, if no match is found, we prefer to repurpose the ﬁrst slot marked withDEFUNCT , if any, when placing the new element in the table. The ﬁndSlot method\nenacts this logic, continuing an unsuccessful search until ﬁnding a truly empty slot,and returning the index of the ﬁrst available slot for an insertion.\nWhen deleting an existing entry within bucketRemove , we intentionally set the\ntable entry to the DEFUNCT sentinel in accordance with our strategy.\n1public class ProbeHashMap <K,V>extends AbstractHashMap <K,V>{\n2privateMapEntry <K,V>[ ] table; // a ﬁxed array of entries (all initially null)\n3privateMapEntry <K,V>DEFUNCT = newMapEntry <>(null,null);//sentinel\n4publicProbeHashMap() {super();}\n5publicProbeHashMap( intcap){super(cap);}\n6publicProbeHashMap( intcap,intp){super(cap, p);}\n7/∗∗Creates an empty table having length equal to current capacity. ∗/\n8protected void createTable(){\n9table = (MapEntry <K,V>[ ])newMapEntry[capacity]; // safe cast\n10}\n11/∗∗Returns true if location is either empty or the ”defunct” sentinel. ∗/\n12private boolean isAvailable( intj){\n13return(table[j] == null||table[j] == DEFUNCT);\n14}\nCode Fragment 10.9: Concrete ProbeHashMap class that uses linear probing for\ncollision resolution. (Continues in Code Fragment 10.10.)\nwww.it-ebooks.info"
  },
  {
    "page": 445,
    "content": "10.2. Hash Tables 427\n15/∗∗Returns index with key k, or −(a+1) such that k could be added at index a. ∗/\n16private int ﬁndSlot(inth, K k){\n17intavail =−1; // no slot available (thus far)\n18intj = h; // index while scanning table\n19do{\n20 if(isAvailable(j)){ // may be either empty or defunct\n21 if(avail ==−1) avail = j; // this is the ﬁrst available slot!\n22 if(table[j] == null)break; // if empty, search fails immediately\n23}else if(table[j].getKey().equals(k))\n24 returnj; // successful match\n25 j = (j+1) % capacity; // keep looking (cyclically)\n26}while(j != h); // stop if we return to the start\n27return−(avail + 1); // search has failed\n28}\n29/∗∗Returns value associated with key k in bucket with hash value h, or else null. ∗/\n30protected V bucketGet( inth, K k){\n31intj = ﬁndSlot(h, k);\n32if(j<0)return null; // no match found\n33returntable[j].getValue();\n34}\n35/∗∗Associates key k with value v in bucket with hash value h; returns old value. ∗/\n36protected V bucketPut( inth, K k, V v){\n37intj = ﬁndSlot(h, k);\n38if(j>= 0) // this key has an existing entry\n39 returntable[j].setValue(v);\n40table[−(j+1)] = newMapEntry <>(k, v); // convert to proper index\n41n++;\n42return null;\n43}\n44/∗∗Removes entry having key k from bucket with hash value h (if any). ∗/\n45protected V bucketRemove( inth, K k){\n46intj = ﬁndSlot(h, k);\n47if(j<0)return null; // nothing to remove\n48V answer = table[j].getValue();\n49table[j] = DEFUNCT; // mark this slot as deactivated\n50n−−;\n51returnanswer;\n52}\n53/∗∗Returns an iterable collection of all key-value entries of the map. ∗/\n54publicIterable<Entry<K,V>>entrySet(){\n55ArrayList< Entry<K,V>>buﬀer = newArrayList <>();\n56for(inth=0; h<capacity; h++)\n57 if(!isAvailable(h)) buﬀer.add(table[h]);\n58returnbuﬀer;\n59}\n60}\nCode Fragment 10.10: Concrete ProbeHashMap class that uses linear probing for\ncollision resolution (continued from Code Fragment 10.9).\nwww.it-ebooks.info"
  },
  {
    "page": 459,
    "content": "10.4. Skip Lists 441\nRemoval in a Skip List\nLike the search and insertion algorithms, the removal algorithm for a skip list is\nquite simple. In fact, it is even easier than the insertion algorithm. To perform the\nmap operation remove( k), we will begin by executing method SkipSearch( k). If\nthe returned position pstores an entry with key different from k, we return null.\nOtherwise, we remove pand all the positions above p, which are easily accessed\nby usingabove operations to climb up the tower of this entry in Sstarting at posi-\ntion p. While removing levels of the tower, we reestablish links between the hori-\nzontal neighbors of each removed position. The removal algorithm is illustrated inFigure 10.13 and a detailed description of it is left as an exercise (R-10.24). As weshow in the next subsection, the remove operation in a skip list with nentries has\nO(logn)expected running time.\nBefore we give this analysis, however, there are some minor improvements to\nthe skip-list data structure we would like to discuss. First, we do not actually need\nto store references to values at the levels of the skip list above the bottom level,\nbecause all that is needed at these levels are references to keys. In fact, we canmore efﬁciently represent a tower as a single object, storing the key-value pair,and maintaining jprevious references and jnext references if the tower reaches\nlevel S\nj. Second, for the horizontal axes, it is possible to keep the list singly linked,\nstoring only the next references. We can perform insertions and removals in strictlya top-down, scan-forward fashion. We explore the details of this optimization inExercise C-10.55. Neither of these optimizations improve the asymptotic perfor-mance of skip lists by more than a constant factor, but these improvements can,\nnevertheless, be meaningful in practice. In fact, experimental evidence suggests\nthat optimized skip lists are faster in practice than A VL trees and other balancedsearch trees, which are discussed in Chapter 11.\n31S5\nS4\nS3\nS2\nS1-∞-∞\n-∞ 1212 -∞\n1717 25\n25 31\n3142\n55 5055+∞\n+∞+∞\n+∞\n+∞-∞-∞\n17\n38 39 424242\n44\n445555+∞\n17\n17\n20 2525\nS0\nFigure 10.13: Removal of the entry with key 25 from the skip list of Figure 10.12.\nThe positions visited after the search for the position of S0holding the entry are\nhighlighted in blue. The positions removed are drawn with dashed lines.\nwww.it-ebooks.info"
  },
  {
    "page": 460,
    "content": "442 Chapter 10. Maps, Hash Tables, and Skip Lists\nMaintaining the Topmost Level\nA skip list Smust maintain a reference to the start position (the topmost, leftmost\nposition in S) as an instance variable, and must have a policy for any insertion that\nwishes to continue growing the tower for a new entry past the top level of S. There\nare two possible courses of action we can take, both of which have their merits.\nOne possibility is to restrict the top level, h, to be kept at some ﬁxed value that\nis a function of n, the number of entries currently in the map (from the analysis we\nwill see that h=max{10,2⌈logn⌉}is a reasonable choice, and picking h=3⌈logn⌉\nis even safer). Implementing this choice means that we must modify the insertion\nalgorithm to stop inserting a new position once we reach the topmost level (unless\n⌈logn⌉<⌈log(n+1)⌉, in which case we can now go at least one more level, since\nthe bound on the height is increasing).\nThe other possibility is to let an insertion continue growing a tower as long\nas heads keep getting returned from the random number generator. This is the\napproach taken by algorithm SkipInsert of Code Fragment 10.15. As we show in\nthe analysis of skip lists, the probability that an insertion will go to a level that is\nmore than O(logn)is very low, so this design choice should also work.\nEither choice will still result in the expected O(logn)time to perform search,\ninsertion, and removal, as we will show in the next section.\n10.4.2 Probabilistic Analysis of Skip Lists ⋆\nAs we have shown above, skip lists provide a simple implementation of a sorted\nmap. In terms of worst-case performance, however, skip lists are not a superior\ndata structure. In fact, if we do not ofﬁcially prevent an insertion from continu-\ning signiﬁcantly past the current highest level, then the insertion algorithm can go\ninto what is almost an inﬁnite loop (it is not actually an inﬁnite loop, however,\nsince the probability of having a fair coin repeatedly come up heads forever is 0).\nMoreover, we cannot inﬁnitely add positions to a list without eventually running\nout of memory. In any case, if we terminate position insertion at the highest level\nh, then the worst-case running time for performing the get,put, andremove map\noperations in a skip list Swith nentries and height hisO(n+h). This worst-case\nperformance occurs when the tower of every entry reaches level h−1, where his\nthe height of S. However, this event has very low probability. Judging from this\nworst case, we might conclude that the skip-list structure is strictly inferior to the\nother map implementations discussed earlier in this chapter. But this would not be\na fair analysis, for this worst-case behavior is a gross overestimate.\n⋆We use a star ( ⋆) to indicate sections containing material more advanced than the material in the\nrest of the chapter; this material can be considered optional in a ﬁrst reading.\nwww.it-ebooks.info"
  },
  {
    "page": 461,
    "content": "10.4. Skip Lists 443\nBounding the Height of a Skip List\nBecause the insertion step involves randomization, a more accurate analysis of skip\nlists involves a bit of probability. At ﬁrst, this might seem like a major undertaking,\nfor a complete and thorough probabilistic analysis could require deep mathemat-\nics (and, indeed, there are several such deep analyses that have appeared in data\nstructures research literature). Fortunately, such an analysis is not necessary to un-\nderstand the expected asymptotic behavior of skip lists. The informal and intuitive\nprobabilistic analysis we give below uses only basic concepts of probability theory.\nLet us begin by determining the expected value of the height hof a skip list S\nwith nentries (assuming that we do not terminate insertions early). The probability\nthat a given entry has a tower of height i≥1 is equal to the probability of getting i\nconsecutive heads when ﬂipping a coin, that is, this probability is 1 /2i. Hence, the\nprobability Pithat level ihas at least one position is at most\nPi≤n\n2i,\nbecause the probability that any one of ndifferent events occurs is at most the sum\nof the probabilities that each occurs.\nThe probability that the height hofSis larger than iis equal to the probability\nthat level ihas at least one position, that is, it is no more than Pi. This means that h\nis larger than, say, 3log nwith probability at most\nP3log n≤n\n23log n\n=n\nn3=1\nn2.\nFor example, if n=1000, this probability is a one-in-a-million long shot. More\ngenerally, given a constant c>1,his larger than clognwith probability at most\n1/nc−1. That is, the probability that his smaller than clognis at least 1−1/nc−1.\nThus, with high probability, the height hofSisO(logn).\nAnalyzing Search Time in a Skip List\nNext, consider the running time of a search in skip list S, and recall that such a\nsearch involves two nested while loops. The inner loop performs a scan forward on\na level of Sas long as the next key is no greater than the search key k, and the outer\nloop drops down to the next level and repeats the scan forward iteration. Since the\nheight hofSisO(logn)with high probability, the number of drop-down steps is\nO(logn)with high probability.\nwww.it-ebooks.info"
  },
  {
    "page": 462,
    "content": "444 Chapter 10. Maps, Hash Tables, and Skip Lists\nSo we have yet to bound the number of scan-forward steps we make. Let nibe\nthe number of keys examined while scanning forward at level i. Observe that, after\nthe key at the starting position, each additional key examined in a scan-forward at\nlevel icannot also belong to level i+1. If any of these keys were on the previous\nlevel, we would have encountered them in the previous scan-forward step. Thus,\nthe probability that any key is counted in niis 1/2. Therefore, the expected value of\nniis exactly equal to the expected number of times we must ﬂip a fair coin before\nit comes up heads. This expected value is 2. Hence, the expected amount of time\nspent scanning forward at any level iisO(1). Since ShasO(logn)levels with high\nprobability, a search in Stakes expected time O(logn). By a similar analysis, we\ncan show that the expected running time of an insertion or a removal is O(logn).\nSpace Usage in a Skip List\nFinally, let us turn to the space requirement of a skip list Swith nentries. As we\nobserved above, the expected number of positions at level iisn/2i, which means\nthat the expected total number of positions in Sis\nh\n∑\ni=0n\n2i=nh\n∑\ni=01\n2i.\nUsing Proposition 4.5 on geometric summations, we have\nh\n∑\ni=01\n2i=/parenleftbig1\n2/parenrightbigh+1−1\n1\n2−1=2·/parenleftbigg\n1−1\n2h+1/parenrightbigg\n<2 for all h≥0.\nHence, the expected space requirement of SisO(n).\nTable 10.4 summarizes the performance of a sorted map realized by a skip list.\nMethod Running Time\nsize,isEmpty O(1)\nget O(logn)expected\nput O(logn)expected\nremove O(logn)expected\nﬁrstEntry ,lastEntry O(1)\nceilingEntry ,ﬂoorEntryO(logn)expectedlowerEntry ,higherEntry\nsubMap O(s+logn)expected, with sentries reported\nentrySet ,keySet ,values O(n)\nTable 10.4: Performance of a sorted map implemented with a skip list. We use nto\ndenote the number of entries in the dictionary at the time the operation is performed.\nThe expected space requirement is O(n).\nwww.it-ebooks.info"
  },
  {
    "page": 463,
    "content": "10.5. Sets, Multisets, and Multimaps 445\n10.5 Sets, Multisets, and Multimaps\nWe conclude this chapter by examining several additional abstractions that are\nclosely related to the map ADT, and that can be implemented using data structures\nsimilar to those for a map.\n•Asetis an unordered collection of elements, without duplicates, that typi-\ncally supports efﬁcient membership tests. In essence, elements of a set are\nlike keys of a map, but without any auxiliary values.\n•Amultiset (also known as a bag) is a set-like container that allows duplicates.\n•Amultimap is similar to a traditional map, in that it associates values with\nkeys; however, in a multimap the same key can be mapped to multiple values.\nFor example, the index of this book (page 714) maps a given term to one or\nmore locations at which the term occurs elsewhere in the book.\n10.5.1 The Set ADT\nThe Java Collections Framework deﬁnes the java.util.Set interface, which includes\nthe following fundamental methods:\nadd( e):Adds the element etoS(if not already present).\nremove( e):Removes the element efrom S(if it is present).\ncontains( e):Returns whether eis an element of S.\niterator() :Returns an iterator of the elements of S.\nThere is also support for the traditional mathematical set operations of union ,\nintersection , and subtraction of two sets SandT:\nS∪T={e:eis in Soreis in T},\nS∩T={e:eis in Sandeis in T},\nS−T={e:eis in Sandeis not in T}.\nIn thejava.util.Set interface, these operations are provided through the following\nmethods, if executed on a set S:\naddAll( T):Updates Sto also include all elements of set T, effec-\ntively replacing SbyS∪T.\nretainAll( T):Updates Sso that it only keeps those elements that are\nalso elements of set T, effectively replacing SbyS∩T.\nremoveAll( T):Updates Sby removing any of its elements that also occur\nin set T, effectively replacing SbyS−T.\nwww.it-ebooks.info"
  },
  {
    "page": 464,
    "content": "446 Chapter 10. Maps, Hash Tables, and Skip Lists\nThetemplate method pattern can be applied to implement each of the methods\naddAll ,retainAll , andremoveAll using only calls to the more fundamental methods\nadd,remove ,contains , anditerator. In fact, the java.util.AbstractSet class pro-\nvides such implementations. To demonstrate the technique, we could implement\ntheaddAll method in the context of a set class as follows:\npublic void addAll(Set <E>other){\nfor(E element : other) // rely on iterator( ) method of other\nadd(element); // duplicates will be ignored by add\n}\nTheremoveAll andretailAll methods can be implemented with similar techniques,\nalthough a bit more care is needed for retainAll, to avoid removing elements while\niterating over the same set (see Exercise C-10.59). The efﬁciency of these methods\nfor a concrete set implementation will depend on the underlying efﬁciency of thefundamental methods upon which they rely.\nSorted Sets\nFor the standard set abstraction, there is no explicit notion of keys being ordered;all that is assumed is that the equals method can detect equivalent elements.\nIf, however, elements come from a Comparable class (or a suitable Comparator\nobject is provided), we can extend the notion of a set to deﬁne the sorted set ADT ,\nincluding the following additional methods:\nﬁrst() :Returns the smallest element in S.\nlast() :Returns the largest element in S.\nceiling( e):Returns the smallest element greater than or equal to e.\nﬂoor( e):Returns the largest element less than or equal to e.\nlower( e):Returns the largest element strictly less than e.\nhigher( e):Returns the smallest element strictly greater than e.\nsubSet( e\n1,e2):Returns an iteration of all elements greater than or equaltoe\n1, but strictly less than e2.\npollFirst() :Returns and removes the smallest element in S.\npollLast() :Returns and removes the largest element in S.\nIn the Java Collection Framework, the above methods are included in a combi-\nnation of the java.util.SortedSet andjava.util.NavigableSet interfaces.\nwww.it-ebooks.info"
  },
  {
    "page": 465,
    "content": "10.5. Sets, Multisets, and Multimaps 447\nImplementing Sets\nAlthough a set is a completely different abstraction than a map, the techniques used\nto implement the two can be quite similar. In effect, a set is simply a map in which\n(unique) keys do not have associated values.\nTherefore, any data structure used to implement a map can be modiﬁed to im-\nplement the set ADT with similar performance guarantees. As a trivial adaption\nof a map, each set element can be stored as a key, and the nullreference can be\nstored as an (irrelevant) value. Of course, such an implementation is unnecessarily\nwasteful; a more efﬁcient set implementation should abandon the Entry composite\nand store set elements directly in a data structure.\nThe Java Collections Framework includes the following set implementations,\nmirroring similar data structures used for maps:\n•java.util.HashSet provides an implementation of the (unordered) set ADT\nwith a hash table.\n•java.util.concurrent.ConcurrentSkipListSet provides an implementation of\nthe sorted set ADT using a skip list.\n•java.util.TreeSet provides an implementation of the sorted set ADT using a\nbalanced search tree. (Search trees are the focus of Chapter 11.)\n10.5.2 The Multiset ADT\nBefore discussing models for a multiset abstraction, we must carefully consider the\nnotion of “duplicate” elements. Throughout the Java Collections Framework, ob-\njects are considered equivalent to each other based on the standard equals method\n(see Section 3.5). For example, keys of a map must be unique, but the notion of\nuniqueness allows distinct yet equivalent objects to be matched. This is impor-\ntant for many typical uses of maps. For example, when strings are used as keys,\nthe instance of the string \"October\" that is used when inserting an entry may not\nbe the same instance of \"October\" that is used when later retrieving the associ-\nated value. The call birthstones.get( \"October\" )will succeed in such a scenario\nbecause strings are considered equal to each other.\nIn the context of multisets, if we represent a collection that appears through\nthe notion of equivalence as {a,a,a,a,b,c,c}, we must decide if we want a data\nstructure to explicitly maintain each instance of a(because each might be distinct\nthough equivalent), or just that there exist four occurrences. In either case, a mul-\ntiset can be implemented by directly adapting a map. We can use one element\nfrom a group of equivalent occurrences as the key in a map, with the associated\nvalue either a secondary container containing all of the equivalent instances, or a\ncount of the number of occurrences. Note that our word-frequency application in\nSection 10.1.2 uses just such a map, associating strings with counts.\nwww.it-ebooks.info"
  },
  {
    "page": 466,
    "content": "448 Chapter 10. Maps, Hash Tables, and Skip Lists\nThe Java Collections Framework does not include any form of a multiset. How-\never, implementations exist in several widely used, open source Java collections\nlibraries. The Apache Commons deﬁnes Bag andSortedBag interfaces that cor-\nrespond respectively to unsorted and sorted multisets. The Google Core Libraries\nfor Java (named Guava ) includes Multiset andSortedMultiset interfaces for these\nabstractions. Both of those libraries take the approach of modeling a multiset as\na collection of elements having multiplicities, and both offer several concrete im-\nplementations using standard data structures. In formalizing the abstract data type,\ntheMultiset interface of the Guava library includes the following behaviors (and\nmore):\nadd( e):Adds a single occurrences of eto the multiset.\ncontains( e):Returns true if the multiset contains an element equal to e.\ncount( e):Returns the number of occurrences of ein the multiset.\nremove( e):Removes a single occurrence of efrom the multiset.\nremove( e,n):Removes noccurrences of efrom the multiset.\nsize() :Returns the number of elements of the multiset (including\nduplicates).\niterator() :Returns an iteration of all elements of the multiset\n(repeating those with multiplicity greater than one).\nThe multiset ADT also includes the notion of an immutable Entry that repre-\nsents an element and its count, and the SortedMultiset interface includes additional\nmethods such as ﬁrstEntry andlastEntry .\n10.5.3 The Multimap ADT\nLike a map, a multimap stores entries that are key-value pairs (k,v), where kis\nthe key and vis the value. Whereas a map insists that entries have unique keys,\na multimap allows multiple entries to have the same key, much like an English\ndictionary, which allows multiple deﬁnitions for the same word. That is, we will\nallow a multimap to contain entries (k,v)and(k,v′)having the same key.\nThere are two standard approaches for representing a multimap as a variation of\na traditional map. One is to redesign the underlying data structure to allow separate\nentries to be stored for pairs such as (k,v)and(k,v′). The other is to map key kto\na secondary container of all values associated with that key (e.g., {v,v′}).\nMuch as it is missing a formal abstraction for a multiset, the Java Collections\nFramework does not include any multiset interface nor implementations. However,\nas we will soon demonstrate, it is easy to represent a multiset by adapting other\ncollection classes that are included in the java.util package.\nwww.it-ebooks.info"
  },
  {
    "page": 467,
    "content": "10.5. Sets, Multisets, and Multimaps 449\nTo formalize the multimap abstract data type, we consider a simpliﬁed version\nof theMultimap interface included in Google’s Guava library. Among its methods\nare the following:\nget(k):Returns a collection of all values associated with key kin the\nmultimap.\nput(k,v):Adds a new entry to the multimap associating key kwith\nvalue v, without overwriting any existing mappings for key k.\nremove( k,v):Removes an entry mapping key kto value vfrom the multimap\n(if one exists).\nremoveAll( k):Removes all entries having key equal to kfrom the multimap.\nsize() :Returns the number of entries of the multiset\n(including multiple associations).\nentries() :Returns a collection of all entries in the multimap.\nkeys() :Returns a collection of keys for all entries in the multimap\n(including duplicates for keys with multiple bindings).\nkeySet() :Returns a nonduplicative collection of keys in the multimap.\nvalues() :Returns a collection of values for all entries in the multimap.\nIn Code Fragments 10.16 and 10.17, we provide an implementation of a class,\nHashMultimap , that uses a java.util.HashMap to map each key to a secondary\nArrayList of all values that are associated with the key. For brevity, we omit the\nformality of deﬁning a Multimap interface, and we provide the entries() method\nas the only form of iteration.\n1public class HashMultimap <K,V>{\n2Map<K,List<V>>map =newHashMap <>();// the primary map\n3inttotal = 0; // total number of entries in the multimap\n4/∗∗Constructs an empty multimap. ∗/\n5publicHashMultimap(){}\n6/∗∗Returns the total number of entries in the multimap. ∗/\n7public int size(){returntotal;}\n8/∗∗Returns whether the multimap is empty. ∗/\n9public boolean isEmpty(){return(total == 0);}\n10/∗∗Returns a (possibly empty) iteration of all values associated with the key. ∗/\n11Iterable<V>get(K key){\n12List<V>secondary = map.get(key);\n13if(secondary != null)\n14 returnsecondary;\n15return new ArrayList<> (); // return an empty list of values\n16}\nCode Fragment 10.16: An implementation of a multimap as an adaptation of classes\nfrom thejava.util package. (Continues in Code Fragment 10.17.)\nwww.it-ebooks.info"
  },
  {
    "page": 468,
    "content": "450 Chapter 10. Maps, Hash Tables, and Skip Lists\n17/∗∗Adds a new entry associating key with value. ∗/\n18voidput(K key, V value) {\n19List<V>secondary = map.get(key);\n20if(secondary == null){\n21 secondary = newArrayList <>();\n22 map.put(key, secondary); // begin using new list as secondary structure\n23}\n24secondary.add(value);\n25total++;\n26}\n27/∗∗Removes the (key,value) entry, if it exists. ∗/\n28boolean remove(K key, V value) {\n29boolean wasRemoved = false;\n30List<V>secondary = map.get(key);\n31if(secondary != null){\n32 wasRemoved = secondary.remove(value);\n33 if(wasRemoved){\n34 total−−;\n35 if(secondary.isEmpty())\n36 map.remove(key); // remove secondary structure from primary map\n37}\n38}\n39returnwasRemoved;\n40}\n41/∗∗Removes all entries with the given key. ∗/\n42Iterable<V>removeAll(K key) {\n43List<V>secondary = map.get(key);\n44if(secondary != null){\n45 total−= secondary.size();\n46 map.remove(key);\n47}else\n48 secondary = newArrayList <>();// return empty list of removed values\n49returnsecondary;\n50}\n51/∗∗Returns an iteration of all entries in the multimap. ∗/\n52Iterable<Map.Entry <K,V>>entries(){\n53List<Map.Entry <K,V>>result =newArrayList <>();\n54for(Map.Entry <K,List<V>>secondary : map.entrySet()) {\n55 K key = secondary.getKey();\n56 for(V value : secondary.getValue())\n57 result.add( newAbstractMap.SimpleEntry <K,V>(key,value));\n58}\n59returnresult;\n60}\n61}\nCode Fragment 10.17: An implementation of a multimap as an adaptation of classes\nfrom thejava.util package. (Continued from Code Fragment 10.16.)\nwww.it-ebooks.info"
  },
  {
    "page": 469,
    "content": "10.6. Exercises 451\n10.6 Exercises\nReinforcement\nR-10.1 What is the worst-case running time for inserting nkey-value pairs into an ini-\ntially empty map Mthat is implemented with the UnsortedTableMap class?\nR-10.2 Reimplement the UnsortedTableMap class using the PositionalList class from\nSection 7.3 rather than an ArrayList .\nR-10.3 The use of nullvalues in a map is problematic, as there is then no way to dif-\nferentiate whether a nullvalue returned by the call get(k)represents the legit-\nimate value of an entry (k,null), or designates that key kwas not found. The\njava.util.Map interface includes a boolean method, containsKey( k), that resolves\nany such ambiguity. Implement such a method for the UnsortedTableMap class.\nR-10.4 Which of the hash table collision-handling schemes could tolerate a load factor\nabove 1 and which could not?\nR-10.5 What would be a good hash code for a vehicle identiﬁcation number that is a\nstring of numbers and letters of the form “9X9XX99X9XX999999,” where a “9”\nrepresents a digit and an “X” represents a letter?\nR-10.6 Draw the 11-entry hash table that results from using the hash function, h(i) =\n(3i+5)mod 11, to hash the keys 12, 44, 13, 88, 23, 94, 11, 39, 20, 16, and 5,\nassuming collisions are handled by chaining.\nR-10.7 What is the result of the previous exercise, assuming collisions are handled by\nlinear probing?\nR-10.8 Show the result of Exercise R-10.6, assuming collisions are handled by quadratic\nprobing, up to the point where the method fails.\nR-10.9 What is the result of Exercise R-10.6 when collisions are handled by double\nhashing using the secondary hash function h′(k)=7−(kmod 7)?\nR-10.10 What is the worst-case time for putting nentries in an initially empty hash table,\nwith collisions resolved by chaining? What is the best case?\nR-10.11 Show the result of rehashing the hash table shown in Figure 10.6 into a table of\nsize 19 using the new hash function h(k)=3kmod 17.\nR-10.12 Modify the Pair class from Code Fragment 2.17 on page 92 so that it provides a\nnatural deﬁnition for both the equals() andhashCode() methods.\nR-10.13 Consider lines 31–33 of Code Fragment 10.8 in our implementation of the class\nChainHashMap . We use the difference in the size of a secondary bucket before\nand after a call to bucket.remove(k) to update the variable n. If we replace those\nthree lines with the following, does the class behave properly? Explain.\nV answer = bucket.remove(k);\nif(answer != null) // value of removed entry\nn−−; // size has decreased\nwww.it-ebooks.info"
  },
  {
    "page": 470,
    "content": "452 Chapter 10. Maps, Hash Tables, and Skip Lists\nR-10.14 OurAbstractHashMap class maintains a load factor λ≤0.5. Reimplement that\nclass to allow the user to specify the maximum load, and adjust the concrete\nsubclasses accordingly.\nR-10.15 Give a pseudocode description of an insertion into a hash table that uses quadraticprobing to resolve collisions, assuming we also use the trick of replacing deleted\nentries with a special “available” object.\nR-10.16 Modify our ProbeHashMap to use quadratic probing.\nR-10.17 Explain why a hash table is not suited to implement a sorted map.\nR-10.18 What is the worst-case asymptotic running time for performing ndeletions from\naSortedTableMap instance that initially contains 2 nentries?\nR-10.19 Implement the containKey( k)method, as described in Exercise R-10.3, for the\nSortedTableClass .\nR-10.20 Describe how a sorted list implemented as a doubly linked list could be used to\nimplement the sorted map ADT.\nR-10.21 Consider the following variant of the ﬁndIndex method of the SortedTableMap\nclass, originally given in Code Fragment 10.11:\n1private int ﬁndIndex(K key, intlow,inthigh){\n2if(high<low)returnhigh + 1;\n3intmid = (low + high) / 2;\n4if(compare(key, table.get(mid)) <0)\n5 returnﬁndIndex(key, low, mid −1);\n6else\n7 returnﬁndIndex(key, mid + 1, high);\n8}\nDoes this always produce the same result as the original version? Justify youranswer.\nR-10.22 What is the expected running time of the methods for maintaining a maxima\nset if we insert npairs such that each pair has lower cost and performance than\none before it? What is contained in the sorted map at the end of this series of\noperations? What if each pair had a lower cost and higher performance than the\none before it?\nR-10.23 Draw the result after performing the following series of operations on the skip list\nshown in Figure 10.13: remove( 38),put(48,x),put(24,y),remove( 55). Use an\nactual coin ﬂip to generate random bits as needed (and report your sequence of\nﬂips).\nR-10.24 Give a pseudocode description of the remove map operation for a skip list.\nR-10.25 Give a description, in pseudocode, for implementing the removeAll method for\nthe set ADT, using only the other fundamental methods of the set.\nR-10.26 Give a description, in pseudocode, for implementing the retainAll method for the\nset ADT, using only the other fundamental methods of the set.\nwww.it-ebooks.info"
  },
  {
    "page": 471,
    "content": "10.6. Exercises 453\nR-10.27 If we let ndenote the size of set S, and mdenote the size of set T, what would\nbe the running time of the operation S.addAll( T), as implemented on page 446,\nif both sets were implemented as skip lists?\nR-10.28 If we let ndenote the size of set S, and mdenote the size of set T, what would\nbe the running time of the operation S.addAll( T), as implemented on page 446,\nif both sets were implemented using hashing?\nR-10.29 If we let ndenote the size of set S, and mdenote the size of set T, what would\nbe the running time of the operation S.removeAll( T)when both sets are imple-\nmented using hashing?\nR-10.30 If we let ndenote the size of set S, and mdenote the size of set T, what would be\nthe running time of the operation S.retainAll( T)when both sets are implemented\nusing hashing?\nR-10.31 What abstraction would you use to manage a database of friends’ birthdays in or-\nder to support efﬁcient queries such as “ﬁnd all friends whose birthday is today”\nand “ﬁnd the friend who will be the next to celebrate a birthday”?\nCreativity\nC-10.32 For an ideal compression function, the capacity of the bucket array for a hash\ntable should be a prime number. Therefore, we consider the problem of locating\na prime number in a range [M,2M]. Implement a method for ﬁnding such a prime\nby using the sieve algorithm . In this algorithm, we allocate a 2 Mcell boolean\narray A, such that cell iis associated with the integer i. We then initialize the array\ncells to all be “true” and we “mark off” all the cells that are multiples of 2, 3, 5,\n7, and so on. This process can stop after it reaches a number larger than√\n2M.\n(Hint: Consider a bootstrapping method for ﬁnding the primes up to√\n2M.)\nC-10.33 Consider the goal of adding entry (k,v)to a map only if there does not yet exist\nsome other entry with key k. For a map M(withoutnullvalues), this might be\naccomplished as follows.\nif(M.get(k) ==null)\nM.put(k,v);\nWhile this accomplishes the goal, its efﬁciency is less than ideal, as time will\nbe spent on the failed search during the getcall, and again during the putcall\n(which always begins by trying to locate an existing entry with the given key).\nTo avoid this inefﬁciency, some map implementations support a custom method\nputIfAbsent( k,v)that accomplishes this goal. Given such an implementation of\nputIfAbsent for theUnsortedTableMap class.\nC-10.34 Repeat Exercise C-10.33 for the ChainHashMap class.\nC-10.35 Repeat Exercise C-10.33 for the ProbeHashMap class.\nC-10.36 Describe how to redesign the AbstractHashMap framework to include support\nfor a method, containsKey , as described in Exercise R-10.3.\nwww.it-ebooks.info"
  },
  {
    "page": 472,
    "content": "454 Chapter 10. Maps, Hash Tables, and Skip Lists\nC-10.37 Modify the ChainHashMap class in accordance with your design for the previous\nexercise.\nC-10.38 Modify the ProbeHashMap class in accordance with Exercise C-10.36.\nC-10.39 Redesign the AbstractHashMap class so that it halves the capacity of the table if\nthe load factor falls below 0 .25. Your solution must not involve any changes to\nthe concrete ProbeHashMap andChainHashMap classes.\nC-10.40 Thejava.util.HashMap class uses separate chaining, but without any explicit sec-\nondary structures. The table is an array of entries, and each entry has an addi-\ntionalnext ﬁeld that can reference another entry in that bucket. In this way,\nthe entry instances can be threaded as a singly linked list. Reimplement ourChainHashMap class using such an approach.\nC-10.41 Describe how to perform a removal from a hash table that uses linear probing\nto resolve collisions where we do not use a special marker to represent deleted\nelements. That is, we must rearrange the contents so that it appears that theremoved entry was never inserted in the ﬁrst place.\nC-10.42 The quadratic probing strategy has a clustering problem related to the way it\nlooks for open slots. Namely, when a collision occurs at bucket h(k), it checks\nbuckets A[(h(k)+i\n2)mod N], for i=1,2,..., N−1.\na.Show that i2mod Nwill assume at most (N+1)/2 distinct values, for N\nprime, as iranges from 1 to N−1. As a part of this justiﬁcation, note that\ni2mod N=(N−i)2mod Nfor all i.\nb.A better strategy is to choose a prime Nsuch that Nmod 4=3 and then\nto check the buckets A[(h(k)±i2)mod N]asiranges from 1 to (N−1)/2,\nalternating between plus and minus. Show that this alternate version isguaranteed to check every bucket in A.\nC-10.43 Redesign our ProbeHashMap class so that the sequence of secondary probes\nfor collision resolution can be more easily customized. Demonstrate your newdesign by providing separate concrete subclasses for linear probing and quadratic\nprobing.\nC-10.44 Thejava.util.LinkedHashMap class is a subclass of the standard HashMap class\nthat retains the expected O(1)performance for the primary map operations while\nguaranteeing that iterations report entries of the map according to ﬁrst-in, ﬁrst-out (FIFO) principle. That is, the key that has been in the map the longest is\nreported ﬁrst. (The order is unaffected when the value for an existing key is\nchanged.) Describe an algorithmic approach for achieving such performance.\nC-10.45 Develop a location-aware version of the UnsortedTableMap class so that an op-\nerationremove( e)for existing Entry ecan be implemented in O(1)time.\nC-10.46 Repeat the previous exercise for the ProbeHashMap class.\nC-10.47 Repeat Exercise C-10.45 for the ChainHashMap class.\nwww.it-ebooks.info"
  },
  {
    "page": 473,
    "content": "10.6. Exercises 455\nC-10.48 Although keys in a map are distinct, the binary search algorithm can be applied\nin a more general setting in which an array stores possibly duplicative elements\nin nondecreasing order. Consider the goal of identifying the index of the leftmost\nelement with key greater than or equal to given k. Does the ﬁndIndex method\nas given in Code Fragment 10.11 guarantee such a result? Does the ﬁndIndex\nmethod as given in Exercise R-10.21 guarantee such a result? Justify your an-\nswers.\nC-10.49 Suppose we are given two sorted search tables SandT, each with nentries (with\nSandTbeing implemented with arrays). Describe an O(log2n)-time algorithm\nfor ﬁnding the kthsmallest key in the union of the keys from SandT(assuming\nno duplicates).\nC-10.50 Give an O(logn)-time solution for the previous problem.\nC-10.51 Give an alternative implementation of the SortedTableMap ’sentrySet method\nthat creates a lazy iterator rather than a snapshot. (See Section 7.4.2 for discus-\nsion of iterators.)\nC-10.52 Repeat the previous exercise for the ChainHashMap class.\nC-10.53 Repeat Exercise C-10.51 for the ProbeHashMap class.\nC-10.54 Given a database Dofncost-performance pairs (c,p), describe an algorithm for\nﬁnding the maxima pairs of CinO(nlogn)time.\nC-10.55 Show that the methods above(p)andbefore(p)are not actually needed to efﬁ-\nciently implement a map using a skip list. That is, we can implement insertions\nand deletions in a skip list using a strictly top-down, scan-forward approach,\nwithout ever using the above orbefore methods. (Hint: In the insertion algo-\nrithm, ﬁrst repeatedly ﬂip the coin to determine the level where you should start\ninserting the new entry.)\nC-10.56 Describe how to modify the skip-list data structure to support the method me-\ndian() , which returns the position of the element in the “bottom” list S0at index\n⌊n/2⌋, Show that your implementation of this method runs in O(logn)expected\ntime.\nC-10.57 Describe how to modify a skip-list representation so that index-based operations,such as retrieving the entry at index j, can be performed in O(logn)expected\ntime.\nC-10.58 Suppose that each row of an n×narray Aconsists of 1’s and 0’s such that, in\nany row of A, all the 1’s come before any 0’s in that row. Assuming Ais already\nin memory, describe a method running in O(nlogn)time (not O(n\n2)time) for\ncounting the number of 1’s in A.\nC-10.59 Give a concrete implementation of the retainAll method for the set ADT, using\nonly the other fundamental methods of the set. You are to assume that the under-\nlying set implementation uses fail-fast iterators (see Section 7.4.2).\nwww.it-ebooks.info"
  },
  {
    "page": 474,
    "content": "456 Chapter 10. Maps, Hash Tables, and Skip Lists\nC-10.60 Consider sets whose elements are integers in the range [0,N−1]. A popular\nscheme for representing a set Aof this type is by means of a boolean array, B,\nwhere we say that xis in Aif and only if B[x] =true. Since each cell of B\ncan be represented with a single bit, Bis sometimes referred to as a bit vector .\nDescribe and analyze efﬁcient algorithms for performing the methods of the set\nADT assuming this representation.\nC-10.61 Aninverted ﬁle is a critical data structure for implementing applications such an\nindex of a book or a search engine. Given a document D, which can be viewed as\nan unordered, numbered list of words, an inverted ﬁle is an ordered list of words,\nL, such that, for each word winL, we store the indices of the places in Dwhere\nwappears. Design an efﬁcient algorithm for constructing Lfrom D.\nC-10.62 The operation get(k)for our multimap ADT is responsible for returning a col-\nlection of allvalues currently associated with key k. Design a variation of binary\nsearch for performing this operation on a sorted search table that includes du-\nplicates, and show that it runs in time O(s+logn), where nis the number of\nelements in the dictionary and sis the number of entries with given key k.\nC-10.63 Describe an efﬁcient multimap structure for storing nentries that have an asso-\nciated set of r<nkeys that come from a total order. That is, the set of keys\nis smaller than the number of entries. Your structure should perform operation\ngetAll inO(logr+s)expected time, where sis the number of entries returned,\noperation entrySet()inO(n)time, and the remaining operations of the multimap\nADT in O(logr)expected time.\nC-10.64 Describe an efﬁcient multimap structure for storing nentries whose r<nkeys\nhave distinct hash codes. Your structure should perform operation getAll in\nO(1+s)expected time, where sis the number of entries returned, operation\nentrySet()inO(n)time, and the remaining operations of the multimap ADT in\nO(1)expected time.\nProjects\nP-10.65 An interesting strategy for hashing with open addressing is known as cuckoo\nhashing . Two independent hash functions are computed for each key, and an\nelement is always stored in one of the two cells indicated by those hash functions.\nWhen a new element is inserted, if either of those two cells is available, it is\nplaced there. Otherwise, it is placed into one of its choice of locations, evicting\nanother entry. The evicted entry is then placed in its alternate choice of cells,\npotentially evicting yet another entry. This continues until an open cell is found,\nor an inﬁnite loop is detected (in which case, two new hash functions are chosen\nand all entries are deleted and reinserted). It can be shown that as long as the\nload factor of the table remains below 0 .5, then an insertion succeeds in expected\nconstant time. Notice that a search can be performed in worst-case constant time,\nbecause it can only be stored in one of two possible locations. Give a complete\nmap implementation based on this strategy.\nwww.it-ebooks.info"
  },
  {
    "page": 475,
    "content": "10.6. Exercises 457\nP-10.66 An interesting strategy for hashing with separate chaining is known as power-\nof-two-choices hashing . Two independent hash functions are computed for each\nkey, and a newly inserted element is placed into the choice of the two indicated\nbuckets that currently has the fewest entries. Give a complete map implementa-\ntion based on this strategy.\nP-10.67 Implement a LinkedHashMap class, as described in Exercise C-10.44, ensuring\nthat the primary map operations run in O(1)expected time.\nP-10.68 Perform experiments on our ChainHashMap andProbeHashMap classes to mea-\nsure its efﬁciency using random key sets and varying limits on the load factor (see\nExercise R-10.14).\nP-10.69 Perform a comparative analysis that studies the collision rates for various hash\ncodes for character strings, such as polynomial hash codes for different values\nof the parameter a. Use a hash table to determine collisions, but only count\ncollisions where different strings map to the same hash code (not if they map tothe same location in this hash table). Test these hash codes on text ﬁles found onthe Internet.\nP-10.70 Perform a comparative analysis as in the previous exercise, but for 10-digit tele-phone numbers instead of character strings.\nP-10.71 Design a Java class that implements the skip-list data structure. Use this class tocreate a complete implementation of the sorted map ADT.\nP-10.72 Extend the previous project by providing a graphical animation of the skip-list\noperations. Visualize how entries move up the skip list during insertions and are\nlinked out of the skip list during removals. Also, in a search operation, visualize\nthe scan-forward and drop-down actions.\nP-10.73 Describe how to use a skip list to implement the array list ADT, so that index-\nbased insertions and removals both run in O(logn)expected time.\nP-10.74 Write a spell-checker class that stores a lexicon of words, W, in a set, and im-\nplements a method, check( s), which performs a spell check on the string swith\nrespect to the set of words, W. If sis in W, then the call to check( s)returns\na list containing only s, as it is assumed to be spelled correctly in this case. If\nsis not in W, then the call to check( s)returns a list of every word in Wthat\nmight be a correct spelling of s. Your program should be able to handle all the\ncommon ways that smight be a misspelling of a word in W, including swapping\nadjacent characters in a word, inserting a single character in between two adja-cent characters in a word, deleting a single character from a word, and replacinga character in a word with another character. For an extra challenge, consider\nphonetic substitutions as well.\nwww.it-ebooks.info"
  },
  {
    "page": 476,
    "content": "458 Chapter 10. Maps, Hash Tables, and Skip Lists\nChapter Notes\nHashing is a well-studied technique. The reader interested in further study is encouraged to\nexplore the book by Knuth [61], as well as the book by Vitter and Chen [92]. The denial-\nof-service vulnerability exploiting the worst-case performance of hash tables was ﬁrst de-\nscribed by Crosby and Wallach [27], and later demonstrated by Klink and W¨ alde [58]. The\nremedy adopted by the OpenJDK team for Java is described in [76].\nSkip lists were introduced by Pugh [80]. Our analysis of skip lists is a simpliﬁcation\nof a presentation given by Motwani and Raghavan [75]. For a more in-depth analysis of\nskip lists, please see the various research papers on skip lists that have appeared in the data\nstructures literature [56, 77, 78]. Exercise C-10.42 was contributed by James Lee.\nwww.it-ebooks.info"
  },
  {
    "page": 477,
    "content": "Chapter\n11Search Trees\nContents\n11.1 Binary Search Trees . . . . . . . . . . . . . . . . . . . . . . 460\n11.1.1 Searching Within a Binary Search Tree . . . . . . . . . . . 461\n11.1.2 Insertions and Deletions . . . . . . . . . . . . . . . . . . . 463\n11.1.3 Java Implementation . . . . . . . . . . . . . . . . . . . . 466\n11.1.4 Performance of a Binary Search Tree . . . . . . . . . . . . 470\n11.2 Balanced Search Trees . . . . . . . . . . . . . . . . . . . . 472\n11.2.1 Java Framework for Balancing Search Trees . . . . . . . . 475\n11.3 AVL Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 479\n11.3.1 Update Operations . . . . . . . . . . . . . . . . . . . . . 481\n11.3.2 Java Implementation . . . . . . . . . . . . . . . . . . . . 486\n11.4 Splay Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 488\n11.4.1 Splaying . . . . . . . . . . . . . . . . . . . . . . . . . . . 488\n11.4.2 When to Splay . . . . . . . . . . . . . . . . . . . . . . . . 492\n11.4.3 Java Implementation . . . . . . . . . . . . . . . . . . . . 494\n11.4.4 Amortized Analysis of Splaying ⋆. . . . . . . . . . . . . 495\n11.5 (2,4) Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 500\n11.5.1 Multiway Search Trees . . . . . . . . . . . . . . . . . . . 500\n11.5.2 (2,4)-Tree Operations . . . . . . . . . . . . . . . . . . . . 503\n11.6 Red-Black Trees . . . . . . . . . . . . . . . . . . . . . . . . 510\n11.6.1 Red-Black Tree Operations . . . . . . . . . . . . . . . . . 512\n11.6.2 Java Implementation . . . . . . . . . . . . . . . . . . . . 522\n11.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525\nwww.it-ebooks.info"
  },
  {
    "page": 478,
    "content": "460 Chapter 11. Search Trees\n11.1 Binary Search Trees\nIn Chapter 8 we introduced the tree data structure and demonstrated a variety of\napplications. One important use is as a search tree (as described on page 338). In\nthis chapter, we use a search-tree structure to efﬁciently implement a sorted map .\nThe three most fundamental methods of of a map (see Section 10.1.1) are:\nget(k):Returns the value vassociated with key k, if such an entry exists;\notherwise returns null.\nput(k,v):Associates value vwith key k, replacing and returning any existing\nvalue if the map already contains an entry with key equal to k.\nremove( k):Removes the entry with key equal to k, if one exists, and returns its\nvalue; otherwise returns null.\nThe sorted map ADT includes additional functionality (see Section 10.3), guar-\nanteeing that an iteration reports keys in sorted order, and supporting additional\nsearches such as higherEntry( k)andsubMap( k1,k2).\nBinary trees are an excellent data structure for storing entries of a map, assum-\ning we have an order relation deﬁned on the keys. In this chapter, we deﬁne a\nbinary search tree as aproper binary tree (see Section 8.2) such that each internal\nposition pstores a key-value pair (k,v)such that:\n•Keys stored in the left subtree of pare less than k.\n•Keys stored in the right subtree of pare greater than k.\nAn example of such a binary search tree is given in Figure 11.1. Notice that the\nleaves of the tree serve only as “placeholders.” Their use as sentinels simpliﬁes the\npresentation of several of our search and update algorithms. With care, they can be\nrepresented as nullreferences in practice, thereby reducing the number of nodes in\nhalf (since there are more leaves than internal nodes in a proper binary tree).\n17 88\n65\n54\n29844\n76\n8097\n93\n2132\n28 82\nFigure 11.1: A binary search tree with integer keys. We omit the display of associ-\nated values in this chapter, since they are not relevant to the order of entries within\na search tree.\nwww.it-ebooks.info"
  },
  {
    "page": 479,
    "content": "11.1. Binary Search Trees 461\n11.1.1 Searching Within a Binary Search Tree\nThe most important consequence of the structural property of a binary search tree\nis its namesake search algorithm. We can attempt to locate a particular key in a\nbinary search tree by viewing it as a decision tree (recall Figure 8.5). In this case,\nthe question asked at each internal position pis whether the desired key kis less\nthan, equal to, or greater than the key stored at position p, which we denote as\nkey(p). If the answer is “less than,” then the search continues in the left subtree.\nIf the answer is “equal,” then the search terminates successfully. If the answer is\n“greater than,” then the search continues in the right subtree. Finally, if we reach a\nleaf, then the search terminates unsuccessfully. (See Figure 11.2.)\n28\n21 298288\n65\n5444\n3217\n8\n9397\n76\n8028\n29\n808288\n65\n5444\n3217\n8\n9397\n76 21\n(a) (b)\nFigure 11.2: (a) A successful search for key 65 in a binary search tree; (b) an\nunsuccessful search for key 68 that terminates at the leaf to the left of the key 76.\nWe describe this approach in Code Fragment 11.1. If key koccurs in a subtree\nrooted at p, a call to TreeSearch( p,k)results in the position at which the key is\nfound. For an unsuccessful search, the TreeSearch algorithm returns the ﬁnal leaf\nexplored on the search path (which we will later make use of when determining\nwhere to insert a new entry in a search tree).\nAlgorithm TreeSearch( p,k):\nifpis external then\nreturn p {unsuccessful search }\nelse if k==key(p)then\nreturn p {successful search}\nelse if k<key(p)then\nreturnTreeSearch (left(p),k) {recur on left subtree }\nelse{we know that k>key(p)}\nreturnTreeSearch (right (p),k) {recur on right subtree }\nCode Fragment 11.1: Recursive search in a binary search tree.\nwww.it-ebooks.info"
  },
  {
    "page": 480,
    "content": "462 Chapter 11. Search Trees\nAnalysis of Binary Tree Searching\nThe analysis of the worst-case running time of searching in a binary search tree\nTis simple. Algorithm TreeSearch is recursive and executes a constant number\nof primitive operations for each recursive call. Each recursive call of TreeSearch\nis made on a child of the previous position. That is, TreeSearch is called on the\npositions of a path of Tthat starts at the root and goes down one level at a time.\nThus, the number of such positions is bounded by h+1, where his the height of T.\nIn other words, since we spend O(1)time per position encountered in the search,\nthe overall search runs in O(h)time, where his the height of the binary search\ntreeT. (See Figure 11.3.)\nTree T:Time per level\nTotal time:Height\nh\nO(h)O(1)\nO(1)\nO(1)\nFigure 11.3: Illustrating the running time of searching in a binary search tree. The\nﬁgure uses a standard visualization shortcut of a binary search tree as a big triangleand a path from the root as a zig-zag line.\nIn the context of the sorted map ADT, the search will be used as a subroutine\nfor implementing the getmethod, as well as for the putandremove methods, since\neach of these begins by trying to locate an existing entry with the given key. We\nwill later demonstrate how to implement sorted map operations, such as lowerEntry\nandhigherEntry , by navigating within the tree after performing a standard search.\nAll of these operations will run in worst-case O(h)time for a tree with height h.\nAdmittedly, the height hofTcan be as large as the number of entries, n, but we\nexpect that it is usually much smaller. Later in this chapter we will show various\nstrategies to maintain an upper bound of O(logn)on the height of a search tree T.\nwww.it-ebooks.info"
  },
  {
    "page": 481,
    "content": "11.1. Binary Search Trees 463\n11.1.2 Insertions and Deletions\nBinary search trees allow implementations of the putandremove operations using\nalgorithms that are fairly straightforward, although not trivial.\nInsertion\nThe map operation put(k,v)begins with a search for an entry with key k. If found,\nthat entry’s existing value is reassigned. Otherwise, the new entry can be inserted\ninto the underlying tree by expanding the leaf that was reached at the end of the\nfailed search into an internal node. The binary search-tree property is sustained by\nthat placement (note that it is placed exactly where a search would expect it). Let\nus assume a proper binary tree supports the following update operation:\nexpandExternal( p,e):Stores entry eat the external position p, and expands p\nto be internal, having two new leaves as children.\nWe can then describe the TreeInsert algorithm with the pseudocode given in in\nCode Fragment 11.2. An example of insertion into a binary search tree is shown\nin Figure 11.4.\nAlgorithm TreeInsert( k,v):\nInput: A search key kto be associated with value v\np=TreeSearch (root(),k)\nifk==key(p)then\nChange p’s value to ( v)\nelse\nexpandExternal (p,(k,v))\nCode Fragment 11.2: Algorithm for inserting a key-value pair into a map that is\nrepresented as a binary search tree.\n28\n29\n808288\n65\n5444\n3217\n8\n9397\n76 21 21\n80 688288\n65\n5444\n3217\n8\n9397\n7628\n29\n(a) (b)\nFigure 11.4: Insertion of an entry with key 68 into the search tree of Figure 11.2.\nFinding the position to insert is shown in (a), and the resulting tree is shown in (b).\nwww.it-ebooks.info"
  },
  {
    "page": 482,
    "content": "464 Chapter 11. Search Trees\nDeletion\nDeleting an entry from a binary search tree is a bit more complex than inserting a\nnew entry because the position of an entry to be deleted might be anywhere in the\ntree (as opposed to insertions, which always occur at a leaf). To delete an entry withkeyk, we begin by calling TreeSearch(root(), k)to ﬁnd the position pstoring an\nentry with key equal to k(if any). If the search returns an external node, then there\nis no entry to remove. Otherwise, we distinguish between two cases (of increasingdifﬁculty):\n•If at most one of the children of position pis internal, the deletion of the\nentry at position pis easily implemented (see Figure 11.5). Let position rbe\na child of pthat is internal (or an arbitrary child, if both are leaves). We will\nremove pand the leaf that is r’s sibling, while promoting rupward to take the\nplace of p. We note that all remaining ancestor-descendant relationships that\nremain in the tree after the operation existed before the operation; therefore,the binary search-tree property is maintained.\n•If position phas two children, we cannot simply remove the node from the\ntree since this would create a “hole” and two orphaned children. Instead, weproceed as follows (see Figure 11.6):\n◦We locate position rcontaining the entry having the greatest key that\nis strictly less than that of position p(its so-called predecessor in the\nordering of keys). That predecessor will always be located in the right-most internal position of the left subtree of position p.\n◦We use r’s entry as a replacement for the one being deleted at posi-\ntion p. Because rhas the immediately preceding key in the map, any\nentries in p’s right subtree will have keys greater than rand any other\nentries in p’s left subtree will have keys less than r. Therefore, the\nbinary search-tree property is satisﬁed after the replacement.\n◦Having used r’s entry as a replacement for p, we instead delete the\nnode at position rfrom the tree. Fortunately, since rwas located as\nthe rightmost internal position in a subtree, rdoes not have an internal\nright child. Therefore, its deletion can be performed using the ﬁrst (andsimpler) approach.\nAs with searching and insertion, this algorithm for a deletion involves the\ntraversal of a single path downward from the root, possibly moving an entry be-tween two positions of this path, and removing a node from that path and promoting\nits child. Therefore, it executes in time O(h)where his the height of the tree.\nwww.it-ebooks.info"
  },
  {
    "page": 483,
    "content": "11.1. Binary Search Trees 465\n21p\n768288\n65\n5444\n3217\n8\n9397\n80 68r\n28\n2928\n29\n68r\n8288\n65\n5444\n17\n9397\n76\n808\n21\n(a) (b)\nFigure 11.5: Deletion from the binary search tree of Figure 11.4b, where the entry\nto delete (with key 32) is stored at a position pwith one child r: (a) before the\ndeletion; (b) after the deletion.\np\n21 298\n8288\n65\n5444\n9397\n76\n80 68r17\n28\n97\nr\n9317\n29\n80 687665 8p\n82\n28\n21 5444\n(a) (b)\nFigure 11.6: Deletion from the binary search tree of Figure 11.5b, where the entry\nto delete (with key 88) is stored at a position pwith two children, and replaced by\nits predecessor r: (a) before the deletion; (b) after the deletion.\nwww.it-ebooks.info"
  },
  {
    "page": 484,
    "content": "466 Chapter 11. Search Trees\n11.1.3 Java Implementation\nIn Code Fragments 11.3 through 11.6 we deﬁne a TreeMap class that implements\nthe sorted map ADT while using a binary search tree for storage. The TreeMap\nclass is declared as a child of the AbstractSortedMap base class, thereby inheriting\nsupport for performing comparisons based upon a given (or default) Comparator ,\na nestedMapEntry class for storing key-value pairs, and concrete implementations\nof methods keySet andvalues based upon the entrySet method, which we will\nprovide. (See Figure 10.2 on page 406 for an overview of our entire map hierarchy.)\nFor representing the tree structure, our TreeMap class maintains an instance of a\nsubclass of the LinkedBinaryTree class from Section 8.3.1. In this implementation,\nwe choose to represent the search tree as a proper binary tree, with explicit leaf\nnodes in the binary tree as sentinels, and map entries stored only at internal nodes.\n(We leave the task of a more space-efﬁcient implementation to Exercise P-11.55.)\nTheTreeSearch algorithm of Code Fragment 11.1 is implemented as a private\nrecursive method, treeSearch( p,k). That method either returns a position with an\nentry equal to key k, or else the last position that is visited on the search path. The\nmethod is not only used for all of the primary map operations, get(k),put(k,v), and\nremove( k), but for most of the sorted map methods, as the ﬁnal internal position\nvisited during an unsuccessful search has either the greatest key less than kor the\nleast key greater than k.\nFinally, we note that our TreeMap class is designed so that it can be subclassed\nto implement various forms of balanced search trees. We discuss the balancing\nframework more thoroughly in Section 11.2, but there are two aspects of the design\nthat impact the code presented in this section. First, our treemember is technically\ndeclared as an instance of a BalanceableBinaryTree class, which is a specialization\nof theLinkedBinaryTree class; however, we rely only on the inherited behaviors in\nthis section. Second, our code is peppered with calls to presumed methods named\nrebalanceAccess ,rebalanceInsert , andrebalanceDelete ; these methods do not do\nanything in this class, but they serve as hooks that can later be customized.\nWe conclude with a brief guide to the organization of our code.\nCode Fragment 11.3: Beginning of TreeMap class, including constructors, size\nmethod, and expandExternal andtreeSearch utilities.\nCode Fragment 11.4: Map operations get(k),put(k,v), andremove( k).\nCode Fragment 11.5: Sorted map ADT methods lastEntry() ,ﬂoorEntry( k), and\nlowerEntry( k), and protected utility treeMax . Symmet-\nric methods ﬁrstEntry() ,ceilingEntry( k),higherEntry( k),\nandtreeMin are provided online.\nCode Fragment 11.6: Support for producing an iteration of all entries (method\nentrySet of the map ADT), or of a selected range of entries\n(methodsubMap( k1,k2)of the sorted map ADT).\nwww.it-ebooks.info"
  },
  {
    "page": 485,
    "content": "11.1. Binary Search Trees 467\n1/∗∗An implementation of a sorted map using a binary search tree. ∗/\n2public class TreeMap<K,V>extends AbstractSortedMap <K,V>{\n3// To represent the underlying tree structure, we use a specialized subclass of the\n4// LinkedBinaryTree class that we name BalanceableBinaryTree (see Section 11.2).\n5protected BalanceableBinaryTree< K,V>tree =newBalanceableBinaryTree <>();\n6\n7/∗∗Constructs an empty map using the natural ordering of keys. ∗/\n8publicTreeMap(){\n9super(); // the AbstractSortedMap constructor\n10tree.addRoot( null); // create a sentinel leaf as root\n11}\n12/∗∗Constructs an empty map using the given comparator to order keys. ∗/\n13publicTreeMap(Comparator <K>comp){\n14super(comp); // the AbstractSortedMap constructor\n15tree.addRoot( null); // create a sentinel leaf as root\n16}\n17/∗∗Returns the number of entries in the map. ∗/\n18public int size(){\n19return(tree.size()−1) / 2; // only internal nodes have entries\n20}\n21/∗∗Utility used when inserting a new entry at a leaf of the tree ∗/\n22private void expandExternal(Position <Entry<K,V>>p, Entry<K,V>entry){\n23tree.set(p, entry); // store new entry at p\n24tree.addLeft(p, null); // add new sentinel leaves as children\n25tree.addRight(p, null);\n26}\n2728// Omitted from this code fragment, but included in the online version of the code,\n29// are a series of protected methods that provide notational shorthands to wrap\n30// operations on the underlying linked binary tree. For example, we support the\n31// protected syntax root() as shorthand for tree.root() with the following utility:\n32protected Position<Entry<K,V>>root(){returntree.root();}\n3334/∗∗Returns the position in p 's subtree having given key (or else the terminal leaf).∗ /\n35privatePosition<Entry<K,V>>treeSearch(Position <Entry<K,V>>p, K key){\n36if(isExternal(p))\n37 returnp; // key not found; return the ﬁnal leaf\n38intcomp = compare(key, p.getElement());\n39if(comp == 0)\n40 returnp; // key found; return its position\n41else if(comp<0)\n42 returntreeSearch(left(p), key); // search left subtree\n43else\n44 returntreeSearch(right(p), key); // search right subtree\n45}\nCode Fragment 11.3:\nBeginning of a TreeMap class based on a binary search tree.\nwww.it-ebooks.info"
  },
  {
    "page": 486,
    "content": "468 Chapter 11. Search Trees\n46/∗∗Returns the value associated with the speciﬁed key (or else null). ∗/\n47publicV get(K key) throwsIllegalArgumentException {\n48checkKey(key); // may throw IllegalArgumentException\n49Position<Entry<K,V>>p = treeSearch(root(), key);\n50rebalanceAccess(p); // hook for balanced tree subclasses\n51if(isExternal(p)) return null ; // unsuccessful search\n52returnp.getElement().getValue(); // match found\n53}\n54/∗∗Associates the given value with the given key, returning any overridden value. ∗/\n55publicV put(K key, V value) throwsIllegalArgumentException {\n56checkKey(key); // may throw IllegalArgumentException\n57Entry<K,V>newEntry = newMapEntry <>(key, value);\n58Position<Entry<K,V>>p = treeSearch(root(), key);\n59if(isExternal(p)){ // key is new\n60 expandExternal(p, newEntry);\n61 rebalanceInsert(p); // hook for balanced tree subclasses\n62 return null ;\n63}else{ // replacing existing key\n64 V old = p.getElement().getValue();\n65 set(p, newEntry);\n66 rebalanceAccess(p); // hook for balanced tree subclasses\n67 returnold;\n68}\n69}\n70/∗∗Removes the entry having key k (if any) and returns its associated value. ∗/\n71publicV remove(K key) throwsIllegalArgumentException {\n72checkKey(key); // may throw IllegalArgumentException\n73Position<Entry<K,V>>p = treeSearch(root(), key);\n74if(isExternal(p)){ // key not found\n75 rebalanceAccess(p); // hook for balanced tree subclasses\n76 return null ;\n77}else{\n78 V old = p.getElement().getValue();\n79 if(isInternal(left(p)) && isInternal(right(p))) {// both children are internal\n80 Position<Entry<K,V>>replacement = treeMax(left(p));\n81 set(p, replacement.getElement());\n82 p = replacement;\n83}// now p has at most one child that is an internal node\n84 Position<Entry<K,V>>leaf = (isExternal(left(p)) ? left(p) : right(p));\n85 Position<Entry<K,V>>sib = sibling(leaf);\n86 remove(leaf);\n87 remove(p); // sib is promoted in p’s place\n88 rebalanceDelete(sib); // hook for balanced tree subclasses\n89 returnold;\n90}\n91}\nCode Fragment 11.4: Primary map operations for the TreeMap class.\nwww.it-ebooks.info"
  },
  {
    "page": 487,
    "content": "11.1. Binary Search Trees 469\n92/∗∗Returns the position with the maximum key in subtree rooted at Position p. ∗/\n93protected Position<Entry<K,V>>treeMax(Position <Entry<K,V>>p){\n94Position<Entry<K,V>>walk = p;\n95while(isInternal(walk))\n96 walk = right(walk);\n97returnparent(walk); // we want the parent of the leaf\n98}\n99/∗∗Returns the entry having the greatest key (or null if map is empty). ∗/\n100publicEntry<K,V>lastEntry(){\n101 if(isEmpty()) return null;\n102 returntreeMax(root()).getElement();\n103}\n104/∗∗Returns the entry with greatest key less than or equal to given key (if any). ∗/\n105publicEntry<K,V>ﬂoorEntry(K key) throwsIllegalArgumentException {\n106 checkKey(key); // may throw IllegalArgumentException\n107 Position<Entry<K,V>>p = treeSearch(root(), key);\n108 if(isInternal(p)) returnp.getElement(); // exact match\n109 while(!isRoot(p)){\n110 if(p == right(parent(p)))\n111 returnparent(p).getElement(); // parent has next lesser key\n112 else\n113 p = parent(p);\n114}\n115 return null; // no such ﬂoor exists\n116}\n117/∗∗Returns the entry with greatest key strictly less than given key (if any). ∗/\n118publicEntry<K,V>lowerEntry(K key) throwsIllegalArgumentException {\n119 checkKey(key); // may throw IllegalArgumentException\n120 Position<Entry<K,V>>p = treeSearch(root(), key);\n121 if(isInternal(p) && isInternal(left(p)))\n122 returntreeMax(left(p)).getElement(); // this is the predecessor to p\n123 // otherwise, we had failed search, or match with no left child\n124 while(!isRoot(p)){\n125 if(p == right(parent(p)))\n126 returnparent(p).getElement(); // parent has next lesser key\n127 else\n128 p = parent(p);\n129}\n130 return null; // no such lesser key exists\n131}\nCode Fragment 11.5: A sample of the sorted map operations for the TreeMap class.\nThe symmetrical utility, treeMin , and public methods ﬁrstEntry ,ceilingEntry , and\nhigherEntry are available online.\nwww.it-ebooks.info"
  },
  {
    "page": 488,
    "content": "470 Chapter 11. Search Trees\n132/∗∗Returns an iterable collection of all key-value entries of the map. ∗/\n133publicIterable<Entry<K,V>>entrySet(){\n134 ArrayList <Entry<K,V>>buﬀer = newArrayList <>(size());\n135 for(Position<Entry<K,V>>p : tree.inorder())\n136 if(isInternal(p)) buﬀer.add(p.getElement());\n137 returnbuﬀer;\n138}\n139/∗∗Returns an iterable of entries with keys in range [fromKey, toKey). ∗/\n140publicIterable<Entry<K,V>>subMap(K fromKey, K toKey) {\n141 ArrayList <Entry<K,V>>buﬀer = newArrayList <>(size());\n142 if(compare(fromKey, toKey) <0) // ensure that fromKey <toKey\n143 subMapRecurse(fromKey, toKey, root(), buﬀer);\n144 returnbuﬀer;\n145}\n146private void subMapRecurse(K fromKey, K toKey, Position <Entry<K,V>>p,\n147 ArrayList <Entry<K,V>>buﬀer){\n148 if(isInternal(p))\n149 if(compare(p.getElement(), fromKey) <0)\n150 // p's key is less than fromKey, so any relevant entries are to the right\n151 subMapRecurse(fromKey, toKey, right(p), buﬀer);\n152 else{\n153 subMapRecurse(fromKey, toKey, left(p), buﬀer); // ﬁrst consider left subtree\n154 if(compare(p.getElement(), toKey) <0){ // p is within range\n155 buﬀer.add(p.getElement()); // so add it to buﬀer, and consider\n156 subMapRecurse(fromKey, toKey, right(p), buﬀer); // right subtree as well\n157}\n158}\n159}\nCode Fragment 11.6: TreeMap operations supporting iteration of the entire map, or\na portion of the map with a given key range.\n11.1.4 Performance of a Binary Search Tree\nAn analysis of the operations of our TreeMap class is given in Table 11.1. Almost\nall operations have a worst-case running time that depends on h, where his the\nheight of the current tree. This is because most operations rely on traversing a path\nfrom the root of the tree, and the maximum path length within a tree is proportional\nto the height of the tree. Most notably, our implementations of map operations\nget,put, andremove , and most of the sorted map operations, each begins with\na call to the treeSearch utility. Similar paths are traced when searching for the\nminimum or maximum entry in a subtree, a task used when ﬁnding a replacement\nduring a deletion or in ﬁnding the overall ﬁrst or last entry in the map. An iteration\nof the entire map is accomplished in O(n)time using an inorder traversal of the\nunderlying tree, and the recursive subMap implementation can be shown to run in\nO(s+h)worst-case bound for a call that reports sresults (see Exercise C-11.34).\nwww.it-ebooks.info"
  },
  {
    "page": 489,
    "content": "11.1. Binary Search Trees 471\nMethod Running Time\nsize,isEmpty O(1)\nget,put,remove O(h)\nﬁrstEntry ,lastEntry O(h)\nceilingEntry ,ﬂoorEntry ,lowerEntry ,higherEntry O(h)\nsubMap O(s+h)\nentrySet ,keySet ,values O(n)\nTable 11.1: Worst-case running times of the operations for a TreeMap . We denote\nthe current height of the tree with h, and the number of entries reported by subMap\nass. The space usage is O(n), where nis the number of entries stored in the map.\nA binary search tree Tis therefore an efﬁcient implementation of a map with n\nentries only if its height is small. In the best case, Thas height h=⌈log(n+1)⌉−1,\nwhich yields logarithmic-time performance for most of the map operations. In the\nworst case, however, Thas height n, in which case it would look and feel like\nan ordered list implementation of a map. Such a worst-case conﬁguration arises,\nfor example, if we insert entries with keys in increasing or decreasing order. (See\nFigure 11.7.)\n30\n4010\n20\nFigure 11.7: Example of a binary search tree with linear height, obtained by insert-\ning entries in increasing order of their keys.\nWe can nevertheless take comfort that, on average, a binary search tree with\nnkeys generated from a random series of insertions and removals of keys has ex-\npected height O(logn); the justiﬁcation of this statement is beyond the scope of the\nbook, requiring careful mathematical language to precisely deﬁne what we mean\nby a random series of insertions and removals, and sophisticated probability theory.\nIn applications where one cannot guarantee the random nature of updates, it\nis better to rely on variations of search trees, presented in the remainder of this\nchapter, that guarantee a worst-case height of O(logn), and thus O(logn)worst-\ncase time for searches, insertions, and deletions.\nwww.it-ebooks.info"
  },
  {
    "page": 490,
    "content": "472 Chapter 11. Search Trees\n11.2 Balanced Search Trees\nIn the closing of the previous section, we noted that if we could assume a random\nseries of insertions and removals, the standard binary search tree supports O(logn)\nexpected running times for the basic map operations. However, we may only claim\nO(n)worst-case time, because some sequences of operations may lead to an unbal-\nanced tree with height proportional to n.\nIn the remainder of this chapter, we will explore four search-tree algorithms that\nprovide stronger performance guarantees. Three of the four data structures (A VL\ntrees, splay trees, and red-black trees) are based on augmenting a standard binary\nsearch tree with occasional operations to reshape the tree and reduce its height.\nThe primary operation to rebalance a binary search tree is known as a rotation .\nDuring a rotation, we “rotate” a child to be above its parent, as diagrammed in\nFigure 11.8.\ny x\ny\nT1\nT2 T3 T1 T2T3x\nFigure 11.8: A rotation operation in a binary search tree. A rotation can be per-\nformed to transform the left formation into the right, or the right formation into the\nleft. Note that all keys in subtree T1have keys less than that of position x, all keys\nin subtree T2have keys that are between those of positions xandy, and all keys in\nsubtree T3have keys that are greater than that of position y.\nTo maintain the binary search-tree property through a rotation, we note that\nif position xwas a left child of position yprior to a rotation (and therefore the\nkey of xis less than the key of y), then ybecomes the right child of xafter the\nrotation, and vice versa. Furthermore, we must relink the subtree of entries with\nkeys that lie between the keys of the two positions that are being rotated. For\nexample, in Figure 11.8 the subtree labeled T2represents entries with keys that are\nknown to be greater than that of position xand less than that of position y. In the\nﬁrst conﬁguration of that ﬁgure, T2is the right subtree of position x; in the second\nconﬁguration, it is the left subtree of position y.\nBecause a single rotation modiﬁes a constant number of parent-child relation-\nships, it can be implemented in O(1)time with a linked binary tree representation.\nwww.it-ebooks.info"
  },
  {
    "page": 491,
    "content": "11.2. Balanced Search Trees 473\nIn the context of a tree-balancing algorithm, a rotation allows the shape of a\ntree to be modiﬁed while maintaining the search-tree property. If used wisely, this\noperation can be performed to avoid highly unbalanced tree conﬁgurations. Forexample, a rightward rotation from the ﬁrst formation of Figure 11.8 to the secondreduces the depth of each node in subtree T\n1by one, while increasing the depth\nof each node in subtree T3by one. (Note that the depth of nodes in subtree T2are\nunaffected by the rotation.)\nOne or more rotations can be combined to provide broader rebalancing within a\ntree. One such compound operation we consider is a trinode restructuring . For this\nmanipulation, we consider a position x, its parent y, and its grandparent z. The goal\nis to restructure the subtree rooted at zin order to reduce the overall path length\ntoxand its subtrees. Pseudocode for a restructure( x)method is given in Code\nFragment 11.7 and illustrated in Figure 11.9. In describing a trinode restructuring,we temporarily rename the positions x,y, and zasa,b, and c, so that aprecedes b\nandbprecedes cin an inorder traversal of T. There are four possible orientations\nmapping x,y, and ztoa,b, and c, as shown in Figure 11.9, which are uniﬁed\ninto one case by our relabeling. The trinode restructuring replaces zwith the node\nidentiﬁed as b, makes the children of this node be aandc, and makes the children\nofaandcbe the four previous children of x,y, and z(other than xandy), while\nmaintaining the inorder relationships of all the nodes in T.\nAlgorithm restructure( x):\nInput: A position xof a binary search tree Tthat has both a parent yand a\ngrandparent z\nOutput: Tree Tafter a trinode restructuring (which corresponds to a single or\ndouble rotation) involving positions x,y, and z\n1:Let ( a,b,c) be a left-to-right (inorder) listing of the positions x,y, and z, and\nlet (T1,T2,T3,T4) be a left-to-right (inorder) listing of the four subtrees of x,y,\nandznot rooted at x,y, orz.\n2:Replace the subtree rooted at zwith a new subtree rooted at b.\n3:Letabe the left child of band let T1andT2be the left and right subtrees of a,\nrespectively.\n4:Letcbe the right child of band let T3andT4be the left and right subtrees of c,\nrespectively.\nCode Fragment 11.7: The trinode restructuring operation in a binary search tree.\nIn practice, the modiﬁcation of a tree Tcaused by a trinode restructuring op-\neration can be implemented through case analysis either as a single rotation (as inFigure 11.9a and b) or as a double rotation (as in Figure 11.9c and d). The doublerotation arises when position xhas the middle of the three relevant keys and is ﬁrst\nrotated above its parent, and then above what was originally its grandparent. In any\nof the cases, the trinode restructuring is completed with O(1)running time.\nwww.it-ebooks.info"
  },
  {
    "page": 492,
    "content": "474 Chapter 11. Search Trees\nsingle rotation\nT1a=z\nb=y\nT2c=x\nT3 T4a=z\nT1 T2b=y\nc=x\nT3 T4\n(a)\nT1a=x\nT1 T2b=y\nc=z\nT3 T4single rotation\nT4c=z\nb=y\nT3a=x\nT2\n(b)\nT3T1a=z\nT1 T2b=x\nc=y\nT3 T4double rotation\na=z\nT4c=y\nb=x\nT2\n(c)\nT2T1 T2b=x\nc=z\nT3 T4double rotation\na=y\nT4c=z\nT1a=y\nb=x\nT3\n(d)\nFigure 11.9: Schematic illustration of a trinode restructuring operation: (a and b)\nrequire a single rotation; (c and d) require a double rotation.\nwww.it-ebooks.info"
  },
  {
    "page": 493,
    "content": "11.2. Balanced Search Trees 475\n11.2.1 Java Framework for Balancing Search Trees\nOurTreeMap class (introduced in Section 11.1.3) is a fully functional map imple-\nmentation. However, the running time for its operations depend on the height of the\ntree, and in the worst-case, that height may be O(n)for a map with nentries. There-\nfore, we have intentionally designed the TreeMap class in a way that allows it to\nbe easily extended to provide more advanced tree-balancing strategies. In later sec-\ntions of this chapter, we will implement subclasses AVLTreeMap ,SplayTreeMap ,\nandRBTreeMap . In this section, we describe three important forms of support that\ntheTreeMap class offers these subclasses.\nHooks for Rebalancing Operations\nOur implementation of the basic map operations in Section 11.1.3 includes strategic\ncalls to three nonpublic methods that serve as hooks for rebalancing algorithms:\n•A call torebalanceInsert( p)is made from within the putmethod, after a new\nnode is added to the tree at position p(line 61 of Code Fragment 11.4).\n•A call to rebalanceDelete( p)is made from within the remove method, after\na node is deleted from the tree (line 88 of Code Fragment 11.4); position p\nidentiﬁes the child of the removed node that was promoted in its place.\n•A call torebalanceAccess( p)is made by any call to get,put, orremove that\ndoes notresult in a structural change. Position p, which could be internal\nor external, represents the deepest node of the tree that was accessed during\nthe operation. This hook is speciﬁcally used by the splay tree structure (see\nSection 11.4) to restructure a tree so that more frequently accessed nodes are\nbrought closer to the root.\nWithin our TreeMap class, we provide the trivial declarations of these three\nmethods, having bodies that do nothing, as shown in Code Fragment 11.8. A sub-\nclass ofTreeMap may override any of these methods to implement a nontrivial\naction to rebalance a tree. This is another example of the template method design\npattern , as originally discussed in Section 2.3.3.\nprotected void rebalanceInsert(Position <Entry<K,V>>p){}\nprotected void rebalanceDelete(Position <Entry<K,V>>p){}\nprotected void rebalanceAccess(Position <Entry<K,V>>p){}\nCode Fragment 11.8: Trivial deﬁnitions of TreeMap methods that serve as hooks\nfor our rebalancing framework. These methods may be overridden by subclasses\nin order to perform appropriate rebalancing operations.\nwww.it-ebooks.info"
  },
  {
    "page": 494,
    "content": "476 Chapter 11. Search Trees\nProtected Methods for Rotating and Restructuring\nTo support common restructuring operations, our TreeMap class relies on storing\nthe tree as an instance of a new nested class, BalanceableBinaryTree (shown in\nCode Fragments 11.9 and 11.10). That class is a specialization of the original\nLinkedBinaryTree class from Section 8.3.1. This new class provides protected util-\nity methods rotate andrestructure that, respectively, implement a single rotation\nand a trinode restructuring (described at the beginning of Section 11.2). Althoughthese methods are not invoked by the standard TreeMap operations, their inclusion\nsupports greater code reuse, as they are available to all balanced-tree subclasses.\nThese methods are implemented in Code Fragment 11.10. To simplify the code,\nwe deﬁne an additional relink utility that properly links parent and child nodes to\neach other. The focus of the rotate method then becomes redeﬁning the relation-\nship between the parent and child, relinking a rotated node directly to its originalgrandparent, and shifting the “middle” subtree (that labeled as T\n2in Figure 11.8)\nbetween the rotated nodes.\nFor the trinode restructuring, we determine whether to perform a single or dou-\nble rotation, as originally described in Figure 11.9. The four cases in that ﬁguredemonstrate a downward path ztoytoxthat are respectively right-right, left-left,\nright-left, and left-right. The ﬁrst two patterns, with matching orientation, war-rant a single rotation moving yupward, while the last two patterns, with opposite\norientations, warrant a double rotation moving xupward.\nSpecialized Nodes with an Auxiliary Data Member\nMany tree-balancing strategies require that some form of auxiliary “balancing” in-formation be stored at nodes of a tree. To ease the burden on the balanced-treesubclasses, we choose to add an auxiliary integer value to every node within the\nBalanceableSearchTree class. This is accomplished by deﬁning a new BSTNode\nclass, which itself inherits from the nested LinkedBinaryTree.Node class. The new\nclass declares the auxiliary variable, and provides methods for getting and setting\nits value.\nWe draw attention to an important subtlety in our design, including that of\nthe original LinkedBinaryTree subclass. Whenever a low-level operation on an\nunderlying linked tree requires a new node, we must ensure that the correct typeof node is created. That is, for our balanceable tree, we need each node to beaBTNode , which includes the auxiliary ﬁeld. However, the creation of nodes\noccurs within low-level operations, such as addLeft andaddRight , that reside in\nthe original LinkedBinaryTree class.\nwww.it-ebooks.info"
  },
  {
    "page": 495,
    "content": "11.2. Balanced Search Trees 477\nWe rely on a technique known as the factory method design pattern . The\nLinkedBinaryTree class includes a protected method, createNode (originally given\nat lines 30–33 of Code Fragment 8.8), that is responsible for instantiating a new\nnode of the appropriate type. The rest of the code in that class makes sure to alwaysuse thecreateNode method when a new node is needed.\nIn theLinkedBinaryTree class, the createNode method returns a simple Node\ninstance. In our new BalanceableBinaryTree class, we override the createNode\nmethod (see lines 22–27 in Code Fragment 11.9), so that a new instance of theBSTNode class is returned. In this way, we effectively change the behavior of the\nlow-level operations in the LinkedBinaryTree class so that it uses instances of our\nspecialized node class, and therefore, that every node in our balanced trees includessupport for the new auxiliary ﬁeld.\n1/∗∗A specialized version of LinkedBinaryTree with support for balancing. ∗/\n2protected static class BalanceableBinaryTree <K,V>\n3 extends LinkedBinaryTree <Entry<K,V>>{\n4//-------------- nested BSTNode class --------------\n5// this extends the inherited LinkedBinaryTree.Node class\n6protected static class BSTNode <E>extends Node<E>{\n7intaux=0;\n8BSTNode(E e, Node< E>parent, Node <E>leftChild, Node <E>rightChild){\n9 super(e, parent, leftChild, rightChild);\n10}\n11public int getAux(){returnaux;}\n12public void setAux(intvalue){aux = value;}\n13}//--------- end of nested BSTNode class ---------\n14\n15// positional-based methods related to aux ﬁeld\n16public int getAux(Position <Entry<K,V>>p){\n17return((BSTNode <Entry<K,V>>) p).getAux();\n18}\n19public void setAux(Position <Entry<K,V>>p,intvalue){\n20((BSTNode <Entry<K,V>>) p).setAux(value);\n21}\n22// Override node factory function to produce a BSTNode (rather than a Node)\n23protected\n24Node<Entry<K,V>>createNode(Entry <K,V>e, Node<Entry<K,V>>parent,\n25 Node<Entry<K,V>>left, Node <Entry<K,V>>right){\n26return new BSTNode <>(e, parent, left, right);\n27}\nCode Fragment 11.9: TheBalanceableBinaryTree class, which is nested within the\nTreeMap class deﬁnition. (Continues in Code Fragment 11.10.)\nwww.it-ebooks.info"
  },
  {
    "page": 496,
    "content": "478 Chapter 11. Search Trees\n28/∗∗Relinks a parent node with its oriented child node. ∗/\n29private void relink(Node <Entry<K,V>>parent, Node <Entry<K,V>>child,\n30 boolean makeLeftChild){\n31child.setParent(parent);\n32if(makeLeftChild)\n33 parent.setLeft(child);\n34else\n35 parent.setRight(child);\n36}\n37/∗∗Rotates Position p above its parent. ∗/\n38public void rotate(Position <Entry<K,V>>p){\n39Node<Entry<K,V>>x = validate(p);\n40Node<Entry<K,V>>y = x.getParent(); // we assume this exists\n41Node<Entry<K,V>>z = y.getParent(); // grandparent (possibly null)\n42if(z ==null){\n43 root = x; // x becomes root of the tree\n44 x.setParent( null);\n45}else\n46 relink(z, x, y == z.getLeft()); // x becomes direct child of z\n47// now rotate x and y, including transfer of middle subtree\n48if(x == y.getLeft()) {\n49 relink(y, x.getRight(), true); // x’s right child becomes y’s left\n50 relink(x, y, false); // y becomes x’s right child\n51}else{\n52 relink(y, x.getLeft(), false); // x’s left child becomes y’s right\n53 relink(x, y, true); // y becomes left child of x\n54}\n55}\n56/∗∗Performs a trinode restructuring of Position x with its parent/grandparent. ∗/\n57publicPosition<Entry<K,V>>restructure(Position <Entry<K,V>>x){\n58Position<Entry<K,V>>y = parent(x);\n59Position<Entry<K,V>>z = parent(y);\n60if((x == right(y)) == (y == right(z))) {// matching alignments\n61 rotate(y); // single rotation (of y)\n62 returny; // y is new subtree root\n63}else{ // opposite alignments\n64 rotate(x); // double rotation (of x)\n65 rotate(x);\n66 returnx; // x is new subtree root\n67}\n68}\n69}\nCode Fragment 11.10: TheBalanceableBinaryTree class, which is nested within\ntheTreeMap class deﬁnition (continued from Code Fragment 11.9).\nwww.it-ebooks.info"
  },
  {
    "page": 497,
    "content": "11.3. A VL Trees 479\n11.3 AVL Trees\nTheTreeMap class, which uses a standard binary search tree as its data structure,\nshould be an efﬁcient map data structure, but its worst-case performance for the\nvarious operations is linear time, because it is possible that a series of operations\nresults in a tree with linear height. In this section, we describe a simple balancing\nstrategy that guarantees worst-case logarithmic running time for all the fundamental\nmap operations.\nDeﬁnition of an AVL Tree\nThe simple correction is to add a rule to the binary search-tree deﬁnition that will\nmaintain a logarithmic height for the tree. Recall that we deﬁned the height of a\nsubtree rooted at position pof a tree to be the number of edges on the longest path\nfrom pto a leaf (see Section 8.1.3). By this deﬁnition, a leaf position has height 0.\nIn this section, we consider the following height-balance property , which char-\nacterizes the structure of a binary search tree Tin terms of the heights of its nodes.\nHeight-Balance Property :For every internal position pofT, the heights of the\nchildren of pdiffer by at most 1.\nAny binary search tree Tthat satisﬁes the height-balance property is said to be an\nAVL tree , named after the initials of its inventors: Adel’son-Vel’skii and Landis.\nAn example of an A VL tree is shown in Figure 11.10.\n14\n1 12 3\n2\n117\n32\n48 625078\n8844\nFigure 11.10: An example of an A VL tree. The keys of the entries are shown inside\nthe nodes, and the heights of the nodes are shown above the nodes (all leaves have\nheight 0).\nwww.it-ebooks.info"
  },
  {
    "page": 498,
    "content": "480 Chapter 11. Search Trees\nAn immediate consequence of the height-balance property is that a subtree of an\nA VL tree is itself an A VL tree. The height-balance property also has the important\nconsequence of keeping the height small, as shown in the following proposition.\nProposition 11.1: The height of an A VL tree storing nentries is O(logn).\nJustiﬁcation: Instead of trying to ﬁnd an upper bound on the height of an A VL\ntree directly, it turns out to be easier to work on the “inverse problem” of ﬁnding\na lower bound on the minimum number of internal nodes, denoted as n(h), of an\nA VL tree with height h. We will show that n(h)grows at least exponentially. From\nthis, it will be an easy step to derive that the height of an A VL tree storing nentries\nisO(logn).\nWe begin by noting that n(1)=1 and n(2)=2, because an A VL tree of height 1\nmust have exactly one internal node and an A VL tree of height 2 must have at least\ntwo internal nodes. Now, an A VL tree with the minimum number of nodes havingheight hforh≥3, is such that both its subtrees are A VL trees with the minimum\nnumber of nodes: one with height h−1 and the other with height h−2. Taking the\nroot into account, we obtain the following formula that relates n(h)ton(h−1)and\nn(h−2), for h≥3:\nn(h)=1+n(h−1)+n(h−2). (11.1)\nAt this point, the reader familiar with the properties of Fibonacci progressions (Sec-tions 2.2.3 and 5.5) will already see that n(h)is a function exponential in h. To\nformalize that observation, we proceed as follows.\nFormula 11.1 implies that n(h)is a strictly increasing function of h. Thus, we\nknow that n(h−1)>n(h−2). Replacing n(h−1)with n(h−2)in Formula 11.1\nand dropping the 1, we get, for h≥3,\nn(h)>2·n(h−2). (11.2)\nFormula 11.2 indicates that n(h)at least doubles each time hincreases by 2, which\nintuitively means that n(h)grows exponentially. To show this fact in a formal way,\nwe apply Formula 11.2 repeatedly, yielding the following series of inequalities:\nn(h)>2·n(h−2)\n>4·n(h−4)\n>8·n(h−6)\n...\n>2\ni·n(h−2i). (11.3)\nThat is, n(h)>2i·n(h−2i), for any integer i, such that h−2i≥1. Since we already\nknow the values of n(1)andn(2), we pick iso that h−2iis equal to either 1 or 2.\nwww.it-ebooks.info"
  },
  {
    "page": 499,
    "content": "11.3. A VL Trees 481\nThat is, we pick\ni=/ceilingleftbiggh\n2/ceilingrightbigg\n−1.\nBy substituting the above value of iin Formula 11.3, we obtain, for h≥3,\nn(h)>2⌈h\n2⌉−1·n/parenleftbigg\nh−2/ceilingleftbiggh\n2/ceilingrightbigg\n+2/parenrightbigg\n≥2⌈h\n2⌉−1n(1)\n≥2h\n2−1. (11.4)\nBy taking logarithms of both sides of Formula 11.4, we obtain\nlog(n(h))>h\n2−1,\nfrom which we geth<2log(n(h))+2, (11.5)\nwhich implies that an A VL tree storing nentries has height at most 2log n+2.\nBy Proposition 11.1 and the analysis of binary search trees given in Section 11.1,\nthe operation get, in a map implemented with an A VL tree, runs in time O(logn),\nwhere nis the number of entries in the map. Of course, we still have to show how\nto maintain the height-balance property after an insertion or deletion.\n11.3.1 Update Operations\nGiven a binary search tree T, we say that a position is balanced if the absolute\nvalue of the difference between the heights of its children is at most 1, and we say\nthat it is unbalanced otherwise. Thus, the height-balance property characterizing\nA VL trees is equivalent to saying that every position is balanced.\nThe insertion and deletion operations for A VL trees begin similarly to the corre-\nsponding operations for (standard) binary search trees, but with post-processing for\neach operation to restore the balance of any portions of the tree that are adversely\naffected by the change.\nInsertion\nSuppose that tree Tsatisﬁes the height-balance property, and hence is an A VL tree,\nprior to the insertion of a new entry. An insertion of a new entry in a binary search\ntree, as described in Section 11.1.2, results in a leaf position pbeing expanded\nto become internal, with two new external children. This action may violate the\nheight-balance property (see, for example, Figure 11.11a), yet the only positions\nthat may become unbalanced are ancestors of p, because those are the only posi-\ntions whose subtrees have changed. Therefore, let us describe how to restructure T\nto ﬁx any unbalance that may have occurred.\nwww.it-ebooks.info"
  },
  {
    "page": 500,
    "content": "482 Chapter 11. Search Trees\nT4\nT2T15488\n245\n44\n17\n32\n112\n11\n48 62z\ny\nx\nT35078\n3\nT1 T2 T4234\n2\ny44\n17\n78x\nz2\n50\n1\n8832\n541 162\n48\nT31\n(a) (b)\nFigure 11.11: An example insertion of an entry with key 54 in the A VL tree of\nFigure 11.10: (a) after adding a new node for key 54, the nodes storing keys 78\nand 44 become unbalanced; (b) a trinode restructuring restores the height-balance\nproperty. We show the heights of nodes above them, and we identify the nodes x,\ny, and zand subtrees T1,T2,T3, and T4participating in the trinode restructuring.\nWe restore the balance of the nodes in the binary search tree Tby a simple\n“search-and-repair” strategy. In particular, let zbe the ﬁrst position we encounter in\ngoing up from ptoward the root of Tsuch that zis unbalanced (see Figure 11.11a.)\nAlso, let ydenote the child of zwith greater height (and note that ymust be an\nancestor of p). Finally, let xbe the child of ywith greater height (there cannot be\na tie and position xmust also be an ancestor of p, possibly pitself). We rebalance\nthe subtree rooted at zby calling the trinode restructuring method,restructure( x),\noriginally described in Section 11.2. An example of such a restructuring in the\ncontext of an A VL insertion is portrayed in Figure 11.11.\nTo formally argue the correctness of this process in reestablishing the A VL\nheight-balance property, we consider the implication of zbeing the nearest ancestor\nofpthat became unbalanced after the insertion of p. It must be that the height\nofyincreased by one due to the insertion and that it is now 2 greater than its\nsibling. Since yremains balanced, it must be that it formerly had subtrees with\nequal heights, and that the subtree containing xhas increased its height by one.\nThat subtree increased either because x=p, and thus its height changed from 0\nto 1, or because xpreviously had equal-height subtrees and the height of the one\ncontaining phas increased by 1. Letting h≥0 denote the height of the tallest child\nofx, this scenario might be portrayed as in Figure 11.12.\nAfter the trinode restructuring, each of x,y, and zis balanced. Furthermore,\nthe root of the subtree after the restructuring has height h+2, which is precisely\nthe height that zhad before the insertion of the new entry. Therefore, any ancestor\nofzthat became temporarily unbalanced becomes balanced again, and this one\nrestructuring restores the height-balance property globally .\nwww.it-ebooks.info"
  },
  {
    "page": 501,
    "content": "11.3. A VL Trees 483\nT3 T2x h−1 h−1\nT4y\nT1z hh+2\nhhh+1\n(a)\nh+2\nT3T2x h−1 h\nT4y\nT1z hh+3\nhh+1\n(b)\nhh+2\nT2T1h z h−1h+1\nT4h\nT3yh+1\nx\n(c)\nFigure 11.12: Rebalancing of a subtree during a typical insertion into an A VL tree:\n(a) before the insertion; (b) after an insertion in subtree T3causes imbalance at z;\n(c) after restoring balance with trinode restructuring. Notice that the overall height\nof the subtree after the insertion is the same as before the insertion.\nwww.it-ebooks.info"
  },
  {
    "page": 502,
    "content": "484 Chapter 11. Search Trees\nDeletion\nRecall that a deletion from a regular binary search tree results in the structural\nremoval of a node having either zero or one internal children. Such a change mayviolate the height-balance property in an A VL tree. In particular, if position p\nrepresents a (possibly external) child of the removed node in tree T, there may be\nan unbalanced node on the path from pto the root of T. (See Figure 11.13a.) In\nfact, there can be at most one such unbalanced node. (The justiﬁcation of this factis left as Exercise C-11.41.)\nT1\nT2T41\n2\n32 x50\n117\n541\n48z44\n62\n2\n881\nT334\ny\n78\nT1 T4\nT24\n62\nx44y\n3\nT378\n2 0\n50\n48 54171\n1 1z2\n1\n88\n(a) (b)\nFigure 11.13: Deletion of the entry with key 32 from the A VL tree of Figure 11.11b:\n(a) after removing the node storing key 32, the root becomes unbalanced; (b) atrinode restructuring of x,y, and zrestores the height-balance property.\nAs with insertion, we use trinode restructuring to restore balance in the tree T.\nIn particular, let zbe the ﬁrst unbalanced position encountered going up from p\ntoward the root of T, and let ybe that child of zwith greater height ( ywill not be an\nancestor of p). Furthermore, let xbe the child of ydeﬁned as follows: if one of the\nchildren of yis taller than the other, let xbe the taller child of y; else (both children\nofyhave the same height), let xbe the child of yon the same side as y(that is, if y\nis the left child of z, letxbe the left child of y, else let xbe the right child of y). We\nthen perform a restructure( x)operation. (See Figure 11.13b.)\nThe restructured subtree is rooted at the middle position denoted as bin the\ndescription of the trinode restructuring operation. The height-balance property isguaranteed to be locally restored within the subtree of b. (See Exercises R-11.11\nand R-11.12.) Unfortunately, this trinode restructuring may reduce the height of the\nsubtree rooted at bby 1, which may cause an ancestor of bto become unbalanced.\nSo, after rebalancing z, we continue walking up Tlooking for unbalanced positions.\nIf we ﬁnd another, we perform a restructure operation to restore its balance, and\ncontinue marching up Tlooking for more, all the way to the root. Since the height\nofTisO(logn), where nis the number of entries, by Proposition 11.1, O(logn)\ntrinode restructurings are sufﬁcient to restore the height-balance property.\nwww.it-ebooks.info"
  },
  {
    "page": 503,
    "content": "11.3. A VL Trees 485\nPerformance of AVL Trees\nBy Proposition 11.1, the height of an A VL tree with nentries is guaranteed to\nbeO(logn). Because the standard binary search-tree operation had running times\nbounded by the height (see Table 11.1), and because the additional work in main-\ntaining balance factors and restructuring an A VL tree can be bounded by the length\nof a path in the tree, the traditional map operations run in worst-case logarithmic\ntime with an A VL tree. We summarize these results in Table 11.2, and illustrate\nthis performance in Figure 11.14.\nMethod Running Time\nsize,isEmpty O(1)\nget,put,remove O(logn)\nﬁrstEntry ,lastEntry O(logn)\nceilingEntry ,ﬂoorEntry ,lowerEntry ,higherEntry O(logn)\nsubMap O(s+logn)\nentrySet ,keySet ,values O(n)\nTable 11.2: Worst-case running times of operations for an n-entry sorted map real-\nized as an A VL tree T, with sdenoting the number of entries reported by subMap .\nWorst-case time: O(logn)Height Time per level\nA VL Tree T:\ndown phase\nup phaseO(logn)O(1)\nO(1)\nO(1)\nFigure 11.14: Illustrating the running time of searches and updates in an A VL tree.\nThe time performance is O(1)per level, broken into a down phase, which typi-\ncally involves searching, and an up phase, which typically involves updating height\nvalues and performing local trinode restructurings (rotations).\nwww.it-ebooks.info"
  },
  {
    "page": 504,
    "content": "486 Chapter 11. Search Trees\n11.3.2 Java Implementation\nA complete implementation of an AVLTreeMap class is provided in Code Frag-\nments 11.11 and 11.12. It inherits from the standard TreeMap class and relies on\nthe balancing framework described in Section 11.2.1. We highlight two important\naspects of our implementation. First, the AVLTreeMap uses the node’s auxiliary\nbalancing variable to store the height of the subtree rooted at that node, with leaves\nhaving a balance factor of 0 by default. We also provide several utilities involving\nheights of nodes (see Code Fragment 11.11).\nTo implement the core logic of the A VL balancing strategy, we deﬁne a utility,\nnamedrebalance , that sufﬁces to restore the height-balance property after an inser-\ntion or a deletion (see Code Fragment 11.11). Although the inherited behaviors for\ninsertion and deletion are quite different, the necessary post-processing for an A VL\ntree can be uniﬁed. In both cases, we trace an upward path from the position pat\nwhich the change took place, recalculating the height of each position based on the\n(updated) heights of its children. We perform a trinode restructuring operation if\nan imbalanced position is reached. The upward march from pcontinues until we\nreach an ancestor with height that was unchanged by the map operation, or with\nheight that was restored to its previous value by a trinode restructuring operation,\nor until reaching the root of the tree (in which case the overall height of the tree\nhas increased by one). To easily detect the stopping condition, we record the “old”\nheight of a position, as it existed before the insertion or deletion operation begin,\nand compare that to the newly calculated height after a possible restructuring.\n1/∗∗An implementation of a sorted map using an AVL tree. ∗/\n2public class AVLTreeMap <K,V>extends TreeMap<K,V>{\n3/∗∗Constructs an empty map using the natural ordering of keys. ∗/\n4publicAVLTreeMap(){super();}\n5/∗∗Constructs an empty map using the given comparator to order keys. ∗/\n6publicAVLTreeMap(Comparator <K>comp){super(comp);}\n7/∗∗Returns the height of the given tree position. ∗/\n8protected int height(Position <Entry<K,V>>p){\n9returntree.getAux(p);\n10}\n11/∗∗Recomputes the height of the given position based on its children 's heights. ∗/\n12protected void recomputeHeight(Position <Entry<K,V>>p){\n13tree.setAux(p, 1 + Math.max(height(left(p)), height(right(p))));\n14}\n15/∗∗Returns whether a position has balance factor between −1 and 1 inclusive. ∗/\n16protected boolean isBalanced(Position <Entry<K,V>>p){\n17returnMath.abs(height(left(p)) −height(right(p))) <= 1;\n18}\nCode Fragment 11.11: AVLTreeMap class. (Continues in Code Fragment 11.12.)\nwww.it-ebooks.info"
  },
  {
    "page": 505,
    "content": "11.3. A VL Trees 487\n19/∗∗Returns a child of p with height no smaller than that of the other child. ∗/\n20protected Position<Entry<K,V>>tallerChild(Position <Entry<K,V>>p){\n21if(height(left(p)) >height(right(p))) returnleft(p); // clear winner\n22if(height(left(p)) <height(right(p))) returnright(p); // clear winner\n23// equal height children; break tie while matching parent' s orientation\n24if(isRoot(p)) returnleft(p); // choice is irrelevant\n25if(p == left(parent(p))) returnleft(p); // return aligned child\n26else return right(p);\n27}\n28/∗∗\n29 ∗Utility used to rebalance after an insert or removal operation. This traverses the\n30 ∗path upward from p, performing a trinode restructuring when imbalance is found,\n31 ∗continuing until balance is restored.\n32 ∗/\n33protected void rebalance(Position <Entry<K,V>>p){\n34intoldHeight, newHeight;\n35do{\n36 oldHeight = height(p); // not yet recalculated if internal\n37 if(!isBalanced(p)){ // imbalance detected\n38 // perform trinode restructuring, setting p to resulting root,\n39 // and recompute new local heights after the restructuring\n40 p = restructure(tallerChild(tallerChild(p)));\n41 recomputeHeight(left(p));\n42 recomputeHeight(right(p));\n43}\n44 recomputeHeight(p);\n45 newHeight = height(p);\n46 p = parent(p);\n47}while(oldHeight != newHeight && p != null);\n48}\n49/∗∗Overrides the TreeMap rebalancing hook that is called after an insertion. ∗/\n50protected void rebalanceInsert(Position <Entry<K,V>>p){\n51rebalance(p);\n52}\n53/∗∗Overrides the TreeMap rebalancing hook that is called after a deletion. ∗/\n54protected void rebalanceDelete(Position <Entry<K,V>>p){\n55if(!isRoot(p))\n56 rebalance(parent(p));\n57}\n58}\nCode Fragment 11.12: AVLTreeMap class (continued from Code Fragment 11.11).\nwww.it-ebooks.info"
  },
  {
    "page": 506,
    "content": "488 Chapter 11. Search Trees\n11.4 Splay Trees\nThe next search-tree structure we study is known as a a splay tree . This structure is\nconceptually quite different from the other balanced search trees we will discuss in\nthis chapter, for a splay tree does not strictly enforce a logarithmic upper bound on\nthe height of the tree. In fact, no additional height, balance, or other auxiliary data\nneed be stored with the nodes of this tree.\nThe efﬁciency of splay trees is due to a certain move-to-root operation, called\nsplaying , that is performed at the bottommost position preached during every in-\nsertion, deletion, or even a search. (In essence, this is a variant of the move-to-front\nheuristic that we explored for lists in Section 7.7.2.) Intuitively, a splay operation\ncauses more frequently accessed elements to remain nearer to the root, thereby re-\nducing the typical search times. The surprising thing about splaying is that it allows\nus to guarantee a logarithmic amortized running time, for insertions, deletions, and\nsearches.\n11.4.1 Splaying\nGiven a node xof a binary search tree T, we splay xby moving xto the root of T\nthrough a sequence of restructurings. The particular restructurings we perform are\nimportant, for it is not sufﬁcient to move xto the root of Tby just any sequence\nof restructurings. The speciﬁc operation we perform to move xup depends upon\nthe relative positions of x, its parent y, and x’s grandparent z(if it exists). There are\nthree cases that we will consider.\nzig-zig :The node xand its parent yare both left children or both right children.\n(See Figure 11.15.) We promote x, making ya child of xandza child of y,\nwhile maintaining the inorder relationships of the nodes in T.\nT1y\nT2\nT3 T4z10\nx20\n30\nT4\nT3\nT2 T120\ny1030\nzx\n(a) (b)\nFigure 11.15: Zig-zig: (a) before; (b) after. There is another symmetric conﬁgura-\ntion where xandyare left children.\nwww.it-ebooks.info"
  },
  {
    "page": 507,
    "content": "11.4. Splay Trees 489\nzig-zag :One of xandyis a left child and the other is a right child. (See Fig-\nure 11.16.) In this case, we promote xby making xhave yandzas its chil-\ndren, while maintaining the inorder relationships of the nodes in T.\nxz\nT4y\nT2 T33010\n20\nT110\nT2y\nT3 T420\nzx30\nT1\n(a) (b)\nFigure 11.16: Zig-zag: (a) before; (b) after. There is another symmetric conﬁgura-\ntion where xis a right child and yis a left child.\nzig:xdoes not have a grandparent. (See Figure 11.17.) In this case, we perform a\nsingle rotation to promote xover y, making ya child of x, while maintaining\nthe relative inorder relationships of the nodes in T.\nT1\nT2 T310\ny\n20\nx\nT1 T2T310\ny20\nx\n(a) (b)\nFigure 11.17: Zig: (a) before; (b) after. There is another symmetric conﬁguration\nwhere xis originally a left child of y.\nWe perform a zig-zig or a zig-zag when xhas a grandparent, and we perform a\nzig when xhas a parent but not a grandparent. A splaying step consists of repeating\nthese restructurings at xuntil xbecomes the root of T. An example of the splaying\nof a node is shown in Figures 11.18 and 11.19.\nwww.it-ebooks.info"
  },
  {
    "page": 508,
    "content": "490 Chapter 11. Search Trees\n4\n56\n1410\n11\n12\n13 17168\n3\n7\n(a)\n4\n56\n1611\n178\n3 10\n12\n14\n137\n(b)\n4\n56\n1710\n11\n12\n13 16148\n3\n7\n(c)\nFigure 11.18: Example of splaying a node: (a) splaying the node storing 14 starts\nwith a zig-zag; (b) after the zig-zag; (c) the next step will be a zig-zig. (Continues\nin Figure 11.19.)\nwww.it-ebooks.info"
  },
  {
    "page": 509,
    "content": "11.4. Splay Trees 491\n4\n56\n178\n3 10\n12 1614\n11 13 7\n(d)\n16\n1110\n4\n1338\n14\n12 6\n5 7 17\n(e)\n8\n312\n413 111716 10\n6\n5 714\n(f)\nFigure 11.19: Example of splaying a node:(d) after the zig-zig; (e) the next step is\nagain a zig-zig; (f) after the zig-zig. (Continued from Figure 11.18.)\nwww.it-ebooks.info"
  },
  {
    "page": 510,
    "content": "492 Chapter 11. Search Trees\n11.4.2 When to Splay\nThe rules that dictate when splaying is performed are as follows:\n•When searching for key k, ifkis found at position p, we splay p, else we\nsplay the parent of the leaf position at which the search terminates unsuccess-\nfully. For example, the splaying in Figures 11.18 and 11.19 would be per-\nformed after searching successfully for key 14 or unsuccessfully for key 15.\n•When inserting key k, we splay the newly created internal node where k\ngets inserted. For example, the splaying in Figures 11.18 and 11.19 would\nbe performed if 14 were the newly inserted key. We show a sequence of\ninsertions in a splay tree in Figure 11.20.\n11\n33\n1\n(a) (b) (c)\n3\n1\n21 322\n1 3\n4\n(d) (e) (f)\n14\n3\n2\n(g)\nFigure 11.20: A sequence of insertions in a splay tree: (a) initial tree; (b) after\ninserting 3, but before a zig step; (c) after splaying; (d) after inserting 2, but before\na zig-zag step; (e) after splaying; (f) after inserting 4, but before a zig-zig step;\n(g) after splaying.\nwww.it-ebooks.info"
  },
  {
    "page": 511,
    "content": "11.4. Splay Trees 493\n•When deleting a key k, we splay the position pthat is the parent of the re-\nmoved node; recall that by the removal algorithm for binary search trees, the\nremoved node may be that originally containing k, or a descendant node with\na replacement key. An example of splaying following a deletion is shown inFigure 11.21.\n8\n10 3\n4\n567\nwp11 410 3\n11\n67\n5\n(a) (b)\n510 6\n11 4\n37\n510 6\n11 4\n37\n(c) (d)\n3 546\n7\n10\n11\n(e)\nFigure 11.21: Deletion from a splay tree: (a) the deletion of 8 from the root node\nis performed by moving to the root the key of its inorder predecessor w, deleting\nw, and splaying the parent pofw; (b) splaying pstarts with a zig-zig; (c) after the\nzig-zig; (d) the next step is a zig; (e) after the zig.\nwww.it-ebooks.info"
  },
  {
    "page": 512,
    "content": "494 Chapter 11. Search Trees\n11.4.3 Java Implementation\nAlthough the mathematical analysis of a splay tree’s performance is complex (see\nSection 11.4.4), the implementation of splay trees is a rather simple adaptation to\na standard binary search tree. Code Fragment 11.13 provides a complete imple-\nmentation of a SplayTreeMap class, based upon the underlying TreeMap class and\nuse of the balancing framework described in Section 11.2.1. Note that the original\nTreeMap class makes calls to the rebalanceAccess method, not just from within the\ngetmethod, but also within the putmethod when modifying the value associated\nwith an existing key, and within a failed remove operation.\n1/∗∗An implementation of a sorted map using a splay tree. ∗/\n2public class SplayTreeMap <K,V>extends TreeMap<K,V>{\n3/∗∗Constructs an empty map using the natural ordering of keys. ∗/\n4publicSplayTreeMap(){super();}\n5/∗∗Constructs an empty map using the given comparator to order keys. ∗/\n6publicSplayTreeMap(Comparator <K>comp){super(comp);}\n7/∗∗Utility used to rebalance after a map operation. ∗/\n8private void splay(Position <Entry<K,V>>p){\n9while(!isRoot(p)){\n10 Position<Entry<K,V>>parent = parent(p);\n11 Position<Entry<K,V>>grand = parent(parent);\n12 if(grand == null) // zig case\n13 rotate(p);\n14 else if((parent == left(grand)) == (p == left(parent))) {// zig-zig case\n15 rotate(parent); // move PARENT upward\n16 rotate(p); // then move p upward\n17}else{ // zig-zag case\n18 rotate(p); // move p upward\n19 rotate(p); // move p upward again\n20}\n21}\n22}\n23// override the various TreeMap rebalancing hooks to perform the appropriate splay\n24protected void rebalanceAccess(Position <Entry<K,V>>p){\n25if(isExternal(p)) p = parent(p);\n26if(p !=null) splay(p);\n27}\n28protected void rebalanceInsert(Position <Entry<K,V>>p){\n29splay(p);\n30}\n31protected void rebalanceDelete(Position <Entry<K,V>>p){\n32if(!isRoot(p)) splay(parent(p));\n33}\n34}\nCode Fragment 11.13: A complete implementation of the SplayTreeMap class.\nwww.it-ebooks.info"
  },
  {
    "page": 513,
    "content": "11.4. Splay Trees 495\n11.4.4 Amortized Analysis of Splaying ⋆\nAfter a zig-zig or zig-zag, the depth of position pdecreases by two, and after a zig\nthe depth of pdecreases by one. Thus, if phas depth d, splaying pconsists of a\nsequence of⌊d/2⌋zig-zigs and/or zig-zags, plus one ﬁnal zig if dis odd. Since a\nsingle zig-zig, zig-zag, or zig affects a constant number of nodes, it can be done in\nO(1)time. Thus, splaying a position pin a binary search tree Ttakes time O(d),\nwhere dis the depth of pinT. In other words, the time for performing a splaying\nstep for a position pis asymptotically the same as the time needed just to reach that\nposition in a top-down search from the root of T.\nWorst-Case Time\nIn the worst case, the overall running time of a search, insertion, or deletion in a\nsplay tree of height hisO(h), since the position we splay might be the deepest\nposition in the tree. Moreover, it is possible for hto be as large as n, as shown in\nFigure 11.20. Thus, from a worst-case point of view, a splay tree is not an attractive\ndata structure.\nIn spite of its poor worst-case performance, a splay tree performs well in an\namortized sense. That is, in a sequence of intermixed searches, insertions, and\ndeletions, each operation takes on average logarithmic time. We perform the amor-\ntized analysis of splay trees using the accounting method.\nAmortized Performance of Splay Trees\nFor our analysis, we note that the time for performing a search, insertion, or deletion\nis proportional to the time for the associated splaying. So let us consider only\nsplaying time.\nLetTbe a splay tree with nkeys, and let wbe a node of T. We deﬁne the\nsizen(w)ofwas the number of nodes in the subtree rooted at w. Note that this\ndeﬁnition implies that the size of an internal node is one more than the sum of the\nsizes of its children. We deﬁne the rank r(w)of a node was the logarithm in base 2\nof the size of w, that is, r(w)=log(n(w)). Clearly, the root of Thas the maximum\nsize, n, and the maximum rank, log n, while each leaf has size 1 and rank 0.\nWe use cyber-dollars to pay for the work we perform in splaying a position p\ninT, and we assume that one cyber-dollar pays for a zig, while two cyber-dollars\npay for a zig-zig or a zig-zag. Hence, the cost of splaying a position at depth dis\ndcyber-dollars. We keep a virtual account storing cyber-dollars at each position of\nT. Note that this account exists only for the purpose of our amortized analysis, and\ndoes not need to be included in a data structure implementing the splay tree T.\nwww.it-ebooks.info"
  },
  {
    "page": 514,
    "content": "496 Chapter 11. Search Trees\nAn Accounting Analysis of Splaying\nWhen we perform a splaying, we pay a certain number of cyber-dollars (the exact\nvalue of the payment will be determined at the end of our analysis). We distinguishthree cases:\n•If the payment is equal to the splaying work, then we use it all to pay for thesplaying.\n•If the payment is greater than the splaying work, we deposit the excess in theaccounts of several nodes.\n•If the payment is less than the splaying work, we make withdrawals from theaccounts of several nodes to cover the deﬁciency.\nWe show below that a payment of O(logn)cyber-dollars per operation is sufﬁcient\nto keep the system working, that is, to ensure that each node keeps a nonnegativeaccount balance.\nAn Accounting Invariant for Splaying\nWe use a scheme in which transfers are made between the accounts of the nodesto ensure that there will always be enough cyber-dollars to withdraw for paying forsplaying work when needed.\nIn order to use the accounting method to perform our analysis of splaying, we\nmaintain the following invariant:\nBefore and after a splaying, each node wofThasr(w)cyber-dollars\nin its account.\nNote that the invariant is “ﬁnancially sound,” since it does not require us to make apreliminary deposit to endow a tree with zero keys.\nLetr(T)be the sum of the ranks of all the nodes of T. To preserve the invariant\nafter a splaying, we must make a payment equal to the splaying work plus the total\nchange in r(T). We refer to a single zig, zig-zig, or zig-zag operation in a splaying\nas a splaying substep . Also, we denote the rank of a node wofTbefore and after\na splaying substep with r(w)andr\n′(w), respectively. The following proposition\ngives an upper bound on the change of r(T)caused by a single splaying substep.\nWe will repeatedly use this lemma in our analysis of a full splaying of a node to theroot.\nwww.it-ebooks.info"
  },
  {
    "page": 515,
    "content": "11.4. Splay Trees 497\nProposition 11.2: Letδbe the variation of r(T)caused by a single splaying sub-\nstep (a zig, zig-zig, or zig-zag) for a node xinT. We have the following:\n•δ≤3(r′(x)−r(x))−2if the substep is a zig-zig or zig-zag.\n•δ≤3(r′(x)−r(x))if the substep is a zig.\nJustiﬁcation: We use the fact that, if a>0,b>0, and c>a+b,\nloga+logb<2log c−2. (11.6)\nLet us consider the change in r(T)caused by each type of splaying substep.\nzig-zig :(Recall Figure 11.15.) Since the size of each node is one more than the\nsize of its two children, note that only the ranks of x,y, and zchange in a\nzig-zig operation, where yis the parent of xandzis the parent of y. Also,\nr′(x)=r(z),r′(y)≤r′(x), and r(x)≤r(y). Thus,\nδ=r′(x)+r′(y)+r′(z)−r(x)−r(y)−r(z)\n=r′(y)+r′(z)−r(x)−r(y)\n≤r′(x)+r′(z)−2r(x). (11.7)\nNote that n(x)+n′(z)<n′(x). Thus, r(x)+r′(z)<2r′(x)−2, as per For-\nmula 11.6; that is,\nr′(z)<2r′(x)−r(x)−2.\nThis inequality and Formula 11.7 imply\nδ≤r′(x)+(2r′(x)−r(x)−2)−2r(x)\n≤3(r′(x)−r(x))−2.\nzig-zag :(Recall Figure 11.16.) Again, by the deﬁnition of size and rank, only the\nranks of x,y, and zchange, where ydenotes the parent of xandzdenotes the\nparent of y. Also, r(x)<r(y)<r(z)=r′(x). Thus,\nδ=r′(x)+r′(y)+r′(z)−r(x)−r(y)−r(z)\n=r′(y)+r′(z)−r(x)−r(y)\n≤r′(y)+r′(z)−2r(x). (11.8)\nNote that n′(y)+n′(z)<n′(x); hence, r′(y)+r′(z)<2r′(x)−2, as per For-\nmula 11.6. Thus,\nδ≤2r′(x)−2−2r(x)\n=2(r′(x)−r(x))−2≤3(r′(x)−r(x))−2.\nzig:(Recall Figure 11.17.) In this case, only the ranks of xandychange, where y\ndenotes the parent of x. Also, r′(y)≤r(y)andr′(x)≥r(x). Thus,\nδ=r′(y)+r′(x)−r(y)−r(x)\n≤r′(x)−r(x)\n≤3(r′(x)−r(x)).\nwww.it-ebooks.info"
  },
  {
    "page": 516,
    "content": "498 Chapter 11. Search Trees\nProposition 11.3: LetTbe a splay tree with root t, and let ∆be the total variation\nofr(T)caused by splaying a node xat depth d. We have\n∆≤3(r(t)−r(x))−d+2.\nJustiﬁcation: Splaying node xconsists of c=⌈d/2⌉splaying substeps, each\nof which is a zig-zig or a zig-zag, except possibly the last one, which is a zig if\ndis odd. Let r0(x) =r(x)be the initial rank of x, and for i=1,..., c, letri(x)be\nthe rank of xafter the ithsubstep and δibe the variation of r(T)caused by the ith\nsubstep. By Proposition 11.2, the total variation ∆ofr(T)caused by splaying xis\n∆=c\n∑\ni=1δi\n≤2+c\n∑\ni=13(ri(x)−ri−1(x))−2\n=3(rc(x)−r0(x))−2c+2\n≤3(r(t)−r(x))−d+2.\nBy Proposition 11.3, if we make a payment of 3 (r(t)−r(x))+2 cyber-dollars\ntowards the splaying of node x, we have enough cyber-dollars to maintain the in-\nvariant, keeping r(w)cyber-dollars at each node winT, and pay for the entire\nsplaying work, which costs dcyber-dollars. Since the size of the root tisn, its\nrank r(t) =logn. Given that r(x)≥0, the payment to be made for splaying is\nO(logn)cyber-dollars. To complete our analysis, we have to compute the cost for\nmaintaining the invariant when a node is inserted or deleted.\nWhen inserting a new node winto a splay tree with nkeys, the ranks of all\nthe ancestors of ware increased. Namely, let w0,wi,..., wdbe the ancestors of w,\nwhere w0=w,wiis the parent of wi−1, and wdis the root. For i=1,..., d, let\nn′(wi)andn(wi)be the size of wibefore and after the insertion, respectively, and\nletr′(wi)andr(wi)be the rank of wibefore and after the insertion. We have\nn′(wi)=n(wi)+1.\nAlso, since n(wi)+1≤n(wi+1), for i=0,1,..., d−1, we have the following for\neach iin this range:\nr′(wi)=log(n′(wi))= log(n(wi)+1)≤log(n(wi+1))= r(wi+1).\nThus, the total variation of r(T)caused by the insertion is\nd\n∑\ni=1/parenleftbig\nr′(wi)−r(wi)/parenrightbig\n≤r′(wd)+d−1\n∑\ni=1(r(wi+1)−r(wi))\n=r′(wd)−r(w0)\n≤logn.\nTherefore, a payment of O(logn)cyber-dollars is sufﬁcient to maintain the invariant\nwhen a new node is inserted.\nwww.it-ebooks.info"
  },
  {
    "page": 517,
    "content": "11.4. Splay Trees 499\nWhen deleting a node wfrom a splay tree with nkeys, the ranks of all the an-\ncestors of ware decreased. Thus, the total variation of r(T)caused by the deletion\nis negative, and we do not need to make any payment to maintain the invariant\nwhen a node is deleted. Therefore, we may summarize our amortized analysis inthe following proposition (which is sometimes called the “balance proposition” forsplay trees):\nProposition 11.4:\nConsider a sequence of moperations on a splay tree, each one\na search, insertion, or deletion, starting from a splay tree with zero keys. Also, let\nnibe the number of keys in the tree after operation i, and nbe the total number of\ninsertions. The total running time for performing the sequence of operations is\nO/parenleftBigg\nm+m\n∑\ni=1logni/parenrightBigg\n,\nwhich is O(mlogn).\nIn other words, the amortized running time of performing a search, insertion,\nor deletion in a splay tree is O(logn), where nis the size of the splay tree at the\ntime. Thus, a splay tree can achieve logarithmic-time amortized performance for\nimplementing a sorted map ADT. This amortized performance matches the worst-case performance of A VL trees, (2,4)trees, and red-black trees, but it does so\nusing a simple binary tree that does not need any extra balance information storedat each of its nodes. In addition, splay trees have a number of other interestingproperties that are not shared by these other balanced search trees. We explore onesuch additional property in the following proposition (which is sometimes calledthe “Static Optimality” proposition for splay trees):\nProposition 11.5:\nConsider a sequence of moperations on a splay tree, each one\na search, insertion, or deletion, starting from a splay tree Twith zero keys. Also, let\nf(i)denote the number of times the entry iis accessed in the splay tree, that is, its\nfrequency, and let ndenote the total number of entries. Assuming that each entry is\naccessed at least once, then the total running time for performing the sequence of\noperations is\nO/parenleftBigg\nm+n\n∑\ni=1f(i)log(m/f(i))/parenrightBigg\n.\nWe omit the proof of this proposition, but it is not as hard to justify as one might\nimagine. The remarkable thing is that this proposition states that the amortized\nrunning time of accessing an entry iisO(log(m/f(i))).\nwww.it-ebooks.info"
  },
  {
    "page": 518,
    "content": "500 Chapter 11. Search Trees\n11.5 (2,4) Trees\nIn this section, we will consider a data structure known as a (2,4) tree . It is a\nparticular example of a more general structure known as a multiway search tree , in\nwhich internal nodes may have more than two children. Other forms of multiway\nsearch trees will be discussed in Section 15.3.\n11.5.1 Multiway Search Trees\nRecall that general trees are deﬁned so that internal nodes may have many children.\nIn this section, we discuss how general trees can be used as multiway search trees.\nMap entries stored in a search tree are pairs of the form (k,v), where kis the key\nandvis the value associated with the key.\nDeﬁnition of a Multiway Search Tree\nLetwbe a node of an ordered tree. We say that wis ad-node ifwhasdchildren.\nWe deﬁne a multiway search tree to be an ordered tree Tthat has the following\nproperties, which are illustrated in Figure 11.22a:\n•Each internal node of Thas at least two children. That is, each internal node\nis ad-node such that d≥2.\n•Each internal d-node wofTwith children c1,..., cdstores an ordered set of\nd−1 key-value pairs (k1,v1),...,(kd−1,vd−1), where k1≤···≤ kd−1.\n•Let us conventionally deﬁne k0=−∞ andkd=+∞. For each entry (k,v)\nstored at a node in the subtree of wrooted at ci,i=1,..., d, we have that\nki−1≤k≤ki.\nThat is, if we think of the set of keys stored at was including the special ﬁctitious\nkeys k0=−∞ andkd= +∞, then a key kstored in the subtree of Trooted at a\nchild node cimust be “in between” two keys stored at w. This simple viewpoint\ngives rise to the rule that a d-node stores d−1 regular keys, and it also forms the\nbasis of the algorithm for searching in a multiway search tree.\nBy the above deﬁnition, the external nodes of a multiway search do not store\nany data and serve only as “placeholders.” As with our convention for binary search\ntrees (Section 11.1), these can be replaced by nullreferences in practice. A binary\nsearch tree can be viewed as a special case of a multiway search tree, where each\ninternal node stores one entry and has two children.\nWhether internal nodes of a multiway tree have two children or many, however,\nthere is an interesting relationship between the number of key-value pairs and the\nnumber of external nodes in a multiway search tree.\nProposition 11.6: Ann-entry multiway search tree has n+1external nodes.\nWe leave the justiﬁcation of this proposition as an exercise (C-11.49).\nwww.it-ebooks.info"
  },
  {
    "page": 519,
    "content": "11.5. (2,4) Trees 501\n25\n11 136 8 27 23 24 3 4 145 1022\n17\n(a)\n6 85 1022\n25\n11 13 1723 24 27 3 4 14\n(b)\n2324\n1727 3 4 6 825\n11 13145 1022\n(c)\nFigure 11.22: (a) A multiway search tree T; (b) search path in Tfor key 12 (unsuc-\ncessful search); (c) search path in Tfor key 24 (successful search).\nwww.it-ebooks.info"
  },
  {
    "page": 520,
    "content": "502 Chapter 11. Search Trees\nSearching in a Multiway Tree\nSearching for an entry with key kin a multiway search tree Tis simple. We perform\nsuch a search by tracing a path in Tstarting at the root. (See Figure 11.22b and c.)\nWhen we are at a d-node wduring this search, we compare the key kwith the keys\nk1,..., kd−1stored at w. Ifk=kifor some i, the search is successfully completed.\nOtherwise, we continue the search in the child ciofwsuch that ki−1<k<ki.\n(Recall that we conventionally deﬁne k0=−∞ andkd= +∞.) If we reach an\nexternal node, then we know that there is no entry with key kinT, and the search\nterminates unsuccessfully.\nData Structures for Representing Multiway Search Trees\nIn Section 8.3.3, we discuss a linked data structure for representing a general tree.\nThis representation can also be used for a multiway search tree. When using ageneral tree to implement a multiway search tree, we must store at each node oneor more key-value pairs associated with that node. That is, we need to store with w\na reference to some collection that stores the entries for w.\nDuring a search for key kin a multiway search tree, the primary operation\nneeded when navigating a node is ﬁnding the smallest key at that node that is greaterthan or equal to k. For this reason, it is natural to model the information at a node\nitself as a sorted map, allowing use of the ceilingEntry( k)method. We say such\na map serves as a secondary data structure to support the primary data structure\nrepresented by the entire multiway search tree. This reasoning may at ﬁrst seemlike a circular argument, since we need a representation of a (secondary) orderedmap to represent a (primary) ordered map. We can avoid any circular dependence,however, by using the bootstrapping technique, where we use a simple solution to\na problem to create a new, more advanced solution.\nIn the context of a multiway search tree, a natural choice for the secondary\nstructure at each node is the SortedTableMap of Section 10.3.1. Because we want\nto determine the associated value in case of a match for key k, and otherwise the\ncorresponding child c\nisuch that ki−1<k<ki, we recommend having each key\nkiin the secondary structure map to the pair (vi,ci). With such a realization of a\nmultiway search tree T, processing a d-node wwhile searching for an entry of T\nwith key kcan be performed using a binary search operation in O(logd)time. Let\ndmaxdenote the maximum number of children of any node of T, and let hdenote the\nheight of T. The search time in a multiway search tree is therefore O(hlogdmax).\nIfdmaxis a constant, the running time for performing a search is O(h).\nThe primary efﬁciency goal for a multiway search tree is to keep the height\nas small as possible. We will next discuss a strategy that caps dmaxat 4 while\nguaranteeing a height hthat is logarithmic in n, the total number of entries stored\nin the map.\nwww.it-ebooks.info"
  },
  {
    "page": 521,
    "content": "11.5. (2,4) Trees 503\n11.5.2 (2,4)-Tree Operations\nOne form of a multiway search tree that keeps the tree balanced while using small\nsecondary data structures at each node is the (2,4)tree, also known as a 2-4 tree\nor 2-3-4 tree. This data structure achieves these goals by maintaining two simple\nproperties (see Figure 11.23):\nSize Property :Every internal node has at most four children.\nDepth Property :All the external nodes have the same depth.\n12\n17 11 6 7 8 3 45 10 15\n13 14\nFigure 11.23: A(2,4)tree.\nAgain, we assume that external nodes are empty and, for the sake of simplicity,\nwe describe our search and update methods assuming that external nodes are real\nnodes, although this latter requirement is not strictly needed.\nEnforcing the size property for (2,4)trees keeps the nodes in the multiway\nsearch tree simple. It also gives rise to the alternative name “2-3-4 tree,” since it\nimplies that each internal node in the tree has 2, 3, or 4 children. Another implica-\ntion of this rule is that we can represent the secondary map stored at each internal\nnode using an unordered list or an ordered array, and still achieve O(1)-time perfor-\nmance for all operations (since dmax=4). The depth property, on the other hand,\nenforces an important bound on the height of a (2,4)tree.\nProposition 11.7: The height of a (2,4)tree storing nentries is O(logn).\nJustiﬁcation: Lethbe the height of a (2,4)treeTstoring nentries. We justify\nthe proposition by showing the claim\n1\n2log(n+1)≤h≤log(n+1). (11.9)\nTo justify this claim note ﬁrst that, by the size property, we can have at most\n4 nodes at depth 1, at most 42nodes at depth 2, and so on. Thus, the number of\nexternal nodes in Tis at most 4h. Likewise, by the depth property and the deﬁnition\nwww.it-ebooks.info"
  },
  {
    "page": 522,
    "content": "504 Chapter 11. Search Trees\nof a(2,4)tree, we must have at least 2 nodes at depth 1, at least 22nodes at depth\n2, and so on. Thus, the number of external nodes in Tis at least 2h. In addition, by\nProposition 11.6, the number of external nodes in Tisn+1. Therefore, we obtain\n2h≤n+1≤4h.\nTaking the logarithm in base 2 of the terms for the above inequalities, we get that\nh≤log(n+1)≤2h,\nwhich justiﬁes our claim (Formula 11.9) when terms are rearranged.\nProposition 11.7 states that the size and depth properties are sufﬁcient for keep-\ning a multiway tree balanced. Moreover, this proposition implies that performing\na search in a (2,4)tree takes O(logn)time and that the speciﬁc realization of the\nsecondary structures at the nodes is not a crucial design choice, since the maximum\nnumber of children dmaxis a constant.\nMaintaining the size and depth properties requires some effort after performing\ninsertions and deletions in a (2,4)tree, however. We discuss these operations next.\nInsertion\nTo insert a new entry (k,v), with key k, into a(2,4)treeT, we ﬁrst perform a\nsearch for k. Assuming that Thas no entry with key k, this search terminates\nunsuccessfully at an external node z. Let wbe the parent of z. We insert the new\nentry into node wand add a new child y(an external node) to won the left of z.\nOur insertion method preserves the depth property, since we add a new external\nnode at the same level as existing external nodes. Nevertheless, it may violate the\nsize property. Indeed, if a node wwas previously a 4-node, then it would become\na 5-node after the insertion, which causes the tree Tto no longer be a (2,4)tree.\nThis type of violation of the size property is called an overﬂow at node w, and it\nmust be resolved in order to restore the properties of a (2,4)tree. Let c1,..., c5be\nthe children of w, and let k1,..., k4be the keys stored at w. To remedy the overﬂow\nat node w, we perform a split operation on was follows (see Figure 11.24):\n•Replace wwith two nodes w′andw′′, where\n◦w′is a 3-node with children c1,c2,c3storing keys k1andk2.\n◦w′′is a 2-node with children c4,c5storing key k4.\n•Ifwis the root of T, create a new root node u; else, let ube the parent of w.\n•Insert key k3intouand make w′andw′′children of u, so that if wwas child\niofu, then w′andw′′become children iandi+1 ofu, respectively.\nAs a consequence of a split operation on node w, a new overﬂow may occur at the\nparent uofw. If such an overﬂow occurs, it triggers in turn a split at node u. (See\nFigure 11.25.) A split operation either eliminates the overﬂow or propagates it into\nthe parent of the current node. We show a sequence of insertions in a (2,4)tree in\nFigure 11.26.\nwww.it-ebooks.info"
  },
  {
    "page": 523,
    "content": "11.5. (2,4) Trees 505\nh1h2\nc3 c2 c1 c5u\nw\nk1k2k3k4\nc4k3\nc3 c2 c1 c5w\nk1k2 k4\nc4u\nh1 h2\nw′\nc2 c1 c4 c5k1k2 k4h1k3h2u\nw′′\nc3\n(a) (b) (c)\nFigure 11.24: A node split: (a) overﬂow at a 5-node w; (b) the third key of winserted\ninto the parent uofw; (c) node wreplaced with a 3-node w′and a 2-node w′′.\n1312\n14 6 7 8 11 3 410 5\n15 151712\n14 6 7 8 11 3 410 5\n13\n(a) (b)\n6 7 8 11 13 14 17155 10 12\n3 4 13 14 17 11 6 7 8 3 45 10 12 15\n(c) (d)\n12\n13 14 17 11 6 7 8 3 45 10 15 15\n17 11 6 7 8 3 412\n5 10\n13 14\n(e) (f)\nFigure 11.25: An insertion in a (2,4)tree that causes a cascading split: (a) before\nthe insertion; (b) insertion of 17, causing an overﬂow; (c) a split; (d) after the split\na new overﬂow occurs; (e) another split, creating a new root node; (f) ﬁnal tree.\nwww.it-ebooks.info"
  },
  {
    "page": 524,
    "content": "506 Chapter 11. Search Trees\n4 46 6 412 6 12 4 15\n(a) (b) (c) (d)\n12\n4 6 1512\n4 15 6\n(e) (f)\n6 1512\n43 1512\n3 4 56\n(g) (h)\n1512\n3 45\n612\n15 4 35\n6\n(i) (j)\n1012\n3 15 6 45\n3 1512\n10 4 685\n(k) (l)\nFigure 11.26: A sequence of insertions into a (2,4)tree: (a) initial tree with one\nentry; (b) insertion of 6; (c) insertion of 12; (d) insertion of 15, which causes an\noverﬂow; (e) split, which causes the creation of a new root node; (f) after the split;(g) insertion of 3; (h) insertion of 5, which causes an overﬂow; (i) split; (j) after the\nsplit; (k) insertion of 10; (l) insertion of 8.\nwww.it-ebooks.info"
  },
  {
    "page": 525,
    "content": "11.5. (2,4) Trees 507\nAnalysis of Insertion in a (2,4) Tree\nBecause dmaxis at most 4, the original search for the placement of new key kuses\nO(1)time at each level, and thus O(logn)time overall, since the height of the tree\nisO(logn)by Proposition 11.7.\nThe modiﬁcations to a single node to insert a new key and child can be im-\nplemented to run in O(1)time, as can a single split operation. The number of\ncascading split operations is bounded by the height of the tree, and so that phase of\nthe insertion process also runs in O(logn)time. Therefore, the total time to perform\nan insertion in a (2,4)tree is O(logn).\nDeletion\nLet us now consider the removal of an entry with key kfrom a(2,4)treeT. We\nbegin such an operation by performing a search in Tfor an entry with key k. Re-\nmoving an entry from a (2,4)tree can always be reduced to the case where the entry\nto be removed is stored at a node wwhose children are external nodes. Suppose,\nfor instance, that the entry with key kthat we wish to remove is stored in the ith\nentry(ki,vi)at a node zthat has internal children. In this case, we swap the entry\n(ki,vi)with an appropriate entry that is stored at a node wwith external children as\nfollows (see Figure 11.27d):\n1.We ﬁnd the rightmost internal node win the subtree rooted at the ithchild of\nz, noting that the children of node ware all external nodes.\n2.We swap the entry (ki,vi)atzwith the last entry of w.\nOnce we ensure that the entry to remove is stored at a node wwith only external\nchildren (because either it was already at wor we swapped it into w), we simply\nremove the entry from wand remove the external node that is the ithchild of w.\nRemoving an entry (and a child) from a node was described above preserves\nthe depth property, for we always remove an external child from a node wwith only\nexternal children. However, in removing such an external node, we may violate the\nsize property at w. Indeed, if wwas previously a 2-node, then it becomes a 1-node\nwith no entries after the removal (Figure 11.27a and d), which is not allowed in\na(2,4)tree. This type of violation of the size property is called an underﬂow at\nnode w. To remedy an underﬂow, we check whether an immediate sibling of w\nis a 3-node or a 4-node. If we ﬁnd such a sibling s, then we perform a transfer\noperation, in which we move a child of stow, a key of sto the parent uofwands,\nand a key of utow. (See Figure 11.27b and c.) If whas only one sibling, or if both\nimmediate siblings of ware 2-nodes, then we perform a fusion operation, in which\nwe merge wwith a sibling, creating a new node w′, and move a key from the parent\nuofwtow′. (See Figure 11.27e and f.)\nwww.it-ebooks.info"
  },
  {
    "page": 526,
    "content": "508 Chapter 11. Search Trees\n6 8 17 13 141512\n1145 10 10\n815\n13 14 17 11u\nw12\n5\n6s\n(a) (b)\nw\n11 17 13 1415 10 6\n5 812\nsu\n13 14 8 56 10\n1712\n1511\n(c) (d)\n1710\nw11\n13 1415\n8 56u\n6\n13 14 8 10w′15\n17u11\n5\n(e) (f)\n6\n14 8 101315\n17 511\n8 1011\n17 146 15\n5\n(g) (h)\nFigure 11.27: A sequence of removals from a (2,4)tree: (a) removal of 4, causing\nan underﬂow; (b) a transfer operation; (c) after the transfer operation; (d) removal\nof 12, causing an underﬂow; (e) a fusion operation; (f) after the fusion operation;\n(g) removal of 13; (h) after removing 13.\nwww.it-ebooks.info"
  },
  {
    "page": 527,
    "content": "11.5. (2,4) Trees 509\nA fusion operation at node wmay cause a new underﬂow to occur at the parent\nuofw, which in turn triggers a transfer or fusion at u. (See Figure 11.28.) Hence,\nthe number of fusion operations is bounded by the height of the tree, which is\nO(logn)by Proposition 11.7. If an underﬂow propagates all the way up to the root,\nthen the root is simply deleted. (See Figure 11.28c and d.)\n11\n17 8 106\n51415 6\nw\n8 10 1711\nu\n515\n(a) (b)\n8 106u\nw\n17 15 511\n17 8 5 106 11\n15\n(c) (d)\nFigure 11.28: A propagating sequence of fusions in a (2,4)tree: (a) removal of 14,\nwhich causes an underﬂow; (b) fusion, which causes another underﬂow; (c) second\nfusion operation, which causes the root to be removed; (d) ﬁnal tree.\nPerformance of (2,4) Trees\nThe asymptotic performance of a (2,4)tree is identical to that of an A VL tree (see\nTable 11.2) in terms of the sorted map ADT, with guaranteed logarithmic boundsfor most operations. The time complexity analysis for a (2,4)tree having nkey-\nvalue pairs is based on the following:\n•The height of a (2,4)tree storing nentries is O(logn), by Proposition 11.7.\n•A split, transfer, or fusion operation takes O(1)time.\n•A search, insertion, or removal of an entry visits O(logn)nodes.\nThus,(2,4)trees provide for fast map search and update operations. (2,4)trees\nalso have an interesting relationship to the data structure we discuss next.\nwww.it-ebooks.info"
  },
  {
    "page": 528,
    "content": "510 Chapter 11. Search Trees\n11.6 Red-Black Trees\nAlthough A VL trees and (2,4)trees have a number of nice properties, they also\nhave some disadvantages. For instance, A VL trees may require many restructure\noperations (rotations) to be performed after a deletion, and (2,4)trees may require\nmany split or fusing operations to be performed after an insertion or removal. The\ndata structure we discuss in this section, the red-black tree, does not have these\ndrawbacks; it uses O(1)structural changes after an update in order to stay balanced.\nFormally, a red-black tree is a binary search tree (see Section 11.1) with nodes\ncolored red and black in a way that satisﬁes the following properties:\nRoot Property :The root is black.\nExternal Property :Every external node is black.\nRed Property :The children of a red node are black.\nDepth Property :All external nodes have the same black depth , deﬁned as the\nnumber of proper ancestors that are black.\nAn example of a red-black tree is shown in Figure 11.29.\n1413 171512\n5\n3\n4\n8 67 1110\nFigure 11.29: An example of a red-black tree, with “red” nodes drawn in white. The\ncommon black depth for this tree is 3.\nWe can make the red-black tree deﬁnition more intuitive by noting an inter-\nesting correspondence between red-black trees and (2,4)trees. Namely, given a\nred-black tree, we can construct a corresponding (2,4)tree by merging every red\nnode winto its parent, storing the entry from wat its parent, and with the chil-\ndren of wbecoming ordered children of the parent. For example, the red-black tree\nin Figure 11.29 corresponds to the (2,4)tree from Figure 11.23, as illustrated in\nFigure 11.30. The depth property of the red-black tree corresponds to the depth\nproperty of the (2,4)tree since exactly one black node of the red-black tree con-\ntributes to each node of the corresponding (2,4)tree.\nwww.it-ebooks.info"
  },
  {
    "page": 529,
    "content": "11.6. Red-Black Trees 511\n12\n15\n1117\n75\n613\n83\n4 1410\nFigure 11.30: An illustration of the correspondance between the red-black tree of\nFigure 11.29 and the (2,4)tree of Figure 11.23, based on the highlighted grouping\nof red nodes with their black parents.\nConversely, we can transform any (2,4)tree into a corresponding red-black tree\nby coloring each node wblack and then performing the following transformations,\nas illustrated in Figure 11.31.\n•Ifwis a 2-node, then keep the (black) children of was is.\n•Ifwis a 3-node, then create a new red node y, give w’s last two (black)\nchildren to y, and make the ﬁrst child of wandybe the two children of w.\n•Ifwis a 4-node, then create two new red nodes yandz, give w’s ﬁrst two\n(black) children to y, give w’s last two (black) children to z, and make yand\nzbe the two children of w.\nNotice that a red node always has a black parent in this construction.\n15\n←→15\n(a)\n13 14\n←→\n1314\n1413\nor\n(b)\n7 6 8\n←→7\n8 6(c)\nFigure 11.31: Correspondence between nodes of a (2,4)tree and a red-black tree:\n(a) 2-node; (b) 3-node; (c) 4-node.\nwww.it-ebooks.info"
  },
  {
    "page": 530,
    "content": "512 Chapter 11. Search Trees\nProposition 11.8: The height of a red-black tree storing nentries is O(logn).\nJustiﬁcation: LetTbe a red-black tree storing nentries, and let hbe the height\nofT. We justify this proposition by establishing the following fact:\nlog(n+1)≤h≤2log(n+1).\nLetdbe the common black depth of all the external nodes of T. Let T′be the\n(2,4)tree associated with T, and let h′be the height of T′. Because of the corre-\nspondence between red-black trees and (2,4)trees, we know that h′=d. Hence, by\nProposition 11.7, d=h′≤log(n+1). By the red property, h≤2d. Thus, we obtain\nh≤2log(n+1). The other inequality, log (n+1)≤h, follows from Proposition 8.7\nand the fact that Thasninternal nodes.\n11.6.1 Red-Black Tree Operations\nThe algorithm for searching in a red-black tree Tis the same as that for a standard\nbinary search tree (Section 11.1). Thus, searching in a red-black tree takes time\nproportional to the height of the tree, which is O(logn)by Proposition 11.8.\nThe correspondence between (2,4)trees and red-black trees provides important\nintuition that we will use in our discussion of how to perform updates in red-black\ntrees; in fact, the update algorithms for red-black trees can seem mysteriously com-\nplex without this intuition. Split and fuse operations of a (2,4)tree will be effec-\ntively mimicked by recoloring neighboring red-black tree nodes. A rotation within\na red-black tree will be used to change orientations of a 3-node between the two\nforms shown in Figure 11.31(b).\nInsertion\nConsider the insertion of a key-value pair (k,v)into a red-black tree T. The al-\ngorithm initially proceeds as in a standard binary search tree (Section 11.1.2).\nNamely, we search for kinTand if we reach an external node, we replace this\nnode with an internal node x, storing the entry and having two external children.\nIf this is the ﬁrst entry in T, and thus xis the root, we color it black. In all other\ncases, we color xred. That action corresponds to inserting (k,v)into a node of the\n(2,4)treeT′at the lowest internal level. The insertion preserves the root and depth\nproperties of T, but it may violate the red property. Indeed, if xis not the root of T\nand its parent yis red, then we have a parent and a child (namely, yandx) that are\nboth red. Note that by the root property, ycannot be the root of T, and by the red\nproperty (which was previously satisﬁed), the parent zofymust be black. Since x\nand its parent are red, but x’s grandparent zis black, we call this violation of the red\nproperty a double red at node x. To remedy a double red, we consider two cases.\nwww.it-ebooks.info"
  },
  {
    "page": 531,
    "content": "11.6. Red-Black Trees 513\nCase 1: The Sibling s of y is Black. (See Figure 11.32.) In this case, the double\nred denotes the fact that we have added the new node to a corresponding\n3-node of the (2,4)treeT′, effectively creating a malformed 4-node. This\nformation has one red node, y, that is the parent of another red node, x;\nwe want the two red nodes to be siblings instead. To ﬁx this problem, weperform a trinode restructuring ofT. The trinode restructuring (introduced\nin Section 11.2) is done by the operation restructure( x), which consists of\nthe following steps (see again Figure 11.32):\n•Take node x, its parent y, and grandparent z, and temporarily relabel\nthem as a,b, and c, in left-to-right order, so that a,b, and cwill be\nvisited in this order by an inorder tree traversal.\n•Replace the grandparent zwith the node labeled b, and make nodes a\nandcthe children of b, keeping inorder relationships unchanged.\nAfter performing the restructure( x)operation, we color bblack and we color\naandcred. Thus, the restructuring eliminates the double-red problem. No-\ntice that the portion of any path through the restructured part of the tree is\nincident to exactly one black node, both before and after the trinode restruc-\nturing. Therefore, the black depth of the tree is unaffected.\nz\nsxy\n102030\nxz\ny\ns1030\n20y\nxz\ns\n302010\nxz\ny\ns3010\n20\n(a)\nb\na c\n10 3020\n(b)\nFigure 11.32: Restructuring a red-black tree to remedy a double red: (a) the four\nconﬁgurations for x,y, and zbefore restructuring; (b) after restructuring.\nwww.it-ebooks.info"
  },
  {
    "page": 532,
    "content": "514 Chapter 11. Search Trees\nCase 2: The Sibling s of y is Red .(See Figure 11.33.) In this case, the double red\ndenotes an overﬂow in the corresponding (2,4)treeT′. To ﬁx the problem,\nwe perform the equivalent of a split operation. Namely, we do a recoloring :\nwe color yandsblack and their parent zred (unless zis the root, in which\ncase, it remains black). Notice that unless zis the root, the portion of any\npath through the affected part of the tree is incident to exactly one black\nnode, both before and after the recoloring. Therefore, the black depth of the\ntree is unaffected by the recoloring unless zis the root, in which case it is\nincreased by one.\nHowever, it is possible that the double-red problem reappears after such a\nrecoloring, albeit higher up in the tree T, since zmay have a red parent. If\nthe double-red problem reappears at z, then we repeat the consideration of the\ntwo cases at z. Thus, a recoloring either eliminates the double-red problem\nat node x, or propagates it to the grandparent zofx. We continue going\nupTperforming recolorings until we ﬁnally resolve the double-red problem\n(with either a ﬁnal recoloring or a trinode restructuring). Thus, the numberof recolorings caused by an insertion is no more than half the height of treeT, that is, O(logn)by Proposition 11.8.\nz\ny\nxs\n1020 4030\n10 20 30 40\n(a)\nz\ny\nxs30\n20 40\n10 40 10 20. . . 30 . . .\n(b)\nFigure 11.33: Recoloring to remedy the double-red problem: (a) before recoloring\nand the corresponding 5-node in the associated (2,4)tree before the split; (b) after\nrecoloring and the corresponding nodes in the associated (2,4)tree after the split.\nAs further examples, Figures 11.34 and 11.35 show a sequence of insertion\noperations in a red-black tree.\nwww.it-ebooks.info"
  },
  {
    "page": 533,
    "content": "11.6. Red-Black Trees 515\n4\n74\n124\n77\n12 4\n(a) (b) (c) (d)\n7\n12 4\n15 157\n4 127\n15 312 4 12 47\n3 15 5\n(e) (f) (g) (h)\n144\n3 5 157\n127\n4\n3 5 1214\n15\n(i) (j)\n3\n181514\n12 57\n4\n3\n181514\n12 57\n4\n(k) (l)\nFigure 11.34: A sequence of insertions in a red-black tree: (a) initial tree; (b) inser-\ntion of 7; (c) insertion of 12, which causes a double red; (d) after restructuring; (e)\ninsertion of 15, which causes a double red; (f) after recoloring (the root remainsblack); (g) insertion of 3; (h) insertion of 5; (i) insertion of 14, which causes adouble red; (j) after restructuring; (k) insertion of 18, which causes a double red;\n(l) after recoloring. (Continues in Figure 11.35.)\nwww.it-ebooks.info"
  },
  {
    "page": 534,
    "content": "516 Chapter 11. Search Trees\n34\n16514\n12\n187\n15 34\n15514\n12\n187\n16\n(m) (n)\n4 14\n5 16\n1812\n1537\n171214\n157\n174\n16 5 3\n18\n(o) (p)\n18 127\n1715\n516\n3414\n(q)\nFigure 11.35: A sequence of insertions in a red-black tree (continued from Fig-\nure 11.34): (m) insertion of 16, which causes a double red; (n) after restructuring;\n(o) insertion of 17, which causes a double red; (p) after recoloring there is again a\ndouble red, to be handled by a restructuring; (q) after restructuring.\nwww.it-ebooks.info"
  },
  {
    "page": 535,
    "content": "11.6. Red-Black Trees 517\nDeletion\nDeleting an entry with key kfrom a red-black tree Tinitially proceeds as for a bi-\nnary search tree (Section 11.1.2). Structurally, the process results in the removal of\nan internal node (either that originally containing key kor its inorder predecessor)\ntogether with a child that is external, and the promotion of its other child.\nIf the removed internal node was red, this structural change does not affect the\nblack depths of any paths in the tree, nor introduce any red violations, and so the\nresulting tree remains a valid red-black tree. In the corresponding (2,4)treeT′, this\ncase denotes the shrinking of a 4-node or 3-node. If the removed internal node was\nblack, it must have had black height 1, and therefore either both of its children wereexternal, or it had one red child that was an internal node with two external children.In the latter case, the removed node represents the black part of a corresponding 3-\nnode, and we restore the red-black properties by recoloring the promoted child to\nbe black.\nThe most complex case is when the removed node was black and had two exter-\nnal children. In the corresponding (2,4)tree, this denotes the removal of an entry\nfrom a 2-node. Without rebalancing, such a change results in a deﬁcit of one forthe black depth of the external position pthat is the promoted child of the deleted\ninternal node. To preserve the depth property, we temporarily assign the promoted\nleaf a ﬁctitious double black color. A double black in Tdenotes an underﬂow in\nthe corresponding (2,4)treeT\n′. To remedy a double-black problem at an arbitrary\nposition p, we will consider three cases.\nCase 1: The Sibling y of p is Black and has a Red Child x. (See Figure 11.36.)\nWe perform a trinode restructuring , as originally described in Section 11.2.\nThe operation restructure (x)takes the node x, its parent y, and grandparent\nz, labels them temporarily left to right as a,b, and c, and replaces zwith the\nnode labeled b, making it the parent of the other two. We color aandcblack,\nand give bthe former color of z.\nNotice that the path to pin the result includes one additional black node af-\nter the restructure, while the number of black nodes on paths to any of theother three subtrees illustrated in Figure 11.36 remains unchanged. There-\nfore, we return pto be colored (regular) black, and the double-black problem\nis eliminated.\nResolving this case corresponds to a transfer operation in the (2,4)treeT\n′\nbetween two children of node z. The fact that yhas a red child assures us\nthat it represents either a 3-node or a 4-node. In effect, the entry previously\nstored at zis demoted to become a new 2-node to resolve the deﬁciency,\nwhile an entry stored at yor its child is promoted to take the place of the\nentry previously stored at z.\nwww.it-ebooks.info"
  },
  {
    "page": 536,
    "content": "518 Chapter 11. Search Trees\nxpz\ny\n204030\n10 20. . . 30 . . .\n4010\n(a)\nxpz\ny\n104030\n10 20. . . 30 . . .\n4020\n(b)\nb\na c\np\n401030\n401020\n30. . . 20 . . .\n(c)\nFigure 11.36: Restructuring of a red-black tree to remedy the double-black problem:\n(a) and (b) conﬁgurations before the restructuring, where pis a right child and\nthe associated nodes in the corresponding (2,4)tree before the transfer (two other\nsymmetric conﬁgurations where pis a left child are possible); (c) conﬁguration\nafter the restructuring and the associated nodes in the corresponding (2,4)tree\nafter the transfer. The gray color for node zin parts (a) and (b) and for node bin\npart (c) denotes the fact that this node may be colored either red or black.\nwww.it-ebooks.info"
  },
  {
    "page": 537,
    "content": "11.6. Red-Black Trees 519\nCase 2: The Sibling y of p is Black and Both Children of y are Black.\nWe do a recoloring , beginning by changing the color of pfrom double black\nto black and the color of yfrom black to red. This does not create any red\nviolation, because both children of yare black. To counteract the decrease in\nblack depth for paths passing through yorp, we consider the common parent\nofpandy, which we denote as z. Ifzis red, we color it black and the problem\nhas been resolved (see Figure 11.37a). If zis black, we color it double black ,\nthereby propagating the problem higher up the tree (see Figure 11.37b).\nResolving this case corresponds to a fusion operation in the corresponding\n(2,4)treeT′, asymust represent a 2-node. The case where the problem\npropagates upward is when parent zalso represents a 2-node.\nz\ny pz\ny p\n2030\n4030\n20 40\n(a)\nz\ny p yz\np\n2030\n4030\n20 40\n(b)\nFigure 11.37: A recoloring operation, which has neutral effect on the black depth\nfor paths: (a) when zis originally red, the recoloring resolves the double-black\nproblem, ending the process; (b) when zis originally black, it becomes double-\nblack, requiring a cascading remedy.\nCase 3: Sibling y of p is Red . (See Figure 11.38.)\nLetzdenote the common parent of yandp, and note that zmust be black,\nbecause yis red. The combination of yandzrepresents a 3-node in the\ncorresponding (2,4)treeT′. In this case, we perform a rotation about yand\nz, and then recolor yblack and zred. This denotes a reorientation of a 3-node\nin the corresponding (2,4)treeT′.\nWe now reconsider the double-black problem at p. After the adjustment,\nthe sibling of pis black, and either Case 1 or Case 2 applies. Furthermore,\nthe next application will be the last, because Case 1 is always terminal and\nCase 2 will be terminal given that the parent of pis now red.\nwww.it-ebooks.info"
  },
  {
    "page": 538,
    "content": "520 Chapter 11. Search Trees\nz\ny p pzy\n403020\n4030\n20\nFigure 11.38: A rotation and recoloring about red node yand black node zin the\npresence of a double-black problem (a symmetric conﬁguration is possible). This\namounts to a change of orientation in the corresponding 3-node of a (2,4)tree.\nThis operation does not affect the black depth of any paths through this portion ofthe tree, but after the operation, one of the other resolutions to the double-black\nproblem may be applied, as the sibling of pwill be black.\nIn Figure 11.39, we show a sequence of deletions on a red-black tree. We\nillustrate a Case 1 restructuring in parts (c) and (d). We illustrate a Case 2 recoloring\nin parts (f) and (g). Finally, we show an example of a Case 3 rotation between parts(i) and (j), concluding with a Case 2 recoloring in part (k).\nPerformance of Red-Black Trees\nThe asymptotic performance of a red-black tree is identical to that of an A VL treeor a(2,4)tree in terms of the sorted map ADT, with guaranteed logarithmic time\nbounds for most operations. (See Table 11.2 for a summary of the A VL perfor-\nmance.) The primary advantage of a red-black tree is that an insertion or deletion\nrequires only a constant number of restructuring operations . (This is in contrast\nto A VL trees and (2,4)trees, both of which require a logarithmic number of struc-\ntural changes per map operation in the worst case.) That is, an insertion or deletionin a red-black tree requires logarithmic time for a search, and may require a loga-\nrithmic number of recoloring operations that cascade upward. We formalize these\nfacts with the following propositions.\nProposition 11.9:\nThe insertion of an entry in a red-black tree storing nentries\ncan be done in O(logn)time and requires O(logn)recolorings and at most one\ntrinode restructuring.\nProposition 11.10: The algorithm for deleting an entry from a red-black tree\nwith nentries takes O(logn)time and performs O(logn)recolorings and at most\ntwo restructuring operations.\nThe proofs of these propositions are left as Exercises R-11.26 and R-11.27.\nwww.it-ebooks.info"
  },
  {
    "page": 539,
    "content": "11.6. Red-Black Trees 521\n18 127\n1715\n516\n3414\n16 7\n4 12 18 15\n5 1714\n(a) (b)\n4 18 15\n5 171614\n7\n75\n15\n1714\n1816\n4\n(c) (d)\n185\n4 714\n1516514\n416\n7 155\n1514\n16\n7 4\n(e) (f) (g)\n514\n716\n4 414\n75 14\n75\n45\n4\n714\n(h) (i) (j) (k)\nFigure 11.39: A sequence of deletions from a red-black tree: (a) initial tree; (b) re-\nmoval of 3; (c) removal of 12, causing a black deﬁcit to the right of 7 (handled by\nrestructuring); (d) after restructuring; (e) removal of 17; (f) removal of 18, causing\na black deﬁcit to the right of 16 (handled by recoloring); (g) after recoloring; (h) re-\nmoval of 15; (i) removal of 16, causing a black deﬁcit to the right of 14 (handledinitially by a rotation); (j) after the rotation the black deﬁcit needs to be handled bya recoloring; (k) after the recoloring.\nwww.it-ebooks.info"
  },
  {
    "page": 540,
    "content": "522 Chapter 11. Search Trees\n11.6.2 Java Implementation\nIn this section, we will provide an implementation of a RBTreeMap class that in-\nherits from the standard TreeMap class and relies on the balancing framework de-\nscribed in Section 11.2.1. In that framework, each node stores an auxiliary integer\nthat can be used for maintaining balance information. For a red-black tree, we use\nthat integer to represent color, choosing to let value 0 (the default) designate the\ncolor black, and value 1 the color red; with this convention, any newly created leaf\nin the tree will be black.\nOur implementation begins in Code Fragment 11.14, with constructors for an\nempty map, and a series of convenient utilities for managing the auxiliary ﬁeld to\nrepresent color information. That code fragment continues with support for rebal-\nancing the tree after an insertion is performed. When an entry has been inserted in a\ntree by the standard search-tree algorithm, it will be stored at a previously external\nnode that was converted to an internal node with two new external children. The\nrebalanceInsert hook is then called, allowing us the opportunity to modify the tree.\nExcept for the special case where the new element is at the root, we change the color\nof the node with the new element to red (it had been black when a leaf), and then we\nconsider the possibility that we have a double-red violation. The resolveRed utility\nclosely follows the case analysis described in Section 11.6.1, recurring in the case\nwhen the red violation is propagated upward.\nCode Fragment 11.15 manages the rebalancing process after a deletion, based\nupon the case analysis described in Section 11.6.1. If the removed node was red,\nthen no other action is necessary; however, if the removed node was black, we\nmust consider a way to restore the depth property. An additional challenge is that\nby the time the rebalanceDelete method is called, a node has already been removed\nfrom the tree (this hook is invoked on the promoted child of that removed node).\nFortunately, we can infer the properties of the removed node based upon the red-\nblack tree properties, which were satisﬁed before the deletion.\nIn particular, let pdenote the promoted child of the removed node. If a black\nnode with a red child has been deleted, then pwill be that red child; we remedy\nthis by coloring pblack. Otherwise, if pis not the root, let sdenote the removed\nnode’s sibling (which will appear as p’s sibling after the deletion). If the deleted\nnode was black with two black children, we must treat pas adouble black node to\nbe remedied. This is the case if, and only if, its sibling’s subtree has a black internal\nnode (because the red-black depth property was satisﬁed prior to the deletion). We\ntherefore test whether sis a black internal node, or a red internal node with an\ninternal node as a child (which must be black due to the red property of the tree).\nWe are able to detect the double-black problem within the rebalanceDelete\nmethod of Code Fragment 11.15, and we rely on the recursive remedyDoubleBlack\nmethod of that code fragment to resolve the problem.\nwww.it-ebooks.info"
  },
  {
    "page": 541,
    "content": "11.6. Red-Black Trees 523\n1/∗∗An implementation of a sorted map using a red-black tree. ∗/\n2public class RBTreeMap <K,V>extends TreeMap<K,V>{\n3/∗∗Constructs an empty map using the natural ordering of keys. ∗/\n4publicRBTreeMap(){super();}\n5/∗∗Constructs an empty map using the given comparator to order keys. ∗/\n6publicRBTreeMap(Comparator <K>comp){super(comp);}\n7// we use the inherited aux ﬁeld with convention that 0=black and 1=red\n8// (note that new leaves will be black by default, as aux=0)\n9private boolean isBlack(Position <Entry<K,V>>p){returntree.getAux(p)==0;}\n10private boolean isRed(Position <Entry<K,V>>p){returntree.getAux(p)==1; }\n11private void makeBlack(Position <Entry<K,V>>p){tree.setAux(p, 0); }\n12private void makeRed(Position <Entry<K,V>>p){tree.setAux(p, 1); }\n13private void setColor(Position <Entry<K,V>>p,boolean toRed){\n14tree.setAux(p, toRed ? 1 : 0);\n15}\n16/∗∗Overrides the TreeMap rebalancing hook that is called after an insertion. ∗/\n17protected void rebalanceInsert(Position <Entry<K,V>>p){\n18if(!isRoot(p)){\n19 makeRed(p); // the new internal node is initially colored red\n20 resolveRed(p); // but this may cause a double-red problem\n21}\n22}\n23/∗∗Remedies potential double-red violation above red position p. ∗/\n24private void resolveRed(Position <Entry<K,V>>p){\n25Position<Entry<K,V>>parent,uncle,middle,grand; // used in case analysis\n26parent = parent(p);\n27if(isRed(parent)){ // double-red problem exists\n28 uncle = sibling(parent);\n29 if(isBlack(uncle)){ // Case 1: misshapen 4-node\n30 middle = restructure(p); // do trinode restructuring\n31 makeBlack(middle);\n32 makeRed(left(middle));\n33 makeRed(right(middle));\n34}else{ // Case 2: overfull 5-node\n35 makeBlack(parent); // perform recoloring\n36 makeBlack(uncle);\n37 grand = parent(parent);\n38 if(!isRoot(grand)){\n39 makeRed(grand); // grandparent becomes red\n40 resolveRed(grand); // recur at red grandparent\n41}\n42}\n43}\n44}\nCode Fragment 11.14: TheRBTreeMap class. (Continues in Code Frag-\nment 11.15.)\nwww.it-ebooks.info"
  },
  {
    "page": 542,
    "content": "524 Chapter 11. Search Trees\n45/∗∗Overrides the TreeMap rebalancing hook that is called after a deletion. ∗/\n46protected void rebalanceDelete(Position <Entry<K,V>>p){\n47if(isRed(p)) // deleted parent was black\n48 makeBlack(p); // so this restores black depth\n49else if(!isRoot(p)){\n50 Position<Entry<K,V>>sib = sibling(p);\n51 if(isInternal(sib) && (isBlack(sib) ||isInternal(left(sib))))\n52 remedyDoubleBlack(p); // sib's subtree has nonzero black height\n53}\n54}\n55\n56/∗∗Remedies a presumed double-black violation at the given (nonroot) position. ∗/\n57private void remedyDoubleBlack(Position <Entry<K,V>>p){\n58Position<Entry<K,V>>z = parent(p);\n59Position<Entry<K,V>>y = sibling(p);\n60if(isBlack(y)){\n61 if(isRed(left(y))||isRed(right(y))){ // Case 1: trinode restructuring\n62 Position<Entry<K,V>>x = (isRed(left(y)) ? left(y) : right(y));\n63 Position<Entry<K,V>>middle = restructure(x);\n64 setColor(middle, isRed(z)); // root of restructured subtree gets z' s old color\n65 makeBlack(left(middle));\n66 makeBlack(right(middle));\n67}else{ // Case 2: recoloring\n68 makeRed(y);\n69 if(isRed(z))\n70 makeBlack(z); // problem is resolved\n71 else if(!isRoot(z))\n72 remedyDoubleBlack(z); // propagate the problem\n73}\n74}else{ // Case 3: reorient 3-node\n75 rotate(y);\n76 makeBlack(y);\n77 makeRed(z);\n78 remedyDoubleBlack(p); // restart the process at p\n79}\n80}\n81}\nCode Fragment 11.15: Support for deletion in the RBTreeMap class (continued\nfrom Code Fragment 11.14).\nwww.it-ebooks.info"
  },
  {
    "page": 543,
    "content": "11.7. Exercises 525\n11.7 Exercises\nReinforcement\nR-11.1 If we insert the entries (1,A),(2,B),(3,C),(4,D), and(5,E), in this order, into\nan initially empty binary search tree, what will it look like?\nR-11.2 Insert, into an empty binary search tree, entries with keys 30, 40, 24, 58, 48, 26,\n11, 13 (in this order). Draw the tree after each insertion.\nR-11.3 How many different binary search trees can store the keys {1,2,3}?\nR-11.4 Dr. Amongus claims that the order in which a ﬁxed set of entries is inserted into\na binary search tree does not matter—the same tree results every time. Give a\nsmall example that proves he is wrong.\nR-11.5 Dr. Amongus claims that the order in which a ﬁxed set of entries is inserted into\nan A VL tree does not matter—the same A VL tree results every time. Give a small\nexample that proves he is wrong.\nR-11.6 Our implementation of the treeSearch utility, from Code Fragment 11.3, relies\non recursion. For a large unbalanced tree, it is possible that Java’s call stack will\nreach its limit due to the recursive depth. Give an alternative implementation of\nthat method that does not rely on the use of recursion.\nR-11.7 Does the trinode restructuring in Figure 11.11 rely on a single or double rotation?\nWhat about the restructuring in Figure 11.13?\nR-11.8 Draw the A VL tree resulting from the insertion of an entry with key 52 into the\nA VL tree of Figure 11.13b.\nR-11.9 Draw the A VL tree resulting from the removal of the entry with key 62 from the\nA VL tree of Figure 11.13b.\nR-11.10 Explain why performing a rotation in an n-node binary tree when using the array-\nbased representation of Section 8.3.2 takes Ω(n)time.\nR-11.11 Consider a deletion operation in an A VL tree that triggers a trinode restructuring\nfor the case in which both children of the node denoted as yhave equal heights.\nGive a schematic ﬁgure, in the style of Figure 11.12, showing the tree before and\nafter the deletion. What is the net effect of the height of the rebalanced subtree\ndue to the operation?\nR-11.12 Repeat the previous problem, considering the case in which y’s children start with\ndifferent heights.\nR-11.13 The rules for a deletion in an A VL tree speciﬁcally require that when the two\nsubtrees of the node denoted as yhave equal height, child xshould be chosen to be\n“aligned” with y(so that xandyare both left children or both right children). To\nbetter understand this requirement, repeat Exercise R-11.11 assuming we picked\nthe misaligned choice of x. Why might there be a problem in restoring the A VL\nproperty with that choice?\nwww.it-ebooks.info"
  },
  {
    "page": 544,
    "content": "526 Chapter 11. Search Trees\nR-11.14 What does a splay tree look like if its entries are accessed in increasing order by\ntheir keys?\nR-11.15 Perform the following sequence of operations in an initially empty splay tree and\ndraw the tree after each set of operations.\na.Insert keys 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, in this order.\nb.Search for keys 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, in this order.\nc.Delete keys 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, in this order.\nR-11.16 The splay tree does not have good performance for the sorted map operations,because those methods lack calls to the rebalanceAccess hook. Reimplement\nTreeMap to include such calls.\nR-11.17 Is the search tree of Figure 11.22(a) a (2,4)tree? Why or why not?\nR-11.18 An alternative way of performing a split at a node win a(2,4)tree is to partition\nwintow\n′andw′′, with w′being a 2-node and w′′a 3-node. Which of the keys\nk1,k2,k3, ork4do we store at w’s parent? Why?\nR-11.19 Dr. Amongus claims that a (2,4)tree storing a set of entries will always have the\nsame structure, regardless of the order in which the entries are inserted. Showthat he is wrong.\nR-11.20 Draw four different red-black trees that correspond to the same (2,4)tree.\nR-11.21 Consider the set of keys K={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}.\na.Draw a(2,4)tree storing Kas its keys using the fewest number of nodes.\nb.Draw a(2,4)tree storing Kas its keys using the greatest number of nodes.\nR-11.22 Consider the sequence of keys (5,16,22,45,2,10,18,30,50,12,1). Draw the\nresult of inserting entries with these keys (in the given order) into\na.An initially empty (2,4)tree.\nb.An initially empty red-black tree.\nR-11.23 For the following statements about red-black trees, provide a justiﬁcation for\neach true statement and a counterexample for each false one.\na.A subtree of a red-black tree is itself a red-black tree.\nb.The sibling of an external node is either external or it is red.\nc.There is a unique (2,4)tree associated with a given red-black tree.\nd.There is a unique red-black tree associated with a given (2,4)tree.\nR-11.24 Consider a tree Tstoring 100,000 entries. What is the worst-case height of Tin\nthe following cases?\na.Tis a binary search tree.\nb.Tis an A VL tree.\nc.Tis a splay tree.\nd.Tis a(2,4)tree.\ne.Tis a red-black tree.\nR-11.25 Draw an example of a red-black tree that is not an A VL tree.\nR-11.26 Give a proof of Proposition 11.9\nR-11.27 Give a proof of Proposition 11.10\nwww.it-ebooks.info"
  },
  {
    "page": 545,
    "content": "11.7. Exercises 527\nCreativity\nC-11.28 Explain why you would get the same output in an inorder listing of the entries\nin a binary search tree, T, independent of whether Tis maintained to be an A VL\ntree, splay tree, or red-black tree.\nC-11.29 Explain how to use an A VL tree or a red-black tree to sort ncomparable elements\ninO(nlogn)time in the worst case.\nC-11.30 Can we use a splay tree to sort ncomparable elements in O(nlogn)time in the\nworst case ? Why or why not?\nC-11.31 Implement a putIfAbsent method, as originally described in Exercise C-10.33,\nfor theTreeMap class.\nC-11.32 Show that any n-node binary tree can be converted to any other n-node binary\ntree using O(n)rotations.\nC-11.33 For a key kthat is not found in binary search tree T, prove that both the greatest\nkey less than kand the least key greater than klie on the path traced by the search\nfork.\nC-11.34 In Section 11.1.4 we claim that the subMap method of a binary search tree, as\nimplemented in Code Fragment 11.6, executes in O(s+h)time where sis the\nnumber of entries contained within the submap and his the height of the tree.\nProve this result, by arguing about the maximum number of times the recursive\nsubmethod can be called on positions that are not included within the submap.\nC-11.35 Consider a sorted map that is implemented with a standard binary search tree T.\nDescribe how to perform an operation removeSubMap( k1,k2)that removes all\nthe entries whose keys fall within subMap( k1,k2), in worst-case time O(s+h),\nwhere sis the number of entries removed and his the height of T.\nC-11.36 Repeat the previous problem using an A VL tree, achieving a running time of\nO(slogn). Why doesn’t the solution to the previous problem trivially result in an\nO(s+logn)algorithm for A VL trees?\nC-11.37 Suppose we wish to support a new method countRange( k1,k2)that determines\nhow many keys of a sorted map fall in the speciﬁed range. We could clearly\nimplement this in O(s+h)time by adapting our approach to subMap . De-\nscribe how to modify the search-tree structure to support O(h)worst-case time\nforcountRange .\nC-11.38 If the approach described in the previous problem were implemented as part of\ntheTreeMap class, what additional modiﬁcations (if any) would be necessary to a\nsubclass such as AVLTreeMap in order to maintain support for the new method?\nC-11.39 Draw a schematic of an A VL tree such that a single remove operation could\nrequire Ω(logn)trinode restructurings (or rotations) from a leaf to the root in\norder to restore the height-balance property.\nC-11.40 Show that the nodes that become temporarily unbalanced in an A VL tree during\nan insertion may be nonconsecutive on the path from the newly inserted node to\nthe root.\nwww.it-ebooks.info"
  },
  {
    "page": 546,
    "content": "528 Chapter 11. Search Trees\nC-11.41 Show that at most one node in an A VL tree becomes temporarily unbalanced after\nthe immediate deletion of a node as part of the standard remove map operation.\nC-11.42 In our A VL implementation, each node stores the height of its subtree, which isan arbitrarily large integer. The space usage for an A VL tree can be reduced by\ninstead storing the balance factor of a node, which is deﬁned as the height of its\nleft subtree minus the height of its right subtree. Thus, the balance factor of anode is always equal to −1, 0, or 1, except during an insertion or removal, when\nit may become temporarily equal to−2 or+2. Reimplement the AVLTreeMap\nclass storing balance factors rather than subtree heights.\nC-11.43 If we maintain a reference to the position of the leftmost node of a binary search\ntree, then operation ﬁrstEntry can be performed in O(1)time. Describe how\nthe implementation of the other map methods need to be modiﬁed to maintain areference to the leftmost position.\nC-11.44 If the approach described in the previous problem were implemented as part oftheTreeMap class, what additional modiﬁcations (if any) would be necessary to\na subclass such as AVLTreeMap in order to accurately maintain the reference to\nthe leftmost position?\nC-11.45 Describe a modiﬁcation to the binary search-tree data structure that would sup-port the following two index-based operations for a sorted map in O(h)time,\nwhere his the height of the tree.\natIndex( i):Return the position pof the entry at index iof a sorted map.\nindexOf( p):Return the index iof the entry at position pof a sorted map.\nC-11.46 Draw a splay tree, T\n1, together with the sequence of updates that produced it, and\na red-black tree, T2, on the same set of ten entries, such that a preorder traversal\nofT1would be the same as a preorder traversal of T2.\nC-11.47 LetTandUbe(2,4)trees storing nandmentries, respectively, such that all\nthe entries in Thave keys less than the keys of all the entries in U. Describe an\nO(logn+logm)-time method for joining TandUinto a single tree that stores\nall the entries in TandU.\nC-11.48 LetTbe a red-black tree storing nentries, and let kbe the key of an entry in T.\nShow how to construct from T, inO(logn)time, two red-black trees T′andT′′,\nsuch that T′contains all the keys of Tless than k, and T′′contains all the keys of\nTgreater than k. This operation destroys T.\nC-11.49 Prove that an n-entry multiway search tree has n+1 external nodes.\nC-11.50 The boolean indicator used to mark nodes in a red-black tree as being “red” or“black” is not strictly needed when we have distinct keys. Describe a scheme for\nimplementing a red-black tree without adding any extra space to standard binary\nsearch-tree nodes.\nC-11.51 Show that the nodes of any A VL tree Tcan be colored “red” and “black” so that\nTbecomes a red-black tree.\nwww.it-ebooks.info"
  },
  {
    "page": 547,
    "content": "11.7. Exercises 529\nC-11.52 The standard splaying step requires two passes, one downward pass to ﬁnd the\nnode xto splay, followed by an upward pass to splay the node x. Describe a\nmethod for splaying and searching for xin one downward pass. Each substep\nnow requires that you consider the next two nodes in the path down to x, with a\npossible zig substep performed at the end. Describe how to perform the zig-zig,\nzig-zag, and zig steps.\nC-11.53 Consider a variation of splay trees, called half-splay trees , where splaying a node\nat depth dstops as soon as the node reaches depth ⌊d/2⌋. Perform an amortized\nanalysis of half-splay trees.\nC-11.54 Describe a sequence of accesses to an n-node splay tree T, where nis odd, that\nresults in Tconsisting of a single chain of nodes such that the path down T\nalternates between left children and right children.\nProjects\nP-11.55 Reimplement the TreeMap class using nullreferences in place of explicit sen-\ntinels for the leaves of a tree.\nP-11.56 Modify the TreeMap implementation to support location-aware entries. Pro-\nvide methods ﬁrstEntry() ,lastEntry() ,ﬁndEntry( k),before( e),after( e), and\nremove( e), with all but the last of these returning an Entry instance, and the\nlatter three accepting an Entry eas a parameter.\nP-11.57 Perform an experimental study to compare the speed of our A VL tree, splay tree,\nand red-black tree implementations for various sequences of operations.\nP-11.58 Redo the previous exercise, including an implementation of skip lists. (See Ex-\nercise P-10.71.)\nP-11.59 Implement the Sorted Map ADT using a (2,4)tree. (See Section 10.3.)\nP-11.60 Write a Java class that can take any red-black tree and convert it into its corre-\nsponding (2,4)tree and can take any (2,4)tree and convert it into its correspond-\ning red-black tree.\nP-11.61 In describing multisets and multimaps in Section 10.5.3, we describe a general\napproach for adapting a traditional map by storing all duplicates within a sec-\nondary container as a value in the map. Give an alternative implementation of\na multimap using a binary search tree such that each entry of the map is stored\nat a distinct node of the tree. With the existence of duplicates, we redeﬁne the\nsearch-tree property so that all entries in the left subtree of a position pwith key k\nhave keys that are less than or equal to k , while all entries in the right subtree of\nphave keys that are greater than or equal to k . Use the public interface given in\nSection 10.5.3.\nP-11.62 Prepare an implementation of splay trees that uses top-down splaying as de-\nscribed in Exercise C-11.52. Perform extensive experimental studies to compare\nits performance to the standard bottom-up splaying implemented in this chapter.\nwww.it-ebooks.info"
  },
  {
    "page": 548,
    "content": "530 Chapter 11. Search Trees\nP-11.63 The mergeable heap ADT is an extension of the priority queue ADT consist-\ning of operations insert( k,v),min() ,removeMin() , andmerge(h) , where the\nmerge(h) operations performs a union of the mergeable heap hwith the present\none, incorporating all entries into the current one while emptying h. Describe a\nconcrete implementation of the mergeable heap ADT that achieves O(logn)per-\nformance for all its operations, where ndenotes the size of the resulting heap for\nthemerge operation.\nP-11.64 Write a program that performs a simple n-body simulation, called “Jumping Lep-\nrechauns.” This simulation involves nleprechauns, numbered 1 to n. It maintains\na gold value gifor each leprechaun i, which begins with each leprechaun start-\ning out with a million dollars worth of gold, that is, gi=1,000,000 for each\ni=1,2,..., n. In addition, the simulation also maintains, for each leprechaun,\ni, a place on the horizon, which is represented as a double-precision ﬂoating-\npoint number, xi. In each iteration of the simulation, the simulation processes the\nleprechauns in order. Processing a leprechaun iduring this iteration begins by\ncomputing a new place on the horizon for i, which is determined by the assign-\nment\nxi=xi+rgi,\nwhere ris a random ﬂoating-point number between −1 and 1. The leprechaun i\nthen steals half the gold from the nearest leprechauns on either side of him and\nadds this gold to his gold value, gi. Write a program that can perform a series\nof iterations in this simulation for a given number, n, of leprechauns. You must\nmaintain the set of horizon positions using a sorted map data structure described\nin this chapter.\nChapter Notes\nSome of the data structures discussed in this chapter are extensively covered by Knuth\nin his Sorting and Searching book [61], and by Mehlhorn in [71]. A VL trees are due to\nAdel’son-Vel’skii and Landis [2], who invented this class of balanced search trees in 1962.\nBinary search trees, A VL trees, and hashing are described in Knuth’s Sorting and Search-\ning[61] book. Average-height analyses for binary search trees can be found in the books by\nAho, Hopcroft, and Ullman [6] and Cormen, Leiserson, Rivest and Stein [25]. The hand-\nbook by Gonnet and Baeza-Yates [38] contains a number of theoretical and experimental\ncomparisons among map implementations. Aho, Hopcroft, and Ullman [5] discuss (2,3)\ntrees, which are similar to (2,4)trees. Red-black trees were deﬁned by Bayer [10]. Vari-\nations and interesting properties of red-black trees are presented in a paper by Guibas and\nSedgewick [42]. The reader interested in learning more about different balanced tree data\nstructures is referred to the books by Mehlhorn [71] and Tarjan [88], and the book chapter\nby Mehlhorn and Tsakalidis [73]. Knuth [61] is excellent additional reading that includes\nearly approaches to balancing trees. Splay trees were invented by Sleator and Tarjan [83]\n(see also [88]).\nwww.it-ebooks.info"
  },
  {
    "page": 549,
    "content": "Chapter\n12Sorting and Selection\nContents\n12.1 Merge-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . 532\n12.1.1 Divide-and-Conquer . . . . . . . . . . . . . . . . . . . . . 532\n12.1.2 Array-Based Implementation of Merge-Sort . . . . . . . . 537\n12.1.3 The Running Time of Merge-Sort . . . . . . . . . . . . . 538\n12.1.4 Merge-Sort and Recurrence Equations ⋆. . . . . . . . . . 540\n12.1.5 Alternative Implementations of Merge-Sort . . . . . . . . 541\n12.2 Quick-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . 544\n12.2.1 Randomized Quick-Sort . . . . . . . . . . . . . . . . . . . 551\n12.2.2 Additional Optimizations for Quick-Sort . . . . . . . . . . 553\n12.3 Studying Sorting through an Algorithmic Lens . . . . . . . 556\n12.3.1 Lower Bound for Sorting . . . . . . . . . . . . . . . . . . 556\n12.3.2 Linear-Time Sorting: Bucket-Sort and Radix-Sort . . . . . 558\n12.4 Comparing Sorting Algorithms . . . . . . . . . . . . . . . . 561\n12.5 Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\n12.5.1 Prune-and-Search . . . . . . . . . . . . . . . . . . . . . . 563\n12.5.2 Randomized Quick-Select . . . . . . . . . . . . . . . . . . 564\n12.5.3 Analyzing Randomized Quick-Select . . . . . . . . . . . . 565\n12.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566\nwww.it-ebooks.info"
  },
  {
    "page": 550,
    "content": "532 Chapter 12. Sorting and Selection\n12.1 Merge-Sort\nWe have introduced several sorting algorithms thus far, including insertion-sort\n(see Sections 3.1.2, 7.6, and 9.4.1); selection-sort (see Section 9.4.1); bubble-sort\n(see Exercise C-7.51); and heap-sort (see Section 9.4.2). In this chapter, we will\npresent four other sorting algorithms, called merge-sort ,quick-sort ,bucket-sort ,\nandradix-sort , and then discuss the advantages and disadvantages of the various\nalgorithms in Section 12.4.\n12.1.1 Divide-and-Conquer\nThe ﬁrst two algorithms we describe in this chapter, merge-sort and quick-sort, use\nrecursion in an algorithmic design pattern called divide-and-conquer . We have\nalready seen the power of recursion in describing algorithms in an elegant manner\n(see Chapter 5). The divide-and-conquer pattern consists of the following three\nsteps:\n1.Divide: If the input size is smaller than a certain threshold (say, one or two\nelements), solve the problem directly using a straightforward method and\nreturn the solution so obtained. Otherwise, divide the input data into two or\nmore disjoint subsets.\n2.Conquer: Recursively solve the subproblems associated with the subsets.\n3.Combine: Take the solutions to the subproblems and merge them into a so-\nlution to the original problem.\nUsing Divide-and-Conquer for Sorting\nWe ﬁrst describe the merge-sort algorithm at a high level, without focusing on\nwhether the data is an array or linked list. (We will soon give concrete implemen-\ntations for each.) To sort a sequence Swith nelements using the three divide-and-\nconquer steps, the merge-sort algorithm proceeds as follows:\n1.Divide: IfShas zero or one element, return Simmediately; it is already\nsorted. Otherwise ( Shas at least two elements), remove all the elements\nfrom Sand put them into two sequences, S1andS2, each containing about\nhalf of the elements of S; that is, S1contains the ﬁrst⌊n/2⌋elements of S,\nandS2contains the remaining ⌈n/2⌉elements.\n2.Conquer: Recursively sort sequences S1andS2.\n3.Combine: Put the elements back into Sby merging the sorted sequences S1\nandS2into a sorted sequence.\nIn reference to the divide step, we recall that the notation ⌊x⌋indicates the ﬂoor of\nx, that is, the largest integer k, such that k≤x. Similarly, the notation ⌈x⌉indicates\ntheceiling ofx, that is, the smallest integer m, such that x≤m.\nwww.it-ebooks.info"
  },
  {
    "page": 551,
    "content": "12.1. Merge-Sort 533\nWe can visualize an execution of the merge-sort algorithm by means of a binary\ntreeT, called the merge-sort tree . Each node of Trepresents a recursive invocation\n(or call) of the merge-sort algorithm. We associate with each node vofTthe\nsequence Sthat is processed by the invocation associated with v. The children of\nnode vare associated with the recursive calls that process the subsequences S1and\nS2ofS. The external nodes of Tare associated with individual elements of S,\ncorresponding to instances of the algorithm that make no recursive calls.\nFigure 12.1 summarizes an execution of the merge-sort algorithm by showing\nthe input and output sequences processed at each node of the merge-sort tree. The\nstep-by-step evolution of the merge-sort tree is shown in Figures 12.2 through 12.4.\nThis algorithm visualization in terms of the merge-sort tree helps us analyze\nthe running time of the merge-sort algorithm. In particular, since the size of theinput sequence roughly halves at each recursive call of merge-sort, the height ofthe merge-sort tree is about log n(recall that the base of log is 2 if omitted).\n4585\n50 3124 17 31 63 45 96 5045 63 24 85 17 31 96 50\n17 31 96 50 85 24 63 45\n85 63 17 96 24\n(a)\n31 5024\n9685 17 31 45 63 50 9617 31 50 96 24 45 63 8545 31 24 17 50 63 85 96\n85 63 17 24 45\n(b)\nFigure 12.1: Merge-sort tree Tfor an execution of the merge-sort algorithm on\na sequence with 8 elements: (a) input sequences processed at each node of T;\n(b) output sequences generated at each node of T.\nwww.it-ebooks.info"
  },
  {
    "page": 552,
    "content": "534 Chapter 12. Sorting and Selection\n96 45 63 24 85 17 31 50 31 96 50\n85 24 63 4517\n(a) (b)\n50\n45 63\n8517 31 96\n2445 6317\n24\n8531 96 50\n(c) (d)\n4550\n8563\n2417 31 96\n246350 96 31 17\n8545\n(e) (f)\nFigure 12.2: Visualization of an execution of merge-sort. Each node of the tree\nrepresents a recursive call of merge-sort. The nodes drawn with dashed lines repre-\nsent calls that have not been made yet. The node drawn with thick lines representsthe current call. The empty nodes drawn with thin lines represent completed calls.The remaining nodes (drawn with thin lines and not empty) represent calls that are\nwaiting for a child call to return. (Continues in Figure 12.3.)\nwww.it-ebooks.info"
  },
  {
    "page": 553,
    "content": "12.1. Merge-Sort 535\n24 85 6350 96 31 17\n45\n63 4517 31 96 50\n24 85\n(g) (h)\n45\n6317 31 96 50\n24 85\n63\n4517 31 96 50\n24 85\n(i) (j)\n45 6317 31 96 50\n24 8517 31 96 50\n24 63 45 85\n(k) (l)\nFigure 12.3: Visualization of an execution of merge-sort. (Combined with Fig-\nures 12.2 and 12.4.)\nwww.it-ebooks.info"
  },
  {
    "page": 554,
    "content": "536 Chapter 12. Sorting and Selection\n24\n17 31 96 5085 63 45\n17 31 50 9685 63 45 24\n(m) (n)\n63 85 96 50 17 24 31 45 45 31 24 17 50 63 85 96\n(o) (p)\nFigure 12.4: Visualization of an execution of merge-sort (continued from Fig-\nure 12.3). Several calls are omitted between (m) and (n). Note the merging of\ntwo halves performed in step (p).\nProposition 12.1: The merge-sort tree associated with an execution of merge-\nsort on a sequence of size nhas height⌈logn⌉.\nWe leave the justiﬁcation of Proposition 12.1 as a simple exercise (R-12.1). We\nwill use this proposition to analyze the running time of the merge-sort algorithm.\nHaving given an overview of merge-sort and an illustration of how it works,\nlet us consider each of the steps of this divide-and-conquer algorithm in more de-\ntail. Dividing a sequence of size ninvolves separating it at the element with index\n⌈n/2⌉, and recursive calls can be started by passing these smaller sequences as pa-\nrameters. The difﬁcult step is combining the two sorted sequences into a singlesorted sequence. Thus, before we present our analysis of merge-sort, we need to\nsay more about how this is done.\nwww.it-ebooks.info"
  },
  {
    "page": 555,
    "content": "12.1. Merge-Sort 537\n12.1.2 Array-Based Implementation of Merge-Sort\nWe begin by focusing on the case when a sequence of items is represented with an\narray. The merge method (Code Fragment 12.1) is responsible for the subtask of\nmerging two previously sorted sequences, S1andS2, with the output copied into S.\nWe copy one element during each pass of the while loop, conditionally determining\nwhether the next element should be taken from S1orS2. The divide-and-conquer\nmerge-sort algorithm is given in Code Fragment 12.2.\nWe illustrate a step of the merge process in Figure 12.5. During the process,\nindex irepresents the number of elements of S1that have been copied to S, while\nindex jrepresents the number of elements of S2that have been copied to S. Assum-\ningS1andS2both have at least one uncopied element, we copy the smaller of the\ntwo elements being considered. Since i+jobjects have been previously copied,\nthe next element is placed in S[i+j]. (For example, when i+jis 0, the next ele-\nment is copied to S[0]). If we reach the end of one of the sequences, we must copy\nthe next element from the other.\n1/∗∗Merge contents of arrays S1 and S2 into properly sized array S. ∗/\n2public static <K>voidmerge(K[ ] S1, K[ ] S2, K[ ] S, Comparator <K>comp){\n3inti = 0, j = 0;\n4while(i + j<S.length){\n5 if(j == S2.length||(i<S1.length && comp.compare(S1[i], S2[j]) <0))\n6 S[i+j] = S1[i++]; // copy ith element of S1 and increment i\n7 else\n8 S[i+j] = S2[j++]; // copy jth element of S2 and increment j\n9}\n10}\nCode Fragment 12.1: An implementation of the merge operation for a Java array.\nS1\nSS20 1 2 3 4 6 5\n25 18 19 22 9 10\n9 2ji\ni+j31 2 3 4 6 5\n11 12 14 2 5 80\n15\n0 1 2 3 4 6 7 8 9 10 5 11 12 13\n8 3 5 SS1\nS2\n50 1 2 3 4 6 5\n25 18 19 22 3 10\n9 2i\nj\ni+j1091 2 3 4 6 5\n11 12 14 2 5 80\n15\n0 1 2 3 4 6 7 8 9 10 5 11 12 13\n8 3\n(a) (b)\nFigure 12.5: A step in the merge of two sorted arrays for which S2[j]<S1[i]. We\nshow the arrays before the copy step in (a) and after it in (b).\nwww.it-ebooks.info"
  },
  {
    "page": 556,
    "content": "538 Chapter 12. Sorting and Selection\n1/∗∗Merge-sort contents of array S. ∗/\n2public static <K>voidmergeSort(K[ ] S, Comparator <K>comp){\n3intn = S.length;\n4if(n<2)return; // array is trivially sorted\n5// divide\n6intmid = n/2;\n7K[ ] S1 = Arrays.copyOfRange(S, 0, mid); // copy of ﬁrst half\n8K[ ] S2 = Arrays.copyOfRange(S, mid, n); // copy of second half\n9// conquer (with recursion)\n10mergeSort(S1, comp); // sort copy of ﬁrst half\n11mergeSort(S2, comp); // sort copy of second half\n12// merge results\n13merge(S1, S2, S, comp); // merge sorted halves back into original\n14}\nCode Fragment 12.2: An implementation of the recursive merge-sort algorithm for\na Java array (using the merge method deﬁned in Code Fragment 12.1).\nWe note that methods merge andmergeSort rely on use of a Comparator in-\nstance to compare a pair of generic objects that are presumed to belong to a total\norder. This is the same approach we introduced when deﬁning priority queues in\nSection 9.2.2, and when studying implementing sorted maps in Chapters 10 and 11.\n12.1.3 The Running Time of Merge-Sort\nWe begin by analyzing the running time of the merge algorithm. Let n1andn2\nbe the number of elements of S1andS2, respectively. It is clear that the operations\nperformed inside each pass of the while loop take O(1)time. The key observation is\nthat during each iteration of the loop, one element is copied from either S1orS2into\nS(and that element is considered no further). Therefore, the number of iterations\nof the loop is n1+n2. Thus, the running time of algorithm merge isO(n1+n2).\nHaving analyzed the running time of the merge algorithm used to combine\nsubproblems, let us analyze the running time of the entire merge-sort algorithm,\nassuming it is given an input sequence of nelements. For simplicity, we restrict our\nattention to the case where nis a power of 2. We leave it to an exercise (R-12.3) to\nshow that the result of our analysis also holds when nis not a power of 2.\nWhen evaluating the merge-sort recursion, we rely on the analysis technique\nintroduced in Section 5.2. We account for the amount of time spent within each\nrecursive call, but excluding any time spent waiting for successive recursive calls\nto terminate. In the case of our mergeSort method, we account for the time to\ndivide the sequence into two subsequences, and the call to merge to combine the\ntwo sorted sequences, but we exclude the two recursive calls to mergeSort .\nwww.it-ebooks.info"
  },
  {
    "page": 557,
    "content": "12.1. Merge-Sort 539\nA merge-sort tree T, as portrayed in Figures 12.2 through 12.4, can guide our\nanalysis. Consider a recursive call associated with a node vof the merge-sort tree T.\nThe divide step at node vis straightforward; this step runs in time proportional to\nthe size of the sequence for v, based on the use of slicing to create copies of the two\nlist halves. We have already observed that the merging step also takes time that is\nlinear in the size of the merged sequence. If we let idenote the depth of node v,\nthe time spent at node visO(n/2i), since the size of the sequence handled by the\nrecursive call associated with vis equal to n/2i.\nLooking at the tree Tmore globally, as shown in Figure 12.6, we see that, given\nour deﬁnition of “time spent at a node,” the running time of merge-sort is equal tothe sum of the times spent at the nodes of T. Observe that Thas exactly 2\ninodes at\ndepth i. This simple observation has an important consequence, for it implies that\nthe overall time spent at all the nodes of Tat depth iisO(2i·n/2i), which is O(n).\nBy Proposition 12.1, the height of Tis⌈logn⌉. Thus, since the time spent at each\nof the⌈logn⌉+1 levels of TisO(n), we have the following result:\nProposition 12.2: Algorithm merge-sort sorts a sequence Sof size ninO(nlogn)\ntime, assuming two elements of Scan be compared in O(1)time.\nHeight Time per level\nTotal time: O(nlogn)O(n)O(n)\nO(logn)\nO(n)n\nn/2\nn/4 n/4 n/4 n/4n/2\nFigure 12.6: A visual analysis of the running time of merge-sort. Each node rep-\nresents the time spent in a particular recursive call, labeled with the size of itssubproblem.\nwww.it-ebooks.info"
  },
  {
    "page": 558,
    "content": "540 Chapter 12. Sorting and Selection\n12.1.4 Merge-Sort and Recurrence Equations ⋆\nThere is another way to justify that the running time of the merge-sort algorithm is\nO(nlogn)(Proposition 12.2). Namely, we can deal more directly with the recursive\nnature of the merge-sort algorithm. In this section, we will present such an analysis\nof the running time of merge-sort, and in so doing, introduce the mathematical\nconcept of a recurrence equation (also known as recurrence relation ).\nLet the function t(n)denote the worst-case running time of merge-sort on an\ninput sequence of size n. Since merge-sort is recursive, we can characterize func-\ntiont(n)by means of an equation where the function t(n)is recursively expressed\nin terms of itself. In order to simplify our characterization of t(n), let us restrict\nour attention to the case when nis a power of 2. (We leave the problem of showing\nthat our asymptotic characterization still holds in the general case as an exercise.)\nIn this case, we can specify the deﬁnition of t(n)as\nt(n) =/braceleftbiggb ifn≤1\n2t(n/2)+cnotherwise.\nAn expression such as the one above is called a recurrence equation, since the\nfunction appears on both the left- and right-hand sides of the equal sign. Although\nsuch a characterization is correct and accurate, what we really desire is a big-Oh\ntype of characterization of t(n)that does not involve the function t(n)itself. That\nis, we want a closed-form characterization of t(n).\nWe can obtain a closed-form solution by applying the deﬁnition of a recurrence\nequation, assuming nis relatively large. For example, after one more application\nof the equation above, we can write a new recurrence for t(n)as\nt(n) = 2(2t(n/22)+(cn/2))+cn\n=22t(n/22)+2(cn/2)+cn=22t(n/22)+2cn.\nIf we apply the equation again, we get t(n) =23t(n/23)+3cn. At this point, we\nshould see a pattern emerging, so that after applying this equation itimes, we get\nt(n) = 2it(n/2i)+icn.\nThe issue that remains, then, is to determine when to stop this process. To see when\nto stop, recall that we switch to the closed form t(n) =bwhen n≤1, which will\noccur when 2i=n. In other words, this will occur when i=logn. Making this\nsubstitution, then, yields\nt(n) = 2lognt(n/2logn)+(logn)cn\n=nt(1)+cnlogn\n=nb+cnlogn.\nThat is, we get an alternative justiﬁcation of the fact that t(n)isO(nlogn).\nwww.it-ebooks.info"
  },
  {
    "page": 559,
    "content": "12.1. Merge-Sort 541\n12.1.5 Alternative Implementations of Merge-Sort\nSorting Linked Lists\nThe merge-sort algorithm can easily be adapted to use any form of a basic queue\nas its container type. In Code Fragment 12.3, we provide such an implementation,\nbased on use of the LinkedQueue class from Section 6.2.3. The O(nlogn)bound\nfor merge-sort from Proposition 12.2 applies to this implementation as well, since\neach basic operation runs in O(1)time when implemented with a linked list. We\nshow an example execution of this version of the merge algorithm in Figure 12.7.\n1/∗∗Merge contents of sorted queues S1 and S2 into empty queue S. ∗/\n2public static <K>voidmerge(Queue <K>S1, Queue <K>S2, Queue <K>S,\n3 Comparator <K>comp){\n4while(!S1.isEmpty() && !S2.isEmpty()) {\n5 if(comp.compare(S1.ﬁrst(), S2.ﬁrst()) <0)\n6 S.enqueue(S1.dequeue()); // take next element from S1\n7 else\n8 S.enqueue(S2.dequeue()); // take next element from S2\n9}\n10while(!S1.isEmpty())\n11 S.enqueue(S1.dequeue()); // move any elements that remain in S1\n12while(!S2.isEmpty())\n13 S.enqueue(S2.dequeue()); // move any elements that remain in S2\n14}\n15\n16/∗∗Merge-sort contents of queue. ∗/\n17public static <K>voidmergeSort(Queue <K>S, Comparator <K>comp){\n18intn = S.size();\n19if(n<2)return; // queue is trivially sorted\n20// divide\n21Queue<K>S1 =newLinkedQueue <>();// (or any queue implementation)\n22Queue<K>S2 =newLinkedQueue <>();\n23while(S1.size() <n/2)\n24 S1.enqueue(S.dequeue()); // move the ﬁrst n/2 elements to S1\n25while(!S.isEmpty())\n26 S2.enqueue(S.dequeue()); // move remaining elements to S2\n27// conquer (with recursion)\n28mergeSort(S1, comp); // sort ﬁrst half\n29mergeSort(S2, comp); // sort second half\n30// merge results\n31merge(S1, S2, S, comp); // merge sorted halves back into original\n32}\nCode Fragment 12.3: An implementation of merge-sort using a basic queue.\nwww.it-ebooks.info"
  },
  {
    "page": 560,
    "content": "542 Chapter 12. Sorting and Selection\n24 45 63 85 S1\n17 31 50 96 S2\nS24 45 63 85 S1\n1731 50 96 S2\nS\n(a) (b)\n2445 63 85 S1\n1731 50 96 S2\nS 2445 63 85 S1\n1750 96 S2\nS 31\n(c) (d)\n2463 85 S1\n1750 96 S2\nS 31 45 2463 85 S1\n1796S2\nS 31 45 50\n(e) (f)\n2485 S1\n1796S2\nS 31 45 50 6324S1\n1796S2\nS 31 45 50 63 85\n(g) (h)\n24S1\n17S2\nS 31 45 50 63 85 96\n(i)\nFigure 12.7: Example of an execution of the merge algorithm, as implemented in\nCode Fragment 12.3 using queues.\nwww.it-ebooks.info"
  },
  {
    "page": 561,
    "content": "12.1. Merge-Sort 543\nA Bottom-Up (Nonrecursive) Merge-Sort\nThere is a nonrecursive version of array-based merge-sort, which runs in O(nlogn)\ntime. It is a bit faster than recursive merge-sort in practice, as it avoids the extra\noverheads of recursive calls and temporary memory at each level. The main idea\nis to perform merge-sort bottom-up, performing the merges level by level going upthe merge-sort tree. Given an input array of elements, we begin by merging everysuccessive pair of elements into sorted runs of length two. We merge these runs intoruns of length four, merge these new runs into runs of length eight, and so on, until\nthe array is sorted. To keep the space usage reasonable, we deploy a second array\nthat stores the merged runs (swapping input and output arrays after each iteration).We give a Java implementation in Code Fragment 12.4, using the built-in methodSystem.arraycopy to copy a range of cells between two arrays. A similar bottom-up\napproach can be used for sorting linked lists. (See Exercise C-12.30.)\n1/∗∗Merges in[start..start+inc −1] and in[start+inc..start+2 ∗inc−1] into out. ∗/\n2public static <K>voidmerge(K[ ] in, K[ ] out, Comparator <K>comp,\n3 intstart,intinc){\n4intend1 = Math.min(start + inc, in.length); // boundary for run 1\n5intend2 = Math.min(start + 2 ∗inc, in.length); // boundary for run 2\n6intx=start; // index into run 1\n7inty=start+inc; // index into run 2\n8intz=start; // index into output\n9while(x<end1 && y <end2)\n10 if(comp.compare(in[x], in[y]) <0)\n11 out[z++] = in[x++]; // take next from run 1\n12 else\n13 out[z++] = in[y++]; // take next from run 2\n14if(x<end1) System.arraycopy(in, x, out, z, end1 −x);// copy rest of run 1\n15else if(y<end2) System.arraycopy(in, y, out, z, end2 −y);// copy rest of run 2\n16}\n17/∗∗Merge-sort contents of data array. ∗/\n18public static <K>voidmergeSortBottomUp(K[ ] orig, Comparator <K>comp){\n19intn = orig.length;\n20K[ ] src = orig; // alias for the original\n21K[ ] dest = (K[ ]) newObject[n]; // make a new temporary array\n22K[ ] temp; // reference used only for swapping\n23for(inti=1; i<n; i∗= 2){ // each iteration sorts all runs of length i\n24 for(intj=0; j<n; j += 2 ∗i) // each pass merges two runs of length i\n25 merge(src, dest, comp, j, i);\n26 temp = src; src = dest; dest = temp; // reverse roles of the arrays\n27}\n28if(orig != src)\n29 System.arraycopy(src, 0, orig, 0, n); // additional copy to get result to original\n30}\nCode Fragment 12.4: An implementation of the nonrecursive merge-sort algorithm.\nwww.it-ebooks.info"
  },
  {
    "page": 562,
    "content": "544 Chapter 12. Sorting and Selection\n12.2 Quick-Sort\nThe next sorting algorithm we discuss is called quick-sort . Like merge-sort, this\nalgorithm is also based on the divide-and-conquer paradigm, but it uses this tech-\nnique in a somewhat opposite manner, as all the hard work is done before the\nrecursive calls.\nHigh-Level Description of Quick-Sort\nThe quick-sort algorithm sorts a sequence Susing a simple recursive approach.\nThe main idea is to apply the divide-and-conquer technique, whereby we divide\nSinto subsequences, recur to sort each subsequence, and then combine the sorted\nsubsequences by a simple concatenation. In particular, the quick-sort algorithm\nconsists of the following three steps (see Figure 12.8):\n1.Divide: IfShas at least two elements (nothing needs to be done if Shas\nzero or one element), select a speciﬁc element xfrom S, which is called the\npivot . As is common practice, choose the pivot xto be the last element in S.\nRemove all the elements from Sand put them into three sequences:\n•L, storing the elements in Sless than x\n•E, storing the elements in Sequal to x\n•G, storing the elements in Sgreater than x\nOf course, if the elements of Sare distinct, then Eholds just one element—\nthe pivot itself.\n2.Conquer: Recursively sort sequences LandG.\n3.Combine: Put back the elements into Sin order by ﬁrst inserting the elements\nofL, then those of E, and ﬁnally those of G.\n2. Recur1. Split using pivot x\n3. Concatenate2. Recur\nG(>x) L(<x)E(=x)\nFigure 12.8: A visual schematic of the quick-sort algorithm.\nwww.it-ebooks.info"
  },
  {
    "page": 563,
    "content": "12.2. Quick-Sort 545\nLike merge-sort, the execution of quick-sort can be visualized by means of a bi-\nnary recursion tree, called the quick-sort tree . Figure 12.9 summarizes an execution\nof the quick-sort algorithm by showing the input and output sequences processed at\neach node of the quick-sort tree. The step-by-step evolution of the quick-sort treeis shown in Figures 12.10, 12.11, and 12.12.\nUnlike merge-sort, however, the height of the quick-sort tree associated with\nan execution of quick-sort is linear in the worst case. This happens, for example,if the sequence consists of ndistinct elements and is already sorted. Indeed, in this\ncase, the standard choice of the last element as pivot yields a subsequence Lof size\nn−1, while subsequence Ehas size 1 and subsequence Ghas size 0. At each call\nof quick-sort on subsequence L, the size decreases by 1. Hence, the height of the\nquick-sort tree is n−1.\n45\n4563 24 85 17 31 96 50\n85 63 96 24 45 17 31\n24 85 63 17\n24 85\n(a)\n2431 63 85 9645 31 24 17 50 63 85 96\n17 24 45\n17 63 85\n24 8545\n(b)\nFigure 12.9: Quick-sort tree Tfor an execution of the quick-sort algorithm on a se-\nquence with 8 elements: (a) input sequences processed at each node of T; (b) output\nsequences generated at each node of T. The pivot used at each level of the recursion\nis shown in bold.\nwww.it-ebooks.info"
  },
  {
    "page": 564,
    "content": "546 Chapter 12. Sorting and Selection\n63 24 85 17 31 96 50 45 24 31 85 63 96 50 45 17\n(a) (b)\n85 63 96 50\n45 17 24 3163 96 50\n45 31 24 1785\n(c) (d)\n63 96 50\n45\n24 173185 63 96 50\n45 31\n172485\n(e) (f)\nFigure 12.10: Visualization of quick-sort. Each node of the tree represents a re-\ncursive call. The nodes drawn with dashed lines represent calls that have not been\nmade yet. The node drawn with thick lines represents the running call. The empty\nnodes drawn with thin lines represent terminated calls. The remaining nodes repre-\nsent suspended calls (that is, active calls that are waiting for a child call to return).Note the divide steps performed in (b), (d), and (f). (Continues in Figure 12.11.)\nwww.it-ebooks.info"
  },
  {
    "page": 565,
    "content": "12.2. Quick-Sort 547\n63 96 50\n45 31\n241785 63 96 50\n45 31\n172485\n(g) (h)\n63 96 50\n45 31\n17\n2485 63 96 50\n45 31\n172485\n(i) (j)\n63 96 50\n45 31\n17 2485 85 63 96 50\n45 31 17 24\n(k) (l)\nFigure 12.11: Visualization of an execution of quick-sort. Note the concatenation\nstep performed in (k). (Continues in Figure 12.12.)\nwww.it-ebooks.info"
  },
  {
    "page": 566,
    "content": "548 Chapter 12. Sorting and Selection\n85 63 96 50\n17 24 31\n4585 63 96 50\n45 31 17 24\n(m) (n)\n85 63 96 50\n24 31 17 4563 96 50 24 31 17 45 85\n(o) (p)\n85 63 45 17 31 24 50 96 50 24 31 17 45 63 85 96\n(q) (r)\nFigure 12.12: Visualization of an execution of quick-sort. Several calls between (p)\nand (q) have been omitted. Note the concatenation steps performed in (o) and (r).\n(Continued from Figure 12.11.)\nwww.it-ebooks.info"
  },
  {
    "page": 567,
    "content": "12.2. Quick-Sort 549\nPerforming Quick-Sort on General Sequences\nIn Code Fragment 12.5, we give an implementation of the quick-sort algorithm\nthat works on any sequence type that operates as a queue. This particular version\nrelies on the LinkedQueue class from Section 6.2.3; we provide a more streamlined\nimplementation of quick-sort using an array-based sequence in Section 12.2.2.\nOur implementation chooses the ﬁrst item of the queue as the pivot (since it\nis easily accessible), and then it divides sequence Sinto queues L,E, and Gof\nelements that are respectively less than, equal to, and greater than the pivot. We\nthen recur on the LandGlists, and transfer elements from the sorted lists L,E,\nandGback to S. All of the queue operations run in O(1)worst-case time when\nimplemented with a linked list.\n1/∗∗Quick-sort contents of a queue. ∗/\n2public static <K>voidquickSort(Queue <K>S, Comparator <K>comp){\n3intn = S.size();\n4if(n<2)return; // queue is trivially sorted\n5// divide\n6K pivot = S.ﬁrst(); // using ﬁrst as arbitrary pivot\n7Queue<K>L =newLinkedQueue <>();\n8Queue<K>E =newLinkedQueue <>();\n9Queue<K>G =newLinkedQueue <>();\n10while(!S.isEmpty()){ // divide original into L, E, and G\n11 K element = S.dequeue();\n12 intc = comp.compare(element, pivot);\n13 if(c<0) // element is less than pivot\n14 L.enqueue(element);\n15 else if(c == 0) // element is equal to pivot\n16 E.enqueue(element);\n17 else // element is greater than pivot\n18 G.enqueue(element);\n19}\n20// conquer\n21quickSort(L, comp); // sort elements less than pivot\n22quickSort(G, comp); // sort elements greater than pivot\n23// concatenate results\n24while(!L.isEmpty())\n25 S.enqueue(L.dequeue());\n26while(!E.isEmpty())\n27 S.enqueue(E.dequeue());\n28while(!G.isEmpty())\n29 S.enqueue(G.dequeue());\n30}\nCode Fragment 12.5: Quick-sort for a sequence Simplemented as a queue.\nwww.it-ebooks.info"
  },
  {
    "page": 568,
    "content": "550 Chapter 12. Sorting and Selection\nRunning Time of Quick-Sort\nWe can analyze the running time of quick-sort with the same technique used for\nmerge-sort in Section 12.1.3. Namely, we can identify the time spent at each nodeof the quick-sort tree Tand sum up the running times for all the nodes.\nExamining Code Fragment 12.5, we see that the divide step and the ﬁnal con-\ncatenation of quick-sort can be implemented in linear time. Thus, the time spentat a node vofTis proportional to the input size s(v)ofv, deﬁned as the size of\nthe sequence handled by the call of quick-sort associated with node v. Since sub-\nsequence Ehas at least one element (the pivot), the sum of the input sizes of the\nchildren of vis at most s(v)−1.\nLets\nidenote the sum of the input sizes of the nodes at depth ifor a particular\nquick-sort tree T. Clearly, s0=n, since the root rofTis associated with the entire\nsequence. Also, s1≤n−1, since the pivot is not propagated to the children of r.\nMore generally, it must be that si<si−1since the elements of the subsequences at\ndepth iall come from distinct subsequences at depth i−1, and at least one element\nfrom depth i−1 does not propagate to depth ibecause it is in a set E(in fact, one\nelement from each node at depth i−1 does not propagate to depth i).\nWe can therefore bound the overall running time of an execution of quick-sort\nasO(n·h)where his the overall height of the quick-sort tree Tfor that execution.\nUnfortunately, in the worst case, the height of a quick-sort tree is n−1, as observed\nin Section 12.2. Thus, quick-sort runs in O(n2)worst-case time. Paradoxically,\nif we choose the pivot as the last element of the sequence, this worst-case behav-ior occurs for problem instances when sorting should be easy—if the sequence isalready sorted.\nGiven its name, we would expect quick-sort to run quickly, and it often does\nin practice. The best case for quick-sort on a sequence of distinct elements oc-curs when subsequences LandGhave roughly the same size. In that case, as\nwe saw with merge-sort, the tree has height O(logn)and therefore quick-sort runs\ninO(nlogn)time; we leave the justiﬁcation of this fact as an exercise (R-12.12).\nMore so, we can observe an O(nlogn)running time even if the split between L\nandGis not as perfect. For example, if every divide step caused one subsequence\nto have one-fourth of those elements and the other to have three-fourths of theelements, the height of the tree would remain O(logn)and thus the overall perfor-\nmance O(nlogn).\nWe will see in the next section that introducing randomization in the choice of\na pivot will makes quick-sort essentially behave in this way on average, with anexpected running time that is O(nlogn).\nwww.it-ebooks.info"
  },
  {
    "page": 569,
    "content": "12.2. Quick-Sort 551\n12.2.1 Randomized Quick-Sort\nOne common method for analyzing quick-sort is to assume that the pivot will al-\nways divide the sequence in a reasonably balanced manner. However, we feel such\nan assumption would presuppose knowledge about the input distribution that is typ-\nically not available. For example, we would have to assume that we will rarely be\ngiven “almost” sorted sequences to sort, which are actually common in many ap-\nplications. Fortunately, this assumption is not needed in order for us to match our\nintuition to quick-sort’s behavior.\nIn general, we desire some way of getting close to the best-case running time\nfor quick-sort. The way to get close to the best-case running time, of course, is for\nthe pivot to divide the input sequence Salmost equally. If this outcome were to\noccur, then it would result in a running time that is asymptotically the same as the\nbest-case running time. That is, having pivots close to the “middle” of the set of\nelements leads to an O(nlogn)running time for quick-sort.\nPicking Pivots at Random\nSince the goal of the partition step of the quick-sort method is to divide the sequence\nSwith sufﬁcient balance, let us introduce randomization into the algorithm and pick\nas the pivot a random element of the input sequence. That is, instead of picking\nthe pivot as the ﬁrst or last element of S, we pick an element of Sat random as the\npivot, keeping the rest of the algorithm unchanged. This variation of quick-sort is\ncalled randomized quick-sort . The following proposition shows that the expected\nrunning time of randomized quick-sort on a sequence with nelements is O(nlogn).\nThis expectation is taken over all the possible random choices the algorithm makes,\nand is independent of any assumptions about the distribution of the possible input\nsequences the algorithm is likely to be given.\nProposition 12.3: The expected running time of randomized quick-sort on a se-\nquence Sof size nisO(nlogn).\nJustiﬁcation: LetSbe a sequence with nelements and let Tbe the binary tree\nassociated with an execution of randomized quick-sort on S. First, we observe that\nthe running time of the algorithm is proportional to the number of comparisons per-\nformed. We consider the recursive call associated with a node of Tand observe that\nduring the call, all comparisons are between the pivot element and another element\nof the input of the call. Thus, we can evaluate the total number of comparisons per-\nformed by the algorithm as ∑s∈SC(x), where C(x)is the number of comparisons\ninvolving xas a nonpivot element. Next, we will show that for every element x∈S,\nthe expected value of C(x)isO(logn). Since the expected value of a sum is the\nsum of the expected values of its terms, an O(logn)bound on the expected value\nofC(x)implies that randomized quick-sort runs in expected O(nlogn)time.\nwww.it-ebooks.info"
  },
  {
    "page": 570,
    "content": "552 Chapter 12. Sorting and Selection\nTo show that the expected value of C(x)isO(nlogn)for any x, we ﬁx an arbi-\ntrary element xand consider the path of nodes in the tree Tassociated with recursive\ncalls for which xis part of the input sequence. (See Figure 12.13.) By deﬁnition,\nC(x)is equal to that path length, as xwill take part in one nonpivot comparison per\nlevel of the tree until it is chosen as the pivot or is the only element that remains.\nLetnddenote the input size for the node of that path at depth dof tree T, for\n0≤d≤C(x). Since all elements are in the initial recursive call, n0=n. We know\nthat the input size for any recursive call is at least one less than the size of its parent,\nand thus that nd+1≤nd−1 for any d<C(x). In the worst case, this implies that\nC(x)≤n−1, as the recursive process stops if nd=1 or if xis chosen as the pivot.\nWe can show the stronger claim that the expected value of C(x)isO(logn)\nbased on the random selection of a pivot at each level. The choice of pivot at\ndepth dof this path is considered “good” if nd+1≤3nd/4. The choice of a pivot\nwill be good with probability at least 1 /2, as there are at least nd/2 elements in the\ninput that, if chosen as pivot, will result in at least nd/4 elements begin placed in\neach subproblem, thereby leaving xin a group with at most 3 nd/4 elements.\nWe conclude by noting that there can be at most log4/3nsuch good pivot\nchoices before xis isolated. Since a choice is good with probability at least 1/2,\nthe expected number of recursive calls before achieving log4/3ngood choices is at\nmost 2log4/3n, which implies that C(x)isO(logn).\nWith a more rigorous analysis, we can show that the running time of random-\nized quick-sort is O(nlogn)with high probability . (See Exercise C-12.55.)\n17 96 63\n24 485 314 10 17 24 31 45 63 85 964 10 17 24 31 45 63 85 96 50\n4 10 24 31\n24 10\nFigure 12.13: An illustration of the analysis of Proposition 12.3 for an execution of\nrandomized quick-sort. We focus on element x=31, which has value C(x)=3, as\nit is the nonpivot element in a comparison with 50, 45, and 17. By our notation,\nn0=10,n1=6,n2=5, and n3=2, and the pivot choices of 50 and 17 are good.\nwww.it-ebooks.info"
  },
  {
    "page": 571,
    "content": "12.2. Quick-Sort 553\n12.2.2 Additional Optimizations for Quick-Sort\nAn algorithm is in-place if it uses only a small amount of memory in addition\nto that needed for the original input. Our implementation of heap-sort, from Sec-\ntion 9.4.2, is an example of such an in-place sorting algorithm. Our implementation\nof quick-sort from Code Fragment 12.5 does not qualify as in-place because we use\nadditional containers L,E, and Gwhen dividing a sequence Swithin each recursive\ncall. Quick-sort of an array-based sequence can be adapted to be in-place, and such\nan optimization is used in most deployed implementations.\nPerforming the quick-sort algorithm in-place requires a bit of ingenuity, how-\never, for we must use the input sequence itself to store the subsequences for all\nthe recursive calls. We show algorithm quickSortInPlace , which performs in-place\nquick-sort of an array, in Code Fragment 12.6. In-place quick-sort modiﬁes the in-\nput sequence using element swapping and does not explicitly create subsequences.\nInstead, a subsequence of the input sequence is implicitly represented by a range\nof positions speciﬁed by a leftmost index aand a rightmost index b. The divide\n1/∗∗Sort the subarray S[a..b] inclusive. ∗/\n2private static <K>voidquickSortInPlace(K[ ] S, Comparator <K>comp,\n3 inta,intb){\n4if(a>= b)return;// subarray is trivially sorted\n5intleft = a;\n6intright = b−1;\n7K pivot = S[b];\n8K temp; // temp object used for swapping\n9while(left<= right){\n10 // scan until reaching value equal or larger than pivot (or right marker)\n11 while(left<= right && comp.compare(S[left], pivot) <0) left++;\n12 // scan until reaching value equal or smaller than pivot (or left marker)\n13 while(left<= right && comp.compare(S[right], pivot) >0) right−−;\n14 if(left<= right){// indices did not strictly cross\n15 // so swap values and shrink range\n16 temp = S[left]; S[left] = S[right]; S[right] = temp;\n17 left++; right−−;\n18}\n19}\n20// put pivot into its ﬁnal place (currently marked by left index)\n21temp = S[left]; S[left] = S[b]; S[b] = temp;\n22// make recursive calls\n23quickSortInPlace(S, comp, a, left −1);\n24quickSortInPlace(S, comp, left + 1, b);\n25}\nCode Fragment 12.6: In-place quick-sort for an array S. The entire array can be\nsorted as quickSortInPlace(S, comp, 0, S.length −1).\nwww.it-ebooks.info"
  },
  {
    "page": 572,
    "content": "554 Chapter 12. Sorting and Selection\nstep is performed by scanning the array simultaneously using local variables left,\nwhich advances forward, and right , which advances backward, swapping pairs of\nelements that are in reverse order, as shown in Figure 12.14. When these two in-\ndices pass each other, the division step is complete and the algorithm completes byrecurring on these two sublists. There is no explicit “combine” step, because theconcatenation of the two sublists is implicit to the in-place use of the original list.\nIt is worth noting that if a sequence has duplicate values, we are not explicitly\ncreating three sublists L,E, and G, as in our original quick-sort description. We in-\nstead allow elements equal to the pivot (other than the pivot itself) to be dispersedacross the two sublists. Exercise R-12.11 explores the subtlety of our implementa-\ntion in the presence of duplicate keys, and Exercise C-12.34 describes an in-place\nalgorithm that strictly partitions into three sublists L,E, and G.\n24 63 45 17 31 96 50\nl85\nr(a)\n24 63 45 17 31 96 50\nl85\nr(b)\n24 63 45 17 85 96 50\nl31\nr(c)\n24 63 45 17 85 96 50\nr31\nl(d)\n24 17 45 63 85 96 50 31\nl,r\n(e)\nr<31 24 17 45 63 85 96 50\nl\n(f)\n24 17 45 31 85 96 63 50\n(g)\nFigure 12.14: Divide step of in-place quick-sort, using index las shorthand for iden-\ntiﬁerleft, and index ras shorthand for identiﬁer right . Index lscans the sequence\nfrom left to right, and index rscans the sequence from right to left. A swap is per-\nformed when lis at an element as large as the pivot and ris at an element as small\nas the pivot. A ﬁnal swap with the pivot, in part (f), completes the divide step.\nwww.it-ebooks.info"
  },
  {
    "page": 598,
    "content": "580 Chapter 13. Text Processing\nfor another occurrence. The efﬁciency of the Boyer-Moore algorithm relies on\nquickly determining where a mismatched character occurs elsewhere in the pat-\ntern. In particular, we deﬁne a function last(c)as\n•Ifcis in the pattern, last(c)is the index of the last (rightmost) occurrence of\ncin the pattern. Otherwise, we conventionally deﬁne last(c)=−1.\nIf we assume that the alphabet is of ﬁxed, ﬁnite size, and that characters can be\nconverted to indices of an array (for example, by using their character code), the\nlastfunction can be easily implemented as a lookup table with worst-case O(1)-\ntime access to the value last(c). However, the table would have length equal to the\nsize of the alphabet (rather than the size of the pattern), and time would be required\nto initialize the entire table.\nWe prefer to use a hash table to represent the lastfunction, with only those\ncharacters from the pattern occurring in the map. The space usage for this approach\nis proportional to the number of distinct alphabet symbols that occur in the pattern,\nand thus O(max(m,|Σ|)). The expected lookup time remains O(1)(as does the\nworst-case, if we consider |Σ|a constant). Our complete implementation of the\nBoyer-Moore pattern-matching algorithm is given in Code Fragment 13.2.\n1/∗∗Returns the lowest index at which substring pattern begins in text (or else −1).∗/\n2public static int ﬁndBoyerMoore( char[ ] text,char[ ] pattern){\n3intn = text.length;\n4intm = pattern.length;\n5if(m == 0) return0; // trivial search for empty string\n6Map<Character,Integer >last =newHashMap <>();// the 'last'map\n7for(inti=0; i<n; i++)\n8last.put(text[i],−1); // set−1 as default for all text characters\n9for(intk=0; k<m; k++)\n10last.put(pattern[k], k); // rightmost occurrence in pattern is last\n11// start with the end of the pattern aligned at index m− 1 of the text\n12inti = m−1; // an index into the text\n13intk = m−1; // an index into the pattern\n14while(i<n){\n15if(text[i] == pattern[k]) { // a matching character\n16 if(k == 0) returni; // entire pattern has been found\n17 i−−; // otherwise, examine previous\n18 k−−; // characters of text/pattern\n19}else{\n20 i += m−Math.min(k, 1 + last.get(text[i])); // case analysis for jump step\n21 k = m−1; // restart at end of pattern\n22}\n23}\n24return−1; // pattern was never found\n25}\nCode Fragment 13.2: An implementation of the Boyer-Moore algorithm.\nwww.it-ebooks.info"
  },
  {
    "page": 599,
    "content": "13.2. Pattern-Matching Algorithms 581\nThe correctness of the Boyer-Moore pattern-matching algorithm follows from\nthe fact that each time the method makes a shift, it is guaranteed not to “skip” over\nany possible matches. For last(c)is the location of the lastoccurrence of cin the\npattern. In Figure 13.4, we illustrate the execution of the Boyer-Moore pattern-\nmatching algorithm on an input string similar to Example 13.1.\nc a b c d\nlast(c)4 5 3−1\na c d a b a a c b a a b c Text:\nPattern: b a a a b c1\nb a a a b c2 3 4\nb a a a b c5\nb a a a b c6b a a a b c7b a a a b c8 9 10 11 12 13b b a a a b a\nFigure 13.4: An illustration of the Boyer-Moore pattern-matching algorithm, in-\ncluding a summary of the last(c)function. The algorithm performs 13 character\ncomparisons, which are indicated with numerical labels.\nPerformance\nIf using a traditional lookup table, the worst-case running time of the Boyer-Moore\nalgorithm is O(nm+|Σ|). The computation of the lastfunction takes O(m+|Σ|)\ntime, although the dependence on |Σ|is removed if using a hash table. The actual\nsearch for the pattern takes O(nm)time in the worst case—the same as the brute-\nforce algorithm. An example that achieves the worst case for Boyer-Moore is\ntext=n/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownrightaaaaaa···a\npattern=bm−1/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownrightaa···a\nThe worst-case performance, however, is unlikely to be achieved for English text;\nin that case, the Boyer-Moore algorithm is often able to skip large portions of text.\nExperimental evidence on English text shows that the average number of compar-\nisons done per character is 0 .24 for a ﬁve-character pattern string.\nWe have actually presented a simpliﬁed version of the Boyer-Moore algorithm.\nThe original algorithm achieves worst-case running time O(n+m+|Σ|)by using\nan alternative shift heuristic for a partially matched text string, whenever it shifts\nthe pattern more than the character-jump heuristic. This alternative shift heuristic\nis based on applying the main idea from the Knuth-Morris-Pratt pattern-matching\nalgorithm, which we discuss next.\nwww.it-ebooks.info"
  },
  {
    "page": 600,
    "content": "582 Chapter 13. Text Processing\n13.2.3 The Knuth-Morris-Pratt Algorithm\nIn examining the worst-case performances of the brute-force and Boyer-Moore\npattern-matching algorithms on speciﬁc instances of the problem, such as that given\nin Example 13.1, we should notice a major inefﬁciency (at least in the worst case).\nFor a certain alignment of the pattern, if we ﬁnd several matching characters but\nthen detect a mismatch, we ignore all the information gained by the successful\ncomparisons after restarting with the next incremental placement of the pattern.\nThe Knuth-Morris-Pratt (or “KMP”) algorithm, discussed in this section, avoids\nthis waste of information and, in so doing, it achieves a running time of O(n+m),\nwhich is asymptotically optimal. That is, in the worst case any pattern-matching\nalgorithm will have to examine all the characters of the text and all the characters\nof the pattern at least once. The main idea of the KMP algorithm is to precom-\npute self-overlaps between portions of the pattern so that when a mismatch occurs\nat one location, we immediately know the maximum amount to shift the pattern\nbefore continuing the search. A motivating example is shown in Figure 13.5.\na · · · · · · · · · ·\na m\na m t o n i m a a g a lt o n i m a a g a l· t\na m t o n i m a a g a lText:\nPattern:a a m a g l a m c\nFigure 13.5: A motivating example for the Knuth-Morris-Pratt algorithm. If a mis-\nmatch occurs at the indicated location, the pattern could be shifted to the second\nalignment, without explicit need to recheck the partial match with the preﬁx ama.\nIf the mismatched character is not an l, then the next potential alignment of the\npattern can take advantage of the common a.\nThe Failure Function\nTo implement the KMP algorithm, we will precompute a failure function ,f, that\nindicates the proper shift of the pattern upon a failed comparison. Speciﬁcally, the\nfailure function f(k)is deﬁned as the length of the longest preﬁx of the pattern that\nis a sufﬁx of the substring pattern[1..k] (note that we did notincludepattern[0]\nhere, since we will shift at least one unit). Intuitively, if we ﬁnd a mismatch upon\ncharacter pattern[k+1] , the function f(k)tells us how many of the immediately\npreceding characters can be reused to restart the pattern. Example 13.2 describes\nthe value of the failure function for the example pattern from Figure 13.5.\nwww.it-ebooks.info"
  },
  {
    "page": 601,
    "content": "13.2. Pattern-Matching Algorithms 583\nExample 13.2: Consider the pattern \"amalgamation\" from Figure 13.5. The\nKnuth-Morris-Pratt (KMP) failure function, f(k), for the string Pis as shown in\nthe following table:\nk 0 1 2 3 4 5 6 7 8 9 10 11\nP[k]a m a l g a m a t i o n\nf(k)0 0 1 0 0 1 2 3 0 0 0 0\nImplementation\nOur implementation of the KMP pattern-matching algorithm is shown in Code\nFragment 13.3. It relies on a utility method, computeFailKMP , discussed on the\nnext page, to compute the failure function efﬁciently.\nThe main part of the KMP algorithm is its while loop, each iteration of which\nperforms a comparison between the character at index jin the text and the character\nat index kin the pattern. If the outcome of this comparison is a match, the algorithm\nmoves on to the next characters in both (or reports a match if reaching the end of\nthe pattern). If the comparison failed, the algorithm consults the failure function\nfor a new candidate character in the pattern, or starts over with the next index in the\ntext if failing on the ﬁrst character of the pattern (since nothing can be reused).\n1/∗∗Returns the lowest index at which substring pattern begins in text (or else −1).∗/\n2public static int ﬁndKMP( char[ ] text,char[ ] pattern){\n3intn = text.length;\n4intm = pattern.length;\n5if(m == 0) return0; // trivial search for empty string\n6int[ ] fail = computeFailKMP(pattern); // computed by private utility\n7intj = 0; // index into text\n8intk = 0; // index into pattern\n9while(j<n){\n10if(text[j] == pattern[k]) { // pattern[0..k] matched thus far\n11 if(k == m−1)returnj−m + 1; // match is complete\n12 j++; // otherwise, try to extend match\n13 k++;\n14}else if(k>0)\n15 k = fail[k−1]; // reuse suﬃx of P[0..k-1]\n16else\n17 j++;\n18}\n19return−1; // reached end without match\n20}\nCode Fragment 13.3: An implementation of the KMP pattern-matching algorithm.\nThecomputeFailKMP utility method is given in Code Fragment 13.4.\nwww.it-ebooks.info"
  },
  {
    "page": 602,
    "content": "584 Chapter 13. Text Processing\nConstructing the KMP Failure Function\nTo construct the failure function, we use the method shown in Code Fragment 13.4,\nwhich is a “bootstrapping” process that compares the pattern to itself as in the KMP\nalgorithm. Each time we have two characters that match, we set f(j)=k+1. Note\nthat since we have j>kthroughout the execution of the algorithm, f(k−1)is\nalways well deﬁned when we need to use it.\n1private static int [ ] computeFailKMP( char[ ] pattern){\n2intm = pattern.length;\n3int[ ] fail = new int[m]; // by default, all overlaps are zero\n4intj = 1;\n5intk = 0;\n6while(j<m){ // compute fail[j] during this pass, if nonzero\n7if(pattern[j] == pattern[k]) {// k + 1 characters match thus far\n8 fail[j] = k + 1;\n9 j++;\n10 k++;\n11}else if(k>0) // k follows a matching preﬁx\n12 k = fail[k−1];\n13else // no match found starting at j\n14 j++;\n15}\n16returnfail;\n17}\nCode Fragment 13.4: An implementation of the computeFailKMP utility in support\nof the KMP pattern-matching algorithm. Note how the algorithm uses the previous\nvalues of the failure function to efﬁciently compute new values.\nPerformance\nExcluding the computation of the failure function, the running time of the KMPalgorithm is clearly proportional to the number of iterations of the while loop. For\nthe sake of the analysis, let us deﬁne s=j−k. Intuitively, sis the total amount by\nwhich the pattern has been shifted with respect to the text. Note that throughout theexecution of the algorithm, we have s≤n. One of the following three cases occurs\nat each iteration of the loop.\n•Iftext[j]=pattern[k], then jandkeach increase by 1, thus sis unchanged.\n•Iftext[j]/n⌉}ationslash=pattern[k]andk>0, then jdoes not change and sincreases by\nat least 1, since in this case schanges from j−ktoj−f(k−1); note that\nthis is an addition of k−f(k−1), which is positive because f(k−1)<k.\n•Iftext[j]/n⌉}ationslash=pattern[k]andk=0, then jincreases by 1 and sincreases by 1,\nsince kdoes not change.\nwww.it-ebooks.info"
  },
  {
    "page": 603,
    "content": "13.2. Pattern-Matching Algorithms 585\nThus, at each iteration of the loop, either jorsincreases by at least 1 (possibly\nboth); hence, the total number of iterations of the while loop in the KMP pattern-\nmatching algorithm is at most 2 n. Achieving this bound, of course, assumes that\nwe have already computed the failure function for the pattern.\nThe algorithm for computing the failure function runs in O(m)time. Its analysis\nis analogous to that of the main KMP algorithm, yet with a pattern of length m\ncompared to itself. Thus, we have:\nProposition 13.3: The Knuth-Morris-Pratt algorithm performs pattern matching\non a text string of length nand a pattern string of length minO(n+m)time.\nThe correctness of this algorithm follows from the deﬁnition of the failure func-\ntion. Any comparisons that are skipped are actually unnecessary, for the failure\nfunction guarantees that all the ignored comparisons are redundant—they would\ninvolve comparing the same matching characters over again.\nIn Figure 13.6, we illustrate the execution of the KMP pattern-matching algo-\nrithm on the same input strings as in Example 13.1. Note the use of the failure\nfunction to avoid redoing one of the comparisons between a character of the pat-\ntern and a character of the text. Also note that the algorithm performs fewer overall\ncomparisons than the brute-force algorithm run on the same strings (Figure 13.1).\nFailure function:k 0 1 2 3 4 5\npattern[k]a b a c a b\nf(k) 0 0 1 0 1 2\na\na b c13\nb a a a b c19 18 17 16 15 14no comparison\nperformedText:\nPattern: b a a a b c6 5 4 3 2 1b b a a a b a c c a b a a c b a a b c\nb a a a b c7\nb a a a b c8 9 10 11 12\nb a a\nFigure 13.6: An illustration of the KMP pattern-matching algorithm. The primary\nalgorithm performs 19 character comparisons, which are indicated with numerical\nlabels. (Additional comparisons would be performed during the computation of the\nfailure function.)\nwww.it-ebooks.info"
  },
  {
    "page": 604,
    "content": "586 Chapter 13. Text Processing\n13.3 Tries\nThe pattern-matching algorithms presented in Section 13.2 speed up the search in a\ntext by preprocessing the pattern (to compute the lastfunction in the Boyer-Moore\nalgorithm or the failure function in the Knuth-Morris-Pratt algorithm). In this sec-\ntion, we take a complementary approach, namely, we present string searching al-\ngorithms that preprocess the text, rather than the pattern. This approach is suitable\nfor applications in which many queries are performed on a ﬁxed text, so that the\ninitial cost of preprocessing the text is compensated by a speedup in each subse-\nquent query (for example, a website that offers pattern matching in Shakespeare’s\nHamlet or a search engine that offers Web pages containing the term Hamlet ).\nAtrie(pronounced “try”) is a tree-based data structure for storing strings in\norder to support fast pattern matching. The main application for tries is in infor-\nmation retrieval. Indeed, the name “trie” comes from the word “re trieval.” In an\ninformation retrieval application, such as a search for a certain DNA sequence in a\ngenomic database, we are given a collection Sof strings, all deﬁned using the same\nalphabet. The primary query operations that tries support are pattern matching and\npreﬁx matching . The latter operation involves being given a string X, and looking\nfor all the strings in Sthat being with X.\n13.3.1 Standard Tries\nLetSbe a set of sstrings from alphabet Σsuch that no string in Sis a preﬁx\nof another string. A standard trie forSis an ordered tree Twith the following\nproperties (see Figure 13.7):\n•Each node of T, except the root, is labeled with a character of Σ.\n•The children of an internal node of Thave distinct labels.\n•Thassleaves, each associated with a string of S, such that the concatenation\nof the labels of the nodes on the path from the root to a leaf vofTyields the\nstring of Sassociated with v.\nThus, a trie Trepresents the strings of Swith paths from the root to the leaves\nofT. Note the importance of assuming that no string in Sis a preﬁx of another\nstring. This ensures that each string of Sis uniquely associated with a leaf of T.\n(This is similar to the restriction for preﬁx codes with Huffman coding, as described\nin Section 13.4.) We can always satisfy this assumption by adding a special char-\nacter that is not in the original alphabet Σat the end of each string.\nAn internal node in a standard trie Tcan have anywhere between 1 and |Σ|\nchildren. There is an edge going from the root rto one of its children for each\ncharacter that is ﬁrst in some string in the collection S. In addition, a path from\nthe root of Tto an internal node vat depth kcorresponds to a k-character preﬁx\nwww.it-ebooks.info"
  },
  {
    "page": 605,
    "content": "13.3. Tries 587\nb\ne\nli\nld\nlyu e\nc\nko arls\nlt\nl\np\nFigure 13.7: Standard trie for the strings {bear, bell, bid, bull, buy, sell, stock, stop }.\nX[0..k−1]of a string XofS. In fact, for each character cthat can follow the preﬁx\nX[0..k−1]in a string of the set S, there is a child of vlabeled with character c.\nIn this way, a trie concisely stores the common preﬁxes that exist among a set of\nstrings.\nAs a special case, if there are only two characters in the alphabet, then the\ntrie is essentially a binary tree, with some internal nodes possibly having only onechild (that is, it may be an improper binary tree). In general, although it is possiblethat an internal node has up to |Σ|children, in practice the average degree of such\nnodes is likely to be much smaller. For example, the trie shown in Figure 13.7 has\nseveral internal nodes with only one child. On larger data sets, the average degree\nof nodes is likely to get smaller at greater depths of the tree, because there maybe fewer strings sharing the common preﬁx, and thus fewer continuations of thatpattern. Furthermore, in many languages, there will be character combinations that\nare unlikely to naturally occur.\nThe following proposition provides some important structural properties of a\nstandard trie:\nProposition 13.4:\nA standard trie storing a collection Sofsstrings of total length\nnfrom an alphabet Σhas the following properties:\n•The height of Tis equal to the length of the longest string in S.\n•Every internal node of Thas at most|Σ|children.\n•Thassleaves.\n•The number of nodes of Tis at most n+1.\nThe worst case for the number of nodes of a trie occurs when no two strings\nshare a common nonempty preﬁx; that is, except for the root, all internal nodes\nhave one child.\nwww.it-ebooks.info"
  },
  {
    "page": 606,
    "content": "588 Chapter 13. Text Processing\nA trie Tfor a set Sof strings can be used to implement a set or map whose keys\nare the strings of S. Namely, we perform a search in Tfor a string Xby tracing\ndown from the root the path indicated by the characters in X. If this path can be\ntraced and terminates at a leaf node, then we know Xis a string in S. For example,\nin the trie in Figure 13.7, tracing the path for “bull” ends up at a leaf. If the path\ncannot be traced or the path can be traced but terminates at an internal node, then\nXis not a string in S. In the example in Figure 13.7, the path for “bet” cannot be\ntraced and the path for “be” ends at an internal node. Neither such word is in the\nsetS.\nIt is easy to see that the running time of the search for a string of length mis\nO(m·|Σ|), because we visit at most m+1 nodes of Tand we spend O(|Σ|)time\nat each node determining the child having the subsequent character as a label. TheO(|Σ|)upper bound on the time to locate a child with a given label is achievable,\neven if the children of a node are unordered, since there are at most |Σ|children.\nWe can improve the time spent at a node to be O(log|Σ|)or expected O(1), by\nmapping characters to children using a secondary search table or hash table at eachnode, or by using a direct lookup table of size |Σ|at each node, if|Σ|is sufﬁciently\nsmall (as is the case for DNA strings). For these reasons, we typically expect asearch for a string of length mto run in O(m)time.\nFrom the discussion above, it follows that we can use a trie to perform a spe-\ncial type of pattern matching, called word matching , where we want to determine\nwhether a given pattern matches one of the words of the text exactly. Word match-ing differs from standard pattern matching because the pattern cannot match an\narbitrary substring of the text—only one of its words. To accomplish this, each\nword of the original document must be added to the trie. (See Figure 13.8.) A sim-ple extension of this scheme supports preﬁx-matching queries. However, arbitraryoccurrences of the pattern in the text (for example, the pattern is a proper sufﬁx of\na word or spans two words) cannot be efﬁciently performed.\nTo construct a standard trie for a set Sof strings, we can use an incremental\nalgorithm that inserts the strings one at a time. Recall the assumption that no string\nofSis a preﬁx of another string. To insert a string Xinto the current trie T, we\ntrace the path associated with XinT, creating a new chain of nodes to store the\nremaining characters of Xwhen we get stuck. The running time to insert Xwith\nlength mis similar to a search, with worst-case O(m·|Σ|)performance, or expected\nO(m)if using secondary hash tables at each node. Thus, constructing the entire trie\nfor set Stakes expected O(n)time, where nis the total length of the strings of S.\nThere is a potential space inefﬁciency in the standard trie that has prompted the\ndevelopment of the compressed trie , which is also known (for historical reasons)\nas the Patricia trie . Namely, there are potentially a lot of nodes in the standard trie\nthat have only one child, and the existence of such nodes is a waste. We discuss the\ncompressed trie next.\nwww.it-ebooks.info"
  },
  {
    "page": 611,
    "content": "13.3. Tries 593\ne\nzeze\nmizei\nnimize ze nimizemi nimize\n(a)\n0..1 6..7\n6..7 2..7 2..72..7 1..1\n6..77..7\n4..7\ne0 1 2 3 4 5 6 7\nm i n i m i z\n(b)\nFigure 13.11: (a) Sufﬁx trie Tfor the string X=\"minimize\" . (b) Compact repre-\nsentation of T, where pair j..kdenotes the substring X[j..k]in the reference string.\nUsing a Suﬃx Trie\nThe sufﬁx trie Tfor a string Xcan be used to efﬁciently perform pattern-matching\nqueries on text X. Namely, we can determine whether a pattern is a substring of X\nby trying to trace a path associated with PinT.Pis a substring of Xif and only\nif such a path can be traced. The search down the trie Tassumes that nodes in T\nstore some additional information, with respect to the compact representation of\nthe sufﬁx trie:\nIf node vhas label j..k and Yis the string of length yassociated with\nthe path from the root to v(included), then X[k−y+1..k]=Y.\nThis property ensures that we can compute the start index of the pattern in the textwhen a match occurs in O(m)time.\nwww.it-ebooks.info"
  },
  {
    "page": 612,
    "content": "594 Chapter 13. Text Processing\n13.3.4 Search Engine Indexing\nThe World Wide Web contains a huge collection of text documents (Web pages).\nInformation about these pages are gathered by a program called a Web crawler ,\nwhich then stores this information in a special dictionary database. A Web search\nengine allows users to retrieve relevant information from this database, thereby\nidentifying relevant pages on the Web containing given keywords. In this section,\nwe will present a simpliﬁed model of a search engine.\nInverted Files\nThe core information stored by a search engine is a map, called an inverted index or\ninverted ﬁle , storing key-value pairs (w,L), where wis a word and Lis a collection\nof pages containing word w. The keys (words) in this map are called index terms\nand should be a set of vocabulary entries and proper nouns as large as possible. The\nvalues in this map are called occurrence lists and should cover as many Web pages\nas possible.\nWe can efﬁciently implement an inverted index with a data structure consisting\nof the following:\n1.An array storing the occurrence lists of the terms (in no particular order).\n2.A compressed trie for the set of index terms, where each leaf stores the index\nof the occurrence list of the associated term.\nThe reason for storing the occurrence lists outside the trie is to keep the size of the\ntrie data structure sufﬁciently small to ﬁt in internal memory. Instead, because of\ntheir large total size, the occurrence lists have to be stored on disk.\nWith our data structure, a query for a single keyword is similar to a word-\nmatching query (Section 13.3.1). Namely, we ﬁnd the keyword in the trie and we\nreturn the associated occurrence list.\nWhen multiple keywords are given and the desired output are the pages con-\ntaining allthe given keywords, we retrieve the occurrence list of each keyword\nusing the trie and return their intersection. To facilitate the intersection computa-\ntion, each occurrence list should be implemented with a sequence sorted by address\nor with a map, to allow efﬁcient set operations.\nIn addition to the basic task of returning a list of pages containing given key-\nwords, search engines provide an important additional service by ranking the pages\nreturned by relevance. Devising fast and accurate ranking algorithms for search\nengines is a major challenge for computer researchers and electronic commerce\ncompanies.\nwww.it-ebooks.info"
  },
  {
    "page": 613,
    "content": "13.4. Text Compression and the Greedy Method 595\n13.4 Text Compression and the Greedy Method\nIn this section, we will consider the important task of text compression . In this\nproblem, we are given a string Xdeﬁned over some alphabet, such as the ASCII\nor Unicode character sets, and we want to efﬁciently encode Xinto a small binary\nstring Y(using only the characters 0 and 1). Text compression is useful in any\nsituation where we wish to reduce bandwidth for digital communications, so as to\nminimize the time needed to transmit our text. Likewise, text compression is useful\nfor storing large documents more efﬁciently, so as to allow a ﬁxed-capacity storage\ndevice to contain as many documents as possible.\nThe method for text compression explored in this section is the Huffman code .\nStandard encoding schemes, such as ASCII, use ﬁxed-length binary strings to en-\ncode characters (with 7 or 8 bits in the traditional or extended ASCII systems,\nrespectively). The Unicode system was originally proposed as a 16-bit ﬁxed-\nlength representation, although common encodings reduce the space usage by al-\nlowing common groups of characters, such as those from the ASCII system, with\nfewer bits. The Huffman code saves space over a ﬁxed-length encoding by using\nshort code-word strings to encode high-frequency characters and long code-word\nstrings to encode low-frequency characters. Furthermore, the Huffman code uses\na variable-length encoding speciﬁcally optimized for a given string Xover any al-\nphabet. The optimization is based on the use of character frequencies , where we\nhave, for each character c, a count f(c)of the number of times cappears in the\nstring X.\nTo encode the string X, we convert each character in Xto a variable-length\ncode-word, and we concatenate all these code-words in order to produce the en-\ncoding YforX. In order to avoid ambiguities, we insist that no code-word in our\nencoding be a preﬁx of another code-word in our encoding. Such a code is called\napreﬁx code , and it simpliﬁes the decoding of Yto retrieve X. (See Figure 13.12.)\nEven with this restriction, the savings produced by a variable-length preﬁx code\ncan be signiﬁcant, particularly if there is a wide variance in character frequencies\n(as is the case for natural language text in almost every written language).\nHuffman’s algorithm for producing an optimal variable-length preﬁx code for\nXis based on the construction of a binary tree Tthat represents the code. Each\nedge in Trepresents a bit in a code-word, with an edge to a left child representing\na “0” and an edge to a right child representing a “1”. Each leaf vis associated\nwith a speciﬁc character, and the code-word for that character is deﬁned by the\nsequence of bits associated with the edges in the path from the root of Ttov. (See\nFigure 13.12.) Each leaf vhas a frequency ,f(v), which is simply the frequency in\nXof the character associated with v. In addition, we give each internal node vinT\na frequency, f(v), that is the sum of the frequencies of all the leaves in the subtree\nrooted at v.\nwww.it-ebooks.info"
  },
  {
    "page": 614,
    "content": "596 Chapter 13. Text Processing\n(a)Character abdefhiknorstuv\nFrequency 9513731114151211\n(b)46\n5\nk\n1i\n12\no\n1248\nt\n2\ns\n115\nn\n47\nf\n34\nv\n1u\n125\nb\n12\nh\n1d\n312\ne\n727 19\na\n510\n9\nr\nFigure 13.12: An illustration of an example Huffman code for the input string\nX=\"a fast runner need never be afraid of the dark\" : (a) frequency\nof each character of X; (b) Huffman tree Tfor string X. The code for a character c\nis obtained by tracing the path from the root of Tto the leaf where cis stored, and\nassociating a left child with 0 and a right child with 1. For example, the code for\n\"r\"is 011, and the code for \"h\"is 10111.\n13.4.1 The Huﬀman Coding Algorithm\nThe Huffman coding algorithm begins with each of the ddistinct characters of the\nstring Xto encode being the root node of a single-node binary tree. The algorithm\nproceeds in a series of rounds. In each round, the algorithm takes the two binary\ntrees with the smallest frequencies and merges them into a single binary tree. It\nrepeats this process until only one tree is left. (See Code Fragment 13.5.)\nEach iteration of the while loop in Huffman’s algorithm can be implemented\ninO(logd)time using a priority queue represented with a heap. In addition, each\niteration takes two nodes out of Qand adds one in, a process that will be repeated\nd−1 times before exactly one node is left in Q. Thus, this algorithm runs in\nO(n+dlogd)time. Although a full justiﬁcation of this algorithm’s correctness is\nbeyond our scope here, we note that its intuition comes from a simple idea—any\noptimal code can be converted into an optimal code in which the code-words for the\ntwo lowest-frequency characters, aandb, differ only in their last bit. Repeating the\nargument for a string with aandbreplaced by a character c, gives the following:\nProposition 13.7: Huffman’s algorithm constructs an optimal preﬁx code for a\nstring of length nwith ddistinct characters in O(n+dlogd)time.\nwww.it-ebooks.info"
  },
  {
    "page": 615,
    "content": "13.4. Text Compression and the Greedy Method 597\nAlgorithm Huﬀman(X):\nInput: String Xof length nwith ddistinct characters\nOutput: Coding tree for X\nCompute the frequency f(c)of each character cofX.\nInitialize a priority queue Q.\nfor each character cinXdo\nCreate a single-node binary tree Tstoring c.\nInsert TintoQwith key f(c).\nwhile Q.size()>1do\nEntry e1=Q.removeMin () with e1having key f1and value T1.\nEntry e2=Q.removeMin () with e2having key f2and value T2.\nCreate a new binary tree Twith left subtree T1and right subtree T2.\nInsert TintoQwith key f1+f2.\nEntry e=Q.removeMin () with ehaving tree Tas its value.\nreturn treeT\nCode Fragment 13.5: Huffman coding algorithm.\n13.4.2 The Greedy Method\nHuffman’s algorithm for building an optimal encoding is an example application\nof an algorithmic design pattern called the greedy method . This design pattern is\napplied to optimization problems, where we are trying to construct some structure\nwhile minimizing or maximizing some property of that structure.\nThe general formula for the greedy-method pattern is almost as simple as that\nfor the brute-force method. In order to solve a given optimization problem using\nthe greedy method, we proceed by a sequence of choices. The sequence starts\nfrom some well-understood starting condition, and computes the cost for that ini-\ntial condition. The pattern then asks that we iteratively make additional choices\nby identifying the decision that achieves the best cost improvement from all of\nthe choices that are currently possible. This approach does not always lead to an\noptimal solution.\nBut there are several problems that it does work for, and such problems are said\nto possess the greedy-choice property. This is the property that a global optimal\ncondition can be reached by a series of locally optimal choices (that is, choices\nthat are each the current best from among the possibilities available at the time),\nstarting from a well-deﬁned starting condition. The problem of computing an opti-\nmal variable-length preﬁx code is just one example of a problem that possesses the\ngreedy-choice property.\nwww.it-ebooks.info"
  },
  {
    "page": 616,
    "content": "598 Chapter 13. Text Processing\n13.5 Dynamic Programming\nIn this section, we will discuss the dynamic-programming algorithmic design pat-\ntern. This technique is similar to the divide-and-conquer technique (Section 12.1.1),\nin that it can be applied to a wide variety of different problems. Dynamic program-\nming can often be used to produce polynomial-time algorithms to solve problems\nthat seem to require exponential time. In addition, the algorithms that result from\napplications of the dynamic programming technique are usually quite simple, often\nneeding little more than a few lines of code to describe some nested loops for ﬁlling\nin a table.\n13.5.1 Matrix Chain-Product\nRather than starting out with an explanation of the general components of the dy-\nnamic programming technique, we begin by giving a classic, concrete example.\nSuppose we are given a collection of ntwo-dimensional matrices for which we\nwish to compute the mathematical product\nA=A0·A1·A2···An−1,\nwhere Aiis a di×di+1matrix, for i=0,1,2,..., n−1. In the standard matrix\nmultiplication algorithm (which is the one we will use), to multiply a d×e-matrix B\ntimes an e×f-matrix C, we compute the product, A, as\nA[i][j]=e−1\n∑\nk=0B[i][k]·C[k][j].\nThis deﬁnition implies that matrix multiplication is associative, that is, it implies\nthatB·(C·D) = ( B·C)·D. Thus, we can parenthesize the expression for Aany\nway we wish and we will end up with the same answer. However, we will not\nnecessarily perform the same number of primitive (that is, scalar) multiplications\nin each parenthesization, as is illustrated in the following example.\nExample 13.8: LetBbe a 2×10-matrix, let Cbe a 10×50-matrix, and let Dbe\na50×20-matrix. Computing B·(C·D)requires 2·10·20+10·50·20=10400\nmultiplications, whereas computing (B·C)·Drequires 2·10·50+2·50·20=3000\nmultiplications.\nThematrix chain-product problem is to determine the parenthesization of the\nexpression deﬁning the product Athat minimizes the total number of scalar mul-\ntiplications performed. As the example above illustrates, the differences between\nparenthesizations can be dramatic, so ﬁnding a good solution can result in signiﬁ-\ncant speedups.\nwww.it-ebooks.info"
  },
  {
    "page": 617,
    "content": "13.5. Dynamic Programming 599\nDeﬁning Subproblems\nOne way to solve the matrix chain-product problem is to simply enumerate all the\npossible ways of parenthesizing the expression for Aand determine the number\nof multiplications performed by each one. Unfortunately, the set of all differentparenthesizations of the expression for Ais equal in number to the set of all dif-\nferent binary trees that have nleaves. This number is exponential in n. Thus, this\nstraightforward (“brute-force”) algorithm runs in exponential time, for there are anexponential number of ways to parenthesize an associative arithmetic expression.\nWe can signiﬁcantly improve the performance achieved by the brute-force al-\ngorithm, however, by making a few observations about the nature of the matrixchain-product problem. The ﬁrst is that the problem can be split into subproblems .\nIn this case, we can deﬁne a number of different subproblems, each of which is tocompute the best parenthesization for some subexpression A\ni·Ai+1···Aj. As a con-\ncise notation, we use Ni,jto denote the minimum number of multiplications needed\nto compute this subexpression. Thus, the original matrix chain-product problemcan be characterized as that of computing the value of N\n0,n−1. This observation\nis important, but we need one more in order to apply the dynamic programmingtechnique.\nCharacterizing Optimal Solutions\nThe other important observation we can make about the matrix chain-product prob-lem is that it is possible to characterize an optimal solution to a particular subprob-lem in terms of optimal solutions to its subproblems. We call this property thesubproblem optimality condition.\nIn the case of the matrix chain-product problem, we observe that, no mat-\nter how we parenthesize a subexpression, there has to be some ﬁnal matrix mul-tiplication that we perform. That is, a full parenthesization of a subexpression\nA\ni·Ai+1···Ajhas to be of the form (Ai···Ak)·(Ak+1···Aj), for some k∈{i,i+\n1,..., j−1}. Moreover, for whichever kis the correct one, the products (Ai···Ak)\nand(Ak+1···Aj)must also be solved optimally. If this were not so, then there would\nbe a global optimal that had one of these subproblems solved suboptimally. But thisis impossible, since we could then reduce the total number of multiplications by re-\nplacing the current subproblem solution by an optimal solution for the subproblem.This observation implies a way of explicitly deﬁning the optimization problem forN\ni,jin terms of other optimal subproblem solutions. Namely, we can compute Ni,j\nby considering each place kwhere we could put the ﬁnal multiplication and taking\nthe minimum over all such choices.\nwww.it-ebooks.info"
  },
  {
    "page": 618,
    "content": "600 Chapter 13. Text Processing\nDesigning a Dynamic Programming Algorithm\nWe can therefore characterize the optimal subproblem solution, Ni,j, as\nNi,j=min\ni≤k<j{Ni,k+Nk+1,j+didk+1dj+1},\nwhere Ni,i=0, since no work is needed for a single matrix. That is, Ni,jis the\nminimum, taken over all possible places to perform the ﬁnal multiplication, of the\nnumber of multiplications needed to compute each subexpression plus the number\nof multiplications needed to perform the ﬁnal matrix multiplication.\nNotice that there is a sharing of subproblems going on that prevents us from\ndividing the problem into completely independent subproblems (as we would need\nto do to apply the divide-and-conquer technique). We can, nevertheless, use the\nequation for Ni,jto derive an efﬁcient algorithm by computing Ni,jvalues in a\nbottom-up fashion, and storing intermediate solutions in a table of Ni,jvalues. We\ncan begin simply enough by assigning Ni,i=0 for i=0,1,..., n−1. We can then\napply the general equation for Ni,jto compute Ni,i+1values, since they depend only\nonNi,iandNi+1,i+1values that are available. Given the Ni,i+1values, we can then\ncompute the Ni,i+2values, and so on. Therefore, we can build Ni,jvalues up from\npreviously computed values until we can ﬁnally compute the value of N0,n−1, which\nis the number that we are searching for. A Java implementation of this dynamic\nprogramming solution is given in Code Fragment 13.6; we use techniques from\nSection 3.1.5 for working with a two-dimensional array in Java.\n1public static int [ ][ ] matrixChain( int[ ] d){\n2intn = d.length−1; // number of matrices\n3int[ ][ ] N = new int[n][n]; // n-by-n matrix; initially zeros\n4for(intb=1; b<n; b++) // number of products in subchain\n5for(inti=0; i<n−b; i++){ // start of subchain\n6 intj = i + b; // end of subchain\n7 N[i][j] = Integer.MAX VALUE; // used as ’inﬁnity’\n8 for(intk=i; k<j; k++)\n9 N[i][j] = Math.min(N[i][j], N[i][k] + N[k+1][j] + d[i] ∗d[k+1] ∗d[j+1]);\n10}\n11returnN;\n12}\nCode Fragment 13.6: Dynamic programming algorithm for the matrix chain-\nproduct problem.\nThus, we can compute N0,n−1with an algorithm that consists primarily of three\nnested loops (the third of which computes the min term). Each of these loops\niterates at most ntimes per execution, with a constant amount of additional work\nwithin. Therefore, the total running time of this algorithm is O(n3).\nwww.it-ebooks.info"
  },
  {
    "page": 619,
    "content": "13.5. Dynamic Programming 601\n13.5.2 DNA and Text Sequence Alignment\nA common text-processing problem, which arises in genetics and software engi-\nneering, is to test the similarity between two text strings. In a genetics application,\nthe two strings could correspond to two strands of DNA, for which we want to com-\npute similarities. Likewise, in a software engineering application, the two strings\ncould come from two versions of source code for the same program, for which we\nwant to determine changes made from one version to the next. Indeed, determining\nthe similarity between two strings is so common that the Unix and Linux operating\nsystems have a built-in program, named diff , for comparing text ﬁles.\nGiven a string X=x0x1x2···xn−1, asubsequence ofXis any string that is of\nthe form xi1xi2···xik, where ij<ij+1; that is, it is a sequence of characters that are\nnot necessarily contiguous but are nevertheless taken in order from X. For example,\nthe string AAAG is a subsequence of the string CGA T AA T T G AGA .\nThe DNA and text similarity problem we address here is the longest common\nsubsequence (LCS) problem. In this problem, we are given two character strings,\nX=x0x1x2···xn−1andY=y0y1y2···ym−1, over some alphabet (such as the al-\nphabet{A,C,G,T}common in computational genomics) and are asked to ﬁnd a\nlongest string Sthat is a subsequence of both XandY. One way to solve the\nlongest common subsequence problem is to enumerate all subsequences of Xand\ntake the largest one that is also a subsequence of Y. Since each character of Xis\neither in or not in a subsequence, there are potentially 2ndifferent subsequences of\nX, each of which requires O(m)time to determine whether it is a subsequence of Y.\nThus, this brute-force approach yields an exponential-time algorithm that runs in\nO(2nm)time, which is very inefﬁcient. Fortunately, the LCS problem is efﬁciently\nsolvable using dynamic programming .\nThe Components of a Dynamic Programming Solution\nAs mentioned above, the dynamic programming technique is used primarily for\noptimization problems, where we wish to ﬁnd the “best” way of doing something.\nWe can apply the dynamic programming technique in such situations if the problem\nhas certain properties:\nSimple Subproblems: There has to be some way of repeatedly breaking the global\noptimization problem into subproblems. Moreover, there should be a way to\nparameterize subproblems with just a few indices, like i,j,k, and so on.\nSubproblem Optimization: An optimal solution to the global problem must be a\ncomposition of optimal subproblem solutions.\nSubproblem Overlap: Optimal solutions to unrelated subproblems can contain\nsubproblems in common.\nwww.it-ebooks.info"
  },
  {
    "page": 620,
    "content": "602 Chapter 13. Text Processing\nApplying Dynamic Programming to the LCS Problem\nRecall that in the LCS problem, we are given two character strings, XandY, of\nlength nandm, respectively, and are asked to ﬁnd a longest string Sthat is a sub-\nsequence of both XandY. Since XandYare character strings, we have a natural\nset of indices with which to deﬁne subproblems—indices into the strings XandY.\nLet us deﬁne a subproblem, therefore, as that of computing the value Lj,k, which\nwe will use to denote the length of a longest string that is a subsequence of both the\nﬁrst jcharacters of Xand the ﬁrst kcharacters of Y, that is of preﬁxes X[0..j−1]\nandY[0..k−1]. If either j=0 ork=0, then Lj,kis trivially deﬁned as 0.\nWhen both j≥1 and k≥1, this deﬁnition allows us to rewrite Lj,krecursively\nin terms of optimal subproblem solutions. This deﬁnition depends on which of twocases we are in. (See Figure 13.13.)\n•x\nj−1=yk−1. In this case, we have a match between the last character of\nX[0..j−1]and the last character of Y[0..k−1]. We claim that this character\nbelongs to a longest common subsequence of X[0..j−1]andY[0..k−1].\nTo justify this claim, let us suppose it is not true. There has to be some\nlongest common subsequence xa1xa2...xac=yb1yb2...ybc. Ifxac=xj−1or\nybc=yk−1, then we get the same sequence by setting ac=j−1 and bc=\nk−1. Alternately, if xac/n⌉}ationslash=xj−1andybc/n⌉}ationslash=yk−1, then we can get an even\nlonger common subsequence by adding xj−1=yk−1to the end. Thus, a\nlongest common subsequence of X[0..j−1]andY[0..k−1]ends with xj−1.\nTherefore, we set\nLj,k=1+Lj−1,k−1ifxj−1=yk−1.\n•xj−1/n⌉}ationslash=yk−1. In this case, we cannot have a common subsequence that in-\ncludes both xj−1andyk−1. That is, we can have a common subsequence end\nwith xj−1or one that ends with yk−1(or possibly neither), but certainly not\nboth. Therefore, we set\nLj,k=max{Lj−1,k,Lj,k−1}ifxj−1/n⌉}ationslash=yk−1.\n6 7 8 9 10 11G T T C C T AA T A\nC A TA A T TG G AGA0 1 2 3 4 5 6 7 8 9\nX =\n0Y =\n1 2 3 4 5 5 6 7 8 9 10G T T C C T AA T\nC A TA A T T G G GA0 1 2 3 4 5 6 7 8\nX =\n0Y =\n1 2 3 4\nL10,12=1+L9,11 L9,11=max(L9,10,L8,11)\n(a) (b)\nFigure 13.13: The two cases in the longest common subsequence algorithm for\ncomputing Lj,kwhen j,k≥1: (a) xj−1=yk−1; (b) xj−1/n⌉}ationslash=yk−1.\nwww.it-ebooks.info"
  },
  {
    "page": 621,
    "content": "13.5. Dynamic Programming 603\nThe LCS Algorithm\nThe deﬁnition of Lj,ksatisﬁes subproblem optimization, for we cannot have a\nlongest common subsequence without also having longest common subsequences\nfor the subproblems. Also, it uses subproblem overlap, because a subproblem so-\nlution Lj,kcan be used in several other problems (namely, the problems Lj+1,k,\nLj,k+1, and Lj+1,k+1). Turning this deﬁnition of Lj,kinto an algorithm is actually\nquite straightforward. We create an (n+1)×(m+1)array, L, deﬁned for 0≤j≤n\nand 0≤k≤m. We initialize all entries to 0, in particular so that all entries of the\nform Lj,0andL0,kare zero. Then, we iteratively build up values in Luntil we have\nLn,m, the length of a longest common subsequence of XandY. We give a Java\nimplementation of this algorithm in Code Fragment 13.7.\n1/∗∗Returns table such that L[j][k] is length of LCS for X[0..j −1] and Y[0..k−1].∗/\n2public static int [ ][ ] LCS( char[ ] X,char[ ] Y){\n3intn = X.length;\n4intm = Y.length;\n5int[ ][ ] L = new int[n+1][m+1];\n6for(intj=0; j<n; j++)\n7for(intk=0; k<m; k++)\n8 if(X[j] == Y[k]) // align this match\n9 L[j+1][k+1] = L[j][k] + 1;\n10 else // choose to ignore one character\n11 L[j+1][k+1] = Math.max(L[j][k+1], L[j+1][k]);\n12returnL;\n13}\nCode Fragment 13.7: Dynamic programming algorithm for the LCS problem.\nThe running time of the algorithm of the LCS algorithm is easy to analyze,\nfor it is dominated by two nested forloops, with the outer one iterating ntimes\nand the inner one iterating mtimes. Since the if-statement and assignment inside\nthe loop each requires O(1)primitive operations, this algorithm runs in O(nm)\ntime. Thus, the dynamic programming technique can be applied to the longestcommon subsequence problem to improve signiﬁcantly over the exponential-time\nbrute-force solution to the LCS problem.\nTheLCS method of Code Fragment 13.7 computes the length of the longest\ncommon subsequence (stored as L\nn,m), but not the subsequence itself. Fortunately,\nit is easy to extract the actual longest common subsequence if given the complete ta-\nble of Lj,kvalues computed by the LCS method. The solution can be reconstructed\nback to front by reverse engineering the calculation of length Ln,m. At any position\nLj,k, ifxj=yk, then the length is based on the common subsequence associated\nwith length Lj−1,k−1, followed by common character xj. We can record xjas part\nof the sequence, and then continue the analysis from Lj−1,k−1. Ifxj/n⌉}ationslash=yk, then we\ncan move to the larger of Lj,k−1andLj−1,k. We continue this process until reaching\nwww.it-ebooks.info"
  },
  {
    "page": 622,
    "content": "604 Chapter 13. Text Processing\nsome Lj,k=0 (for example, if jorkis 0 as a boundary case). A Java implemen-\ntation of this strategy is given in Code Fragment 13.8. This method constructs a\nlongest common subsequence in O(n+m)additional time, since each pass of the\nwhile loop decrements either jork(or both). An illustration of the algorithm for\ncomputing the longest common subsequence is given in Figure 13.14.\n1/∗∗Returns the longest common substring of X and Y, given LCS table L. ∗/\n2public static char [ ] reconstructLCS( char[ ] X,char[ ] Y,int[ ][ ] L){\n3StringBuilder solution = newStringBuilder();\n4intj = X.length;\n5intk = Y.length;\n6while(L[j][k]>0) // common characters remain\n7if(X[j−1] == Y[k −1]){\n8 solution.append(X[j −1]);\n9 j−−;\n10 k−−;\n11}else if(L[j−1][k]>= L[j][k−1])\n12 j−−;\n13else\n14 k−−;\n15// return left-to-right version, as char array\n16returnsolution.reverse().toString().toCharArray();\n17}\nCode Fragment 13.8: Reconstructing the longest common subsequence.\n111 1 1 1 1 1 1 1 1\n0 0 1 1 2 2 22 2 2 2 2 2\n0 0 1 1 2 2 2 3 3 3 3 3 3\n0 1 1 1 2 2 2 3 3 3 3 3 3\n0 1 1 1 2 2 2 33 3 3 3 3\n0 1 1 1 2 2 2 3 4 44 4 4\n0 1 1 2 2 3 3 3 4 4 5 5 5\n0 1 1 2 3 3 4 5 5 5 5 56\n0 1 1 2 3 40\n5 5 5 6 6 60 1 1 2 2 3 4 4 4 4 5 5 60\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 41234 5 6 7 89101112\n0 0 0 0 0 0 0 0 0 0 0 00\n0 0\n6 7 8 9 10 11G T T C C T A A T A\nC A TA A T TG G AGA Y =0 1 2 3 4 5 6 7 8 9\n0X =\n1 2 3 4 5\nFigure 13.14: Illustration of the algorithm for constructing a longest common sub-\nsequence from the array L. A diagonal step on the highlighted path represents the\nuse of a common character (with that character’s respective indices in the sequences\nhighlighted in the margins).\nwww.it-ebooks.info"
  },
  {
    "page": 623,
    "content": "13.6. Exercises 605\n13.6 Exercises\nReinforcement\nR-13.1 List the preﬁxes of the string P=\"aaabbaaa\" that are also sufﬁxes of P.\nR-13.2 What is the longest (proper) preﬁx of the string \"cgtacgttcgtacg\" that is also\na sufﬁx of this string?\nR-13.3 Draw a ﬁgure illustrating the comparisons done by brute-force pattern matching\nfor the text \"aaabaadaabaaa\" and pattern \"aabaaa\" .\nR-13.4 Repeat the previous problem for the Boyer-Moore algorithm, not counting the\ncomparisons made to compute the last(c)function.\nR-13.5 Repeat Exercise R-13.3 for the Knuth-Morris-Pratt algorithm, not counting the\ncomparisons made to compute the failure function.\nR-13.6 Compute a map representing the lastfunction used in the Boyer-Moore pattern-\nmatching algorithm for characters in the pattern string:\n\"the quick brown fox jumped over a lazy cat\" .\nR-13.7 Compute a table representing the Knuth-Morris-Pratt failure function for the pat-\ntern string \"cgtacgttcgtac\" .\nR-13.8 Draw a standard trie for the following set of strings:\n{abab, baba, ccccc, bbaaaa, caa, bbaacc, cbcc, cbca }.\nR-13.9 Draw a compressed trie for the strings given in the previous problem.\nR-13.10 Draw the compact representation of the sufﬁx trie for the string:\n\"minimize minime\" .\nR-13.11 Draw the frequency array and Huffman tree for the following string:\n\"dogs do not spot hot pots or cats\" .\nR-13.12 What is the best way to multiply a chain of matrices with dimensions that are\n10×5, 5×2, 2×20, 20×12, 12×4, and 4×60? Show your work.\nR-13.13 In Figure 13.14, we illustrate that GTTTAA is a longest common subsequence for\nthe given strings XandY. However, that answer is not unique. Give another\ncommon subsequence of XandYhaving length six.\nR-13.14 Show the longest common subsequence array Lfor the two strings:\nX=\"skullandbones\"\nY=\"lullabybabies\"\nWhat is a longest common subsequence between these strings?\nwww.it-ebooks.info"
  },
  {
    "page": 624,
    "content": "606 Chapter 13. Text Processing\nCreativity\nC-13.15 Describe an example of a text Tof length nand a pattern Pof length msuch\nthat the brute-force pattern-matching algorithm achieves a running time that is\nΩ(nm).\nC-13.16 Adapt the brute-force pattern-matching algorithm so as to implement a method\nﬁndLastBrute(T,P) that returns the index at which the rightmost occurrence of\npattern Pwithin text T, if any.\nC-13.17 Redo the previous problem, adapting the Boyer-Moore pattern-matching algo-\nrithm to implement a method ﬁndLastBoyerMoore(T,P) .\nC-13.18 Redo Exercise C-13.16, adapting the Knuth-Morris-Pratt pattern-matching algo-\nrithm appropriately to implement a method ﬁndLastKMP(T,P) .\nC-13.19 Give a justiﬁcation of why the computeFailKMP method (Code Fragment 13.4)\nruns in O(m)time on a pattern of length m.\nC-13.20 LetTbe a text of length n, and let Pbe a pattern of length m. Describe an O(n+\nm)-time method for ﬁnding the longest preﬁx of Pthat is a substring of T.\nC-13.21 Say that a pattern Pof length mis acircular substring of a text Tof length\nn>mifPis a (normal) substring of T, or if Pis equal to the concatenation\nof a sufﬁx of Tand a preﬁx of T, that is, if there is an index 0 ≤k<m, such\nthatP=T[n−m+k..n−1]+T[0..k−1]. Give an O(n+m)-time algorithm for\ndetermining whether Pis a circular substring of T.\nC-13.22 The Knuth-Morris-Pratt pattern-matching algorithm can be modiﬁed to run faster\non binary strings by redeﬁning the failure function as:\nf(k)=the largest j<ksuch that P[0..j−1]/hatwidepjis a sufﬁx of P[1..k],\nwhere/hatwidepjdenotes the complement of the jthbit of P. Describe how to modify the\nKMP algorithm to be able to take advantage of this new failure function and also\ngive a method for computing this failure function. Show that this method makes\nat most ncomparisons between the text and the pattern (as opposed to the 2 n\ncomparisons needed by the standard KMP algorithm given in Section 13.2.3).\nC-13.23 Modify the simpliﬁed Boyer-Moore algorithm presented in this chapter using\nideas from the KMP algorithm so that it runs in O(n+m)time.\nC-13.24 LetTbe a text string of length n. Describe an O(n)-time method for ﬁnding the\nlongest preﬁx of Tthat is a substring of the reversal of T.\nC-13.25 Describe an efﬁcient algorithm to ﬁnd the longest palindrome that is a sufﬁx of\na string Tof length n. Recall that a palindrome is a string that is equal to its\nreversal. What is the running time of your method?\nC-13.26 Give an efﬁcient algorithm for deleting a string from a standard trie and analyze\nits running time.\nC-13.27 Give an efﬁcient algorithm for deleting a string from a compressed trie and ana-\nlyze its running time.\nwww.it-ebooks.info"
  },
  {
    "page": 625,
    "content": "13.6. Exercises 607\nC-13.28 Describe an algorithm for constructing the compact representation of a sufﬁx trie,\ngiven its noncompact representation, and analyze its running time.\nC-13.29 Create a class that implements a standard trie for a set of strings. The class shouldhave a constructor that takes a list of strings as an argument, and the class should\nhave a method that tests whether a given string is stored in the trie.\nC-13.30 Create a class that implements a compressed trie for a set of strings. The classshould have a constructor that takes a list of strings as an argument, and the classshould have a method that tests whether a given string is stored in the trie.\nC-13.31 Create a class that implements a preﬁx trie for a string. The class should have aconstructor that takes a string as an argument, and a method for pattern matching\non the string.\nC-13.32 Given a string Xof length nand a string Yof length m, describe an O(n+m)-time\nalgorithm for ﬁnding the longest preﬁx of Xthat is a sufﬁx of Y.\nC-13.33 Describe an efﬁcient greedy algorithm for making change for a speciﬁed value\nusing a minimum number of coins, assuming there are four denominations of\ncoins (called quarters, dimes, nickels, and pennies), with values 25, 10, 5, and 1,\nrespectively. Argue why your algorithm is correct.\nC-13.34 Give an example set of denominations of coins so that a greedy change-makingalgorithm will not use the minimum number of coins.\nC-13.35 In the art gallery guarding problem we are given a line Lthat represents a long\nhallway in an art gallery. We are also given a set X={x\n0,x1,..., xn−1}of real\nnumbers that specify the positions of paintings in this hallway. Suppose that a\nsingle guard can protect all the paintings within distance at most 1 of his or her\nposition (on both sides). Design an algorithm for ﬁnding a placement of guardsthat uses the minimum number of guards to guard all the paintings with positions\ninX.\nC-13.36 Anna has just won a contest that allows her to take npieces of candy out of a\ncandy store for free. Anna is old enough to realize that some candy is expensive,\nwhile other candy is relatively cheap, costing much less. The jars of candy arenumbered 0, 1, ...,m−1, so that jar jhasn\njpieces in it, with a price of cj\nper piece. Design an O(n+m)-time algorithm that allows Anna to maximize the\nvalue of the pieces of candy she takes for her winnings. Show that your algorithmproduces the maximum value for Anna.\nC-13.37 Implement a compression and decompression scheme that is based on Huffmancoding.\nC-13.38 Design an efﬁcient algorithm for the matrix chain multiplication problem thatoutputs a fully parenthesized expression for how to multiply the matrices in the\nchain using the minimum number of operations.\nC-13.39 A native Australian named Anatjari wishes to cross a desert carrying only a sin-gle water bottle. He has a map that marks all the watering holes along the way.\nAssuming he can walk kmiles on one bottle of water, design an efﬁcient algo-\nrithm for determining where Anatjari should reﬁll his bottle in order to make as\nfew stops as possible. Argue why your algorithm is correct.\nwww.it-ebooks.info"
  },
  {
    "page": 626,
    "content": "608 Chapter 13. Text Processing\nC-13.40 Given a sequence S=(x0,x1,..., xn−1)of numbers, describe an O(n2)-time algo-\nrithm for ﬁnding a longest subsequence T=(xi0,xi1,..., xik−1)of numbers, such\nthatij<ij+1andxij>xij+1. That is, Tis a longest decreasing subsequence of S.\nC-13.41 LetPbe a convex polygon, a triangulation ofPis an addition of diagonals\nconnecting the vertices of Pso that each interior face is a triangle. The weight of\na triangulation is the sum of the lengths of the diagonals. Assuming that we can\ncompute lengths and add and compare them in constant time, give an efﬁcient\nalgorithm for computing a minimum-weight triangulation of P.\nC-13.42 Give an efﬁcient algorithm for determining if a pattern Pis a subsequence (not\nsubstring) of a text T. What is the running time of your algorithm?\nC-13.43 Deﬁne the edit distance between two strings XandYof length nandm, respec-\ntively, to be the number of edits that it takes to change XintoY. An edit consists\nof a character insertion, a character deletion, or a character replacement. For ex-\nample, the strings \"algorithm\" and\"rhythm\" have edit distance 6. Design an\nO(nm)-time algorithm for computing the edit distance between XandY.\nC-13.44 Write a program that takes two character strings (which could be, for example,\nrepresentations of DNA strands) and computes their edit distance, based on your\nalgorithm from the previous exercise.\nC-13.45 LetXandYbe strings of length nandm, respectively. Deﬁne B(j,k)to be the\nlength of the longest common substring of the sufﬁx X[n−j..n−1]and the sufﬁx\nY[m−k..m−1]. Design an O(nm)-time algorithm for computing all the values\nofB(j,k)forj=1,..., nandk=1,..., m.\nC-13.46 Let three integer arrays, A,B, and C, be given, each of size n. Given an arbi-\ntrary integer k, design an O(n2logn)-time algorithm to determine if there exist\nnumbers, ainA,binB, and cinC, such that k=a+b+c.\nC-13.47 Give an O(n2)-time algorithm for the previous problem.\nProjects\nP-13.48 Perform an experimental analysis of the efﬁciency (number of character compar-\nisons performed) of the brute-force and KMP pattern-matching algorithms for\nvarying-length patterns.\nP-13.49 Perform an experimental analysis of the efﬁciency (number of character com-\nparisons performed) of the brute-force and Boyer-Moore pattern-matching algo-\nrithms for varying-length patterns.\nP-13.50 Perform an experimental comparison of the relative speeds of the brute-force,\nKMP, and Boyer-Moore pattern-matching algorithms. Document the relative\nrunning times on large text documents that are then searched using varying-\nlength patterns.\nP-13.51 Experiment with the efﬁciency of the indexOf method of Java’s String class and\ndevelop a hypothesis about which pattern-matching algorithm it uses. Describe\nyour experiments and your conclusions.\nwww.it-ebooks.info"
  },
  {
    "page": 627,
    "content": "13.6. Exercises 609\nP-13.52 A very effective pattern-matching algorithm, developed by Rabin and Karp [54],\nrelies on the use of hashing to produce an algorithm with very good expected\nperformance. Recall that the brute-force algorithm compares the pattern to eachpossible placement in the text, spending O(m)time, in the worst case, for each\nsuch comparison. The premise of the Rabin-Karp algorithm is to compute a hashfunction, h(·), on the length- mpattern, and then to compute the hash function on\nall length- msubstrings of the text. The pattern Poccurs at substring, T[j..j+\nm−1], only if h(P)equals h(T[j..j+m−1]). If the hash values are equal, the\nauthenticity of the match at that location must then be veriﬁed with the brute-\nforce approach, since there is a possibility that there was a coincidental collision\nof hash values for distinct strings. But with a good hash function, there will bevery few such false matches.\nThe next challenge, however, is that computing a good hash function on a length-\nmsubstring would presumably require O(m)time. If we did this for each of\nO(n)possible locations, the algorithm would be no better than the brute-force\napproach. The trick is to rely on the use of a polynomial hash code, as originally\nintroduced in Section 10.2.1, such as\n(x\n0am−1+x1am−2+···+xn−2a+xm−1)mod p\nfor a substring (x0,x1,..., xm−1), randomly chosen a, and large prime p. We can\ncompute the hash value of each successive substring of the text in O(1)time each,\nby using the following formula\nh(T[j+1..j+m])=( a·h(T[j..j+m−1])−xjam+xj+m)mod p.\nImplement the Rabin-Karp algorithm and evaluate its efﬁciency.\nP-13.53 Implement the simpliﬁed search engine described in Section 13.3.4 for the pagesof a small Web site. Use all the words in the pages of the site as index terms,\nexcluding stop words such as articles, prepositions, and pronouns.\nP-13.54 Implement a search engine for the pages of a small Web site by adding a page-ranking feature to the simpliﬁed search engine described in Section 13.3.4. Your\npage-ranking feature should return the most relevant pages ﬁrst. Use all the words\nin the pages of the site as index terms, excluding stop words, such as articles,\nprepositions, and pronouns.\nP-13.55 Use the LCS algorithm to compute the best sequence alignment between someDNA strings, which you can get online from GenBank.\nP-13.56 Develop a spell-checker that uses edit distance (see Exercise C-13.43) to deter-\nmine which correctly spelled words are closest to a misspelling.\nwww.it-ebooks.info"
  },
  {
    "page": 628,
    "content": "610 Chapter 13. Text Processing\nChapter Notes\nThe KMP algorithm is described by Knuth, Morris, and Pratt in their journal article [62],\nand Boyer and Moore describe their algorithm in a journal article published the same\nyear [17]. In their article, however, Knuth et al. [62] also prove that the Boyer-Moore\nalgorithm runs in linear time. More recently, Cole [23] shows that the Boyer-Moore algo-\nrithm makes at most 3 ncharacter comparisons in the worst case, and this bound is tight.\nAll of the algorithms discussed above are also discussed in the book chapter by Aho [4],\nalbeit in a more theoretical framework, including the methods for regular-expression pat-\ntern matching. The reader interested in further study of string pattern-matching algorithms\nis referred to the book by Stephen [84] and the book chapters by Aho [4], and Crochemore\nand Lecroq [26]. The trie was invented by Morrison [74] and is discussed extensively in the\nclassic Sorting and Searching book by Knuth [61]. The name “Patricia” is short for “Prac-\ntical Algorithm to Retrieve Information Coded in Alphanumeric” [74]. McCreight [68]\nshows how to construct sufﬁx tries in linear time. Dynamic programming was developed\nin the operations research community and formalized by Bellman [12].\nwww.it-ebooks.info"
  },
  {
    "page": 629,
    "content": "Chapter\n14Graph Algorithms\nContents\n14.1 Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n14.1.1 The Graph ADT . . . . . . . . . . . . . . . . . . . . . . . 618\n14.2 Data Structures for Graphs . . . . . . . . . . . . . . . . . . 619\n14.2.1 Edge List Structure . . . . . . . . . . . . . . . . . . . . . 620\n14.2.2 Adjacency List Structure . . . . . . . . . . . . . . . . . . 622\n14.2.3 Adjacency Map Structure . . . . . . . . . . . . . . . . . . 624\n14.2.4 Adjacency Matrix Structure . . . . . . . . . . . . . . . . . 625\n14.2.5 Java Implementation . . . . . . . . . . . . . . . . . . . . 626\n14.3 Graph Traversals . . . . . . . . . . . . . . . . . . . . . . . . 630\n14.3.1 Depth-First Search . . . . . . . . . . . . . . . . . . . . . 631\n14.3.2 DFS Implementation and Extensions . . . . . . . . . . . . 636\n14.3.3 Breadth-First Search . . . . . . . . . . . . . . . . . . . . 640\n14.4 Transitive Closure . . . . . . . . . . . . . . . . . . . . . . . 643\n14.5 Directed Acyclic Graphs . . . . . . . . . . . . . . . . . . . . 647\n14.5.1 Topological Ordering . . . . . . . . . . . . . . . . . . . . 647\n14.6 Shortest Paths . . . . . . . . . . . . . . . . . . . . . . . . . 651\n14.6.1 Weighted Graphs . . . . . . . . . . . . . . . . . . . . . . 651\n14.6.2 Dijkstra’s Algorithm . . . . . . . . . . . . . . . . . . . . . 653\n14.7 Minimum Spanning Trees . . . . . . . . . . . . . . . . . . . 662\n14.7.1 Prim-Jarn ´ ık Algorithm . . . . . . . . . . . . . . . . . . . 664\n14.7.2 Kruskal’s Algorithm . . . . . . . . . . . . . . . . . . . . . 667\n14.7.3 Disjoint Partitions and Union-Find Structures . . . . . . . 672\n14.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677\nwww.it-ebooks.info"
  },
  {
    "page": 630,
    "content": "612 Chapter 14. Graph Algorithms\n14.1 Graphs\nAgraph is a way of representing relationships that exist between pairs of objects.\nThat is, a graph is a set of objects, called vertices, together with a collection of\npairwise connections between them, called edges. Graphs have applications in\nmodeling many domains, including mapping, transportation, computer networks,\nand electrical engineering. By the way, this notion of a “graph” should not be\nconfused with bar charts and function plots, as these kinds of “graphs” are unrelated\nto the topic of this chapter.\nViewed abstractly, a graph Gis simply a set Vofvertices and a collection E\nof pairs of vertices from V, called edges . Thus, a graph is a way of representing\nconnections or relationships between pairs of objects from some set V. Incidentally,\nsome books use different terminology for graphs and refer to what we call vertices\nasnodes and what we call edges as arcs. We use the terms “vertices” and “edges.”\nEdges in a graph are either directed orundirected . An edge (u,v)is said to\nbedirected from utovif the pair (u,v)is ordered, with upreceding v. An edge\n(u,v)is said to be undirected if the pair (u,v)is not ordered. Undirected edges are\nsometimes denoted with set notation, as {u,v}, but for simplicity we use the pair\nnotation(u,v), noting that in the undirected case (u,v)is the same as (v,u). Graphs\nare typically visualized by drawing the vertices as ovals or rectangles and the edges\nas segments or curves connecting pairs of ovals and rectangles. The following are\nsome examples of directed and undirected graphs.\nExample 14.1: We can visualize collaborations among the researchers of a cer-\ntain discipline by constructing a graph whose vertices are associated with the re-\nsearchers themselves, and whose edges connect pairs of vertices associated with\nresearchers who have coauthored a paper or book. (See Figure 14.1.) Such edges\nare undirected because coauthorship is a symmetric relation; that is, if Ahas coau-\nthored something with B, then Bnecessarily has coauthored something with A.\nChiangGoldwasser\nTamassia GoodrichGarg Snoeyink\nTollis\nVitter Preparata\nFigure 14.1: Graph of coauthorship among some authors.\nwww.it-ebooks.info"
  },
  {
    "page": 631,
    "content": "14.1. Graphs 613\nExample 14.2: We can associate with an object-oriented program a graph whose\nvertices represent the classes deﬁned in the program, and whose edges indicate\ninheritance between classes. There is an edge from a vertex vto a vertex uif\nthe class for vinherits from the class for u. Such edges are directed because the\ninheritance relation only goes in one direction (that is, it is asymmetric ).\nIf all the edges in a graph are undirected, then we say the graph is an undirected\ngraph . Likewise, a directed graph , also called a digraph , is a graph whose edges\nare all directed. A graph that has both directed and undirected edges is often called\namixed graph . Note that an undirected or mixed graph can be converted into a\ndirected graph by replacing every undirected edge (u,v)by the pair of directed\nedges(u,v)and(v,u). It is often useful, however, to keep undirected and mixed\ngraphs represented as they are, for such graphs have several applications, as in thefollowing example.\nExample 14.3:\nA city map can be modeled as a graph whose vertices are intersec-\ntions or dead ends, and whose edges are stretches of streets without intersections.\nThis graph has both undirected edges, which correspond to stretches of two-way\nstreets, and directed edges, which correspond to stretches of one-way streets. Thus,\nin this way, a graph modeling a city map is a mixed graph.\nExample 14.4: Physical examples of graphs are present in the electrical wiring\nand plumbing networks of a building. Such networks can be modeled as graphs,\nwhere each connector, ﬁxture, or outlet is viewed as a vertex, and each uninter-\nrupted stretch of wire or pipe is viewed as an edge. Such graphs are actually com-\nponents of much larger graphs, namely the local power and water distribution net-\nworks. Depending on the speciﬁc aspects of these graphs that we are interested in,\nwe may consider their edges as undirected or directed, for, in principle, water can\nﬂow in a pipe and current can ﬂow in a wire in either direction.\nThe two vertices joined by an edge are called the end vertices (orendpoints )\nof the edge. If an edge is directed, its ﬁrst endpoint is its origin and the other is the\ndestination of the edge. Two vertices uandvare said to be adjacent if there is an\nedge whose end vertices are uandv. An edge is said to be incident to a vertex if\nthe vertex is one of the edge’s endpoints. The outgoing edges of a vertex are the\ndirected edges whose origin is that vertex. The incoming edges of a vertex are the\ndirected edges whose destination is that vertex. The degree of a vertex v, denoted\ndeg(v), is the number of incident edges of v. The in-degree andout-degree of a\nvertex vare the number of the incoming and outgoing edges of v, and are denoted\nindeg(v)and outdeg (v), respectively.\nwww.it-ebooks.info"
  },
  {
    "page": 632,
    "content": "614 Chapter 14. Graph Algorithms\nExample 14.5: We can study air transportation by constructing a graph G, called\naﬂight network , whose vertices are associated with airports, and whose edges\nare associated with ﬂights. (See Figure 14.2.) In graph G, the edges are directed\nbecause a given ﬂight has a speciﬁc travel direction. The endpoints of an edge ein\nGcorrespond respectively to the origin and destination of the ﬂight corresponding\ntoe. Two airports are adjacent in Gif there is a ﬂight that ﬂies between them,\nand an edge eis incident to a vertex vinGif the ﬂight for eﬂies to or from the\nairport for v. The outgoing edges of a vertex vcorrespond to the outbound ﬂights\nfrom v’s airport, and the incoming edges correspond to the inbound ﬂights to v’s\nairport. Finally, the in-degree of a vertex vofGcorresponds to the number of\ninbound ﬂights to v’s airport, and the out-degree of a vertex vinGcorresponds to\nthe number of outbound ﬂights.\nORD\nMIANW 35\nAA 903DL 247DL 335\nAA 49\nAA 411AA 523UA 120UA 877SW 45\nAA 1387\nDFW\nLAXSFOBOS\nJFK\nFigure 14.2: Example of a directed graph representing a ﬂight network. The end-\npoints of edge UA 120 are LAX and ORD; hence, LAX and ORD are adjacent.\nThe in-degree of DFW is 3, and the out-degree of DFW is 2.\nThe deﬁnition of a graph refers to the group of edges as a collection , not a\nset, thus allowing two undirected edges to have the same end vertices, and for two\ndirected edges to have the same origin and the same destination. Such edges arecalled parallel edges ormultiple edges . A ﬂight network can contain parallel edges\n(Example 14.5), such that multiple edges between the same pair of vertices couldindicate different ﬂights operating on the same route at different times of the day.Another special type of edge is one that connects a vertex to itself. Namely, we say\nthat an edge (undirected or directed) is a self-loop if its two endpoints coincide. A\nself-loop may occur in a graph associated with a city map (Example 14.3), where\nit would correspond to a “circle” (a curving street that returns to its starting point).\nWith few exceptions, graphs do not have parallel edges or self-loops. Such\ngraphs are said to be simple . Thus, we can usually say that the edges of a simple\ngraph are a setof vertex pairs (and not just a collection). Throughout this chapter,\nwe will assume that a graph is simple unless otherwise speciﬁed.\nwww.it-ebooks.info"
  },
  {
    "page": 633,
    "content": "14.1. Graphs 615\nApath is a sequence of alternating vertices and edges that starts at a vertex and\nends at a vertex such that each edge is incident to its predecessor and successor\nvertex. A cycle is a path that starts and ends at the same vertex, and that includes at\nleast one edge. We say that a path is simple if each vertex in the path is distinct, and\nwe say that a cycle is simple if each vertex in the cycle is distinct, except for the\nﬁrst and last one. A directed path is a path such that all edges are directed and are\ntraversed along their direction. A directed cycle is similarly deﬁned. For example,\nin Figure 14.2, (BOS, NW 35, JFK, AA 1387, DFW) is a directed simple path, and(LAX, UA 120, ORD, UA 877, DFW, AA 49, LAX) is a directed simple cycle.Note that a directed graph may have a cycle consisting of two edges with opposite\ndirection between the same pair of vertices, for example (ORD, UA 877, DFW,\nDL 335, ORD) in Figure 14.2. A directed graph is acyclic if it has no directed\ncycles. For example, if we were to remove the edge UA 877 from the graph inFigure 14.2, the remaining graph is acyclic. If a graph is simple, we may omit theedges when describing path Por cycle C, as these are well deﬁned, in which case\nPis a list of adjacent vertices and Cis a cycle of adjacent vertices.\nExample 14.6:\nGiven a graph Grepresenting a city map (see Example 14.3), we\ncan model a couple driving to dinner at a recommended restaurant as traversing a\npath though G. If they know the way, and do not accidentally go through the same\nintersection twice, then they traverse a simple path in G. Likewise, we can model\nthe entire trip the couple takes, from their home to the restaurant and back, as a\ncycle. If they go home from the restaurant in a completely different way than how\nthey went, not even going through the same intersection twice, then their entire\nround trip is a simple cycle. Finally, if they travel along one-way streets for their\nentire trip, we can model their night out as a directed cycle.\nGiven vertices uandvof a (directed) graph G, we say that ureaches v, and\nthatvisreachable from u, ifGhas a (directed) path from utov. In an undirected\ngraph, the notion of reachability is symmetric, that is to say, ureaches vif an only\nifvreaches u. However, in a directed graph, it is possible that ureaches vbutvdoes\nnot reach u, because a directed path must be traversed according to the respective\ndirections of the edges. A graph is connected if, for any two vertices, there is a path\nbetween them. A directed graph /vectorGisstrongly connected if for any two vertices u\nandvof/vectorG,ureaches vandvreaches u. (See Figure 14.3 for some examples.)\nAsubgraph of a graph Gis a graph Hwhose vertices and edges are subsets of\nthe vertices and edges of G, respectively. A spanning subgraph ofGis a subgraph\nofGthat contains all the vertices of the graph G. If a graph Gis not connected,\nits maximal connected subgraphs are called the connected components ofG. A\nforest is a graph without cycles. A treeis a connected forest, that is, a connected\ngraph without cycles. A spanning tree of a graph is a spanning subgraph that is a\ntree. (Note that this deﬁnition of a tree is somewhat different from the one given in\nChapter 8, as there is not necessarily a designated root.)\nwww.it-ebooks.info"
  },
  {
    "page": 634,
    "content": "616 Chapter 14. Graph Algorithms\nDFW\nMIAORD\nJFKBOS\nSFO\nLAXDFWSFO\nMIAORD\nJFKBOS\nLAX\n(a) (b)\nJFKBOS\nLAXDFWORD\nMIASFO\nDFW\nMIAORD\nJFKBOS\nSFO\nLAX\n(c) (d)\nFigure 14.3: Examples of reachability in a directed graph: (a) a directed path from\nBOS to LAX is highlighted; (b) a directed cycle (ORD, MIA, DFW, LAX, ORD) is\nhighlighted; its vertices induce a strongly connected subgraph; (c) the subgraph of\nthe vertices and edges reachable from ORD is highlighted; (d) the removal of thedashed edges results in a directed acyclic graph.\nExample 14.7:\nPerhaps the most talked about graph today is the Internet, which\ncan be viewed as a graph whose vertices are computers and whose (undirected)\nedges are communication connections between pairs of computers on the Inter-\nnet. The computers and the connections between them in a single domain, like\nwiley.com , form a subgraph of the Internet. If this subgraph is connected, then two\nusers on computers in this domain can send email to one another without having\ntheir information packets ever leave their domain. Suppose the edges of this sub-\ngraph form a spanning tree. This implies that, if even a single connection goes\ndown (for example, because someone pulls a communication cable out of the back\nof a computer in this domain), then this subgraph will no longer be connected.\nwww.it-ebooks.info"
  },
  {
    "page": 635,
    "content": "14.1. Graphs 617\nIn the propositions that follow, we explore a few important properties of graphs.\nProposition 14.8: IfGis a graph with medges and vertex set V, then\n∑\nvinVdeg(v)=2m.\nJustiﬁcation: An edge(u,v)is counted twice in the summation above; once by\nits endpoint uand once by its endpoint v. Thus, the total contribution of the edges\nto the degrees of the vertices is twice the number of edges.\nProposition 14.9: IfGis a directed graph with medges and vertex set V, then\n∑\nvinVindeg(v) = ∑\nvinVoutdeg(v)=m.\nJustiﬁcation: In a directed graph, an edge (u,v)contributes one unit to the\nout-degree of its origin uand one unit to the in-degree of its destination v. Thus,\nthe total contribution of the edges to the out-degrees of the vertices is equal to the\nnumber of edges, and similarly for the in-degrees.\nWe next show that a simple graph with nvertices has O(n2)edges.\nProposition 14.10: LetGbe a simple graph with nvertices and medges. If Gis\nundirected, then m≤n(n−1)/2, and if Gis directed, then m≤n(n−1).\nJustiﬁcation: Suppose that Gis undirected. Since no two edges can have the\nsame endpoints and there are no self-loops, the maximum degree of a vertex in G\nisn−1 in this case. Thus, by Proposition 14.8, 2 m≤n(n−1). Now suppose that\nGis directed. Since no two edges can have the same origin and destination, and\nthere are no self-loops, the maximum in-degree of a vertex in Gisn−1 in this case.\nThus, by Proposition 14.9, m≤n(n−1).\nThere are a number of simple properties of trees, forests, and connected graphs.\nProposition 14.11: LetGbe an undirected graph with nvertices and medges.\n•IfGis connected, then m≥n−1.\n•IfGis a tree, then m=n−1.\n•IfGis a forest, then m≤n−1.\nwww.it-ebooks.info"
  },
  {
    "page": 636,
    "content": "618 Chapter 14. Graph Algorithms\n14.1.1 The Graph ADT\nA graph is a collection of vertices and edges. We model the abstraction as a com-\nbination of three data types: Vertex ,Edge , andGraph . AVertex is a lightweight\nobject that stores an arbitrary element provided by the user (e.g., an airport code);\nwe assume the element can be retrieved with the getElement() method. An Edge\nalso stores an associated object (e.g., a ﬂight number, travel distance, cost), which\nis returned by its getElement() method.\nThe primary abstraction for a graph is the Graph ADT. We presume that a graph\ncan be either undirected ordirected , with the designation declared upon construc-\ntion; recall that a mixed graph can be represented as a directed graph, modeling\nedge{u,v}as a pair of directed edges (u,v)and(v,u). TheGraph ADT includes\nthe following methods:\nnumVertices() :Returns the number of vertices of the graph.\nvertices() :Returns an iteration of all the vertices of the graph.\nnumEdges() :Returns the number of edges of the graph.\nedges() :Returns an iteration of all the edges of the graph.\ngetEdge( u,v):Returns the edge from vertex uto vertex v, if one exists;\notherwise return null. For an undirected graph, there is no\ndifference between getEdge( u,v)andgetEdge( v,u).\nendVertices( e):Returns an array containing the two endpoint vertices of\nedge e. If the graph is directed, the ﬁrst vertex is the origin\nand the second is the destination.\nopposite( v,e):For edge eincident to vertex v, returns the other vertex of\nthe edge; an error occurs if eis not incident to v.\noutDegree( v):Returns the number of outgoing edges from vertex v.\ninDegree( v):Returns the number of incoming edges to vertex v. For\nan undirected graph, this returns the same value as does\noutDegree( v).\noutgoingEdges( v):Returns an iteration of all outgoing edges from vertex v.\nincomingEdges( v):Returns an iteration of all incoming edges to vertex v. For\nan undirected graph, this returns the same collection as\ndoesoutgoingEdges( v).\ninsertVertex( x):Creates and returns a new Vertex storing element x.\ninsertEdge( u,v,x):Creates and returns a new Edge from vertex uto vertex v,\nstoring element x; an error occurs if there already exists an\nedge from utov.\nremoveVertex( v):Removes vertex vand all its incident edges from the graph.\nremoveEdge( e):Removes edge efrom the graph.\nwww.it-ebooks.info"
  },
  {
    "page": 637,
    "content": "14.2. Data Structures for Graphs 619\n14.2 Data Structures for Graphs\nIn this section, we introduce four data structures for representing a graph. In each\nrepresentation, we maintain a collection to store the vertices of a graph. However,\nthe four representations differ greatly in the way they organize the edges.\n•In an edge list , we maintain an unordered list of all edges. This minimally\nsufﬁces, but there is no efﬁcient way to locate a particular edge (u,v), or the\nset of all edges incident to a vertex v.\n•In an adjacency list , we additionally maintain, for each vertex, a separate\nlist containing those edges that are incident to the vertex. This organization\nallows us to more efﬁciently ﬁnd all edges incident to a given vertex.\n•Anadjacency map is similar to an adjacency list, but the secondary container\nof all edges incident to a vertex is organized as a map, rather than as a list,\nwith the adjacent vertex serving as a key. This allows more efﬁcient access\nto a speciﬁc edge (u,v), for example, in O(1)expected time with hashing.\n•Anadjacency matrix provides worst-case O(1)access to a speciﬁc edge\n(u,v)by maintaining an n×nmatrix, for a graph with nvertices. Each\nslot is dedicated to storing a reference to the edge (u,v)for a particular pair\nof vertices uandv; if no such edge exists, the slot will store null.\nA summary of the performance of these structures is given in Table 14.1.\nMethod Edge List Adj. List Adj. Map Adj. Matrix\nnumVertices() O(1) O(1) O(1) O(1)\nnumEdges() O(1) O(1) O(1) O(1)\nvertices() O(n) O(n) O(n) O(n)\nedges() O(m) O(m) O(m) O(m)\ngetEdge( u,v) O(m) O(min(du,dv))O(1)exp. O(1)\noutDegree( v) O(m) O(1) O(1) O(n)\ninDegree( v)\noutgoingEdges( v)O(m) O(dv) O(dv) O(n)\nincomingEdges( v)\ninsertVertex( x) O(1) O(1) O(1) O(n2)\nremoveVertex( v) O(m) O(dv) O(dv) O(n2)\ninsertEdge( u,v,x)O(1) O(1) O(1)exp. O(1)\nremoveEdge( e) O(1) O(1) O(1)exp. O(1)\nTable 14.1: A summary of the running times for the methods of the graph ADT, us-\ning the graph representations discussed in this section. We let ndenote the number\nof vertices, mthe number of edges, and dvthe degree of vertex v. Note that the\nadjacency matrix uses O(n2)space, while all other structures use O(n+m)space.\nwww.it-ebooks.info"
  },
  {
    "page": 638,
    "content": "620 Chapter 14. Graph Algorithms\n14.2.1 Edge List Structure\nTheedge list structure is possibly the simplest, though not the most efﬁcient, rep-\nresentation of a graph G. All vertex objects are stored in an unordered list V, and\nall edge objects are stored in an unordered list E. We illustrate an example of the\nedge list structure for a graph Gin Figure 14.4.\nhe g\nvu\nw zf\nze\nf\ng\nhV E\nvu\nw\n(a) (b)\nFigure 14.4: (a) A graph G; (b) schematic representation of the edge list structure\nforG. Notice that an edge object refers to the two vertex objects that correspond to\nits endpoints, but that vertices do not refer to incident edges.\nTo support the many methods of the Graph ADT (Section 14.1), we assume the\nfollowing additional features of an edge list representation. Collections VandE\nare represented with doubly linked lists using our LinkedPositionalList class from\nChapter 7.\nVertex Objects\nThe vertex object for a vertex vstoring element xhas instance variables for:\n•A reference to element x, to support the getElement() method.\n•A reference to the position of the vertex instance in the list V, thereby allow-\ningvto be efﬁciently removed from Vif it were removed from the graph.\nEdge Objects\nThe edge object for an edge estoring element xhas instance variables for:\n•A reference to element x, to support the getElement() method.\n•References to the vertex objects associated with the endpoint vertices of e.\nThese will allow for constant-time support for methods endVertices( e)and\nopposite( v,e).\n•A reference to the position of the edge instance in list E, thereby allowing e\nto be efﬁciently removed from Eif it were removed from the graph.\nwww.it-ebooks.info"
  },
  {
    "page": 639,
    "content": "14.2. Data Structures for Graphs 621\nPerformance of the Edge List Structure\nThe performance of an edge list structure in fulﬁlling the graph ADT is summarized\nin Table 14.2. We begin by discussing the space usage, which is O(n+m)for\nrepresenting a graph with nvertices and medges. Each individual vertex or edge\ninstance uses O(1)space, and the additional lists VandEuse space proportional\nto their number of entries.\nIn terms of running time, the edge list structure does as well as one could hope\nin terms of reporting the number of vertices or edges, or in producing an iteration\nof those vertices or edges. By querying the respective list VorE, thenumVertices\nandnumEdges methods run in O(1)time, and by iterating through the appropriate\nlist, the methods vertices andedges run respectively in O(n)andO(m)time.\nThe most signiﬁcant limitations of an edge list structure, especially when com-\npared to the other graph representations, are the O(m)running times of methods\ngetEdge( u,v),outDegree( v), andoutgoingEdges( v)(and corresponding methods\ninDegree andincomingEdges ). The problem is that with all edges of the graph in\nan unordered list E, the only way to answer those queries is through an exhaustive\ninspection of all edges.\nFinally, we consider the methods that update the graph. It is easy to add a new\nvertex or a new edge to the graph in O(1)time. For example, a new edge can be\nadded to the graph by creating an Edge instance storing the given element as data,\nadding that instance to the positional list E, and recording its resulting Position\nwithin Eas an attribute of the edge. That stored position can later be used to\nlocate and remove this edge from EinO(1)time, and thus implement the method\nremoveEdge( e).\nIt is worth discussing why the removeVertex( v)method has a running time of\nO(m). As stated in the graph ADT, when a vertex vis removed from the graph, all\nedges incident to vmust also be removed (otherwise, we would have a contradiction\nof edges that refer to vertices that are not part of the graph). To locate the incident\nedges to the vertex, we must examine all edges of E.\nMethod Running Time\nnumVertices(), numEdges() O(1)\nvertices() O(n)\nedges() O(m)\ngetEdge( u,v), outDegree( v), outgoingEdges( v) O(m)\ninsertVertex( x), insertEdge( u,v,x), removeEdge( e)O(1)\nremoveVertex( v) O(m)\nTable 14.2: Running times of the methods of a graph implemented with the edge\nlist structure. The space used is O(n+m), where nis the number of vertices and m\nis the number of edges.\nwww.it-ebooks.info"
  },
  {
    "page": 640,
    "content": "622 Chapter 14. Graph Algorithms\n14.2.2 Adjacency List Structure\nThe adjacency list structure for a graph adds extra information to the edge list struc-\nture that supports direct access to the incident edges (and thus to the adjacent ver-\ntices) of each vertex. Speciﬁcally, for each vertex v, we maintain a collection I(v),\ncalled the incidence collection ofv, whose entries are edges incident to v. In the\ncase of a directed graph, outgoing and incoming edges can be respectively stored in\ntwo separate collections, Iout(v)andIin(v). Traditionally, the incidence collection\nI(v)for a vertex vis a list, which is why we call this way of representing a graph\ntheadjacency list structure.\nWe require that the primary structure for an adjacency list maintain the col-\nlection Vof vertices in a way so that we can locate the secondary structure I(v)\nfor a given vertex vinO(1)time. This could be done by using a positional list\nto represent V, with each Vertex instance maintaining a direct reference to its I(v)\nincidence collection; we illustrate such an adjacency list structure of a graph in Fig-\nure 14.5. If vertices can be uniquely numbered from 0 to n−1, we could instead\nuse a primary array-based structure to access the appropriate secondary lists.\nThe primary beneﬁt of an adjacency list is that the collection I(v)(or more\nspeciﬁcally, Iout(v)) contains exactly those edges that should be reported by the\nmethodoutgoingEdges( v). Therefore, we can implement this method by iterating\nthe edges of I(v)inO(deg(v))time, where deg (v)is the degree of vertex v. This\nis the best possible outcome for any graph representation, because there are deg (v)\nedges to be reported.\nhe g\nvu\nw zf f h\nhg e\nf e\ngu\nv\nw\nzV\n(a) (b)\nFigure 14.5: (a) An undirected graph G; (b) a schematic representation of the ad-\njacency list structure for G. Collection Vis the primary list of vertices, and each\nvertex has an associated list of incident edges. Although not diagrammed as such,\nwe presume that each edge of the graph is represented with a unique Edge instance\nthat maintains references to its endpoint vertices, and that Eis a list of all edges.\nwww.it-ebooks.info"
  },
  {
    "page": 641,
    "content": "14.2. Data Structures for Graphs 623\nPerformance of the Adjacency List Structure\nTable 14.3 summarizes the performance of the adjacency list structure implemen-\ntation of a graph, assuming that the primary collection VandE, and all secondary\ncollections I(v)are implemented with doubly linked lists.\nAsymptotically, the space requirements for an adjacency list are the same as\nan edge list structure, using O(n+m)space for a graph with nvertices and m\nedges. It is clear that the primary lists of vertices and edges use O(n+m)space.\nIn addition, the sum of the lengths of all secondary lists is O(m), for reasons that\nwere formalized in Propositions 14.8 and 14.9. In short, an undirected edge (u,v)\nis referenced in both I(u)andI(v), but its presence in the graph results in only a\nconstant amount of additional space.\nWe have already noted that the outgoingEdges( v)method can be achieved in\nO(deg(v))time based on use of I(v). For a directed graph, this is more speciﬁcally\nO(outdeg(v))based on use of Iout(v). TheoutDegree( v)method of the graph ADT\ncan run in O(1)time, assuming collection I(v)can report its size in similar time.\nTo locate a speciﬁc edge for implementing getEdge( u,v), we can search through\neither I(u)andI(v)(or for a directed graph, either Iout(u)orIin(v)). By choosing\nthe smaller of the two, we get O(min(deg(u),deg(v)))running time.\nThe rest of the bounds in Table 14.3 can be achieved with additional care. To\nefﬁciently support deletions of edges, an edge (u,v)would need to maintain a ref-\nerence to its positions within both I(u)andI(v), so that it could be deleted from\nthose collections in O(1)time. To remove a vertex v, we must also remove any\nincident edges, but at least we can locate those edges in O(deg(v))time.\nMethod Running Time\nnumVertices(), numEdges() O(1)\nvertices() O(n)\nedges() O(m)\ngetEdge( u,v) O(min(deg(u),deg(v)))\noutDegree( v), inDegree( v) O(1)\noutgoingEdges( v), incomingEdges( v)O(deg(v))\ninsertVertex( x), insertEdge( u,v,x) O(1)\nremoveEdge( e) O(1)\nremoveVertex( v) O(deg(v))\nTable 14.3: Running times of the methods of a graph implemented with the adja-\ncency list structure. The space used is O(n+m), where nis the number of vertices\nandmis the number of edges.\nwww.it-ebooks.info"
  },
  {
    "page": 642,
    "content": "624 Chapter 14. Graph Algorithms\n14.2.3 Adjacency Map Structure\nIn the adjacency list structure, we assume that the secondary incidence collections\nare implemented as unordered linked lists. Such a collection I(v)uses space pro-\nportional to O(deg(v)), allows an edge to be added or removed in O(1)time, and\nallows an iteration of all edges incident to vertex vinO(deg(v))time. However,\nthe best implementation of getEdge( u,v)requires O(min(deg(u),deg(v)))time,\nbecause we must search through either I(u)orI(v).\nWe can improve the performance by using a hash-based map to implement I(v)\nfor each vertex v. Speciﬁcally, we let the opposite endpoint of each incident edge\nserve as a key in the map, with the edge structure serving as the value. We call such\na graph representation an adjacency map . (See Figure 14.6.) The space usage for\nan adjacency map remains O(n+m), because I(v)uses O(deg(v))space for each\nvertex v, as with the adjacency list.\nThe advantage of the adjacency map, relative to an adjacency list, is that the\ngetEdge( u,v)method can be implemented in expected O(1)time by searching for\nvertex uas a key in I(v), or vice versa. This provides a likely improvement over the\nadjacency list, while retaining the worst-case bound of O(min(deg(u),deg(v))).\nIn comparing the performance of adjacency map to other representations (see\nTable 14.1), we ﬁnd that it essentially achieves optimal running times for all meth-\nods, making it an excellent all-purpose choice as a graph representation.\nhe g\nvu\nw zf g h\nw\nhuuw v\ng e\nf ew\nv u z\nfv\nw\nzV\n(a) (b)\nFigure 14.6: (a) An undirected graph G; (b) a schematic representation of the ad-\njacency map structure for G. Each vertex maintains a secondary map in which\nneighboring vertices serve as keys, with the connecting edges as associated values.\nAs with the adjacency list, we presume that there is also an overall list Eof allEdge\ninstances.\nwww.it-ebooks.info"
  },
  {
    "page": 643,
    "content": "14.2. Data Structures for Graphs 625\n14.2.4 Adjacency Matrix Structure\nTheadjacency matrix structure for a graph Gaugments the edge list structure with\na matrix A(that is, a two-dimensional array, as in Section 3.1.5), which allows us\nto locate an edge between a given pair of vertices in worst-case constant time. In\nthe adjacency matrix representation, we think of the vertices as being the integers\nin the set{0,1,..., n−1}and the edges as being pairs of such integers. This allows\nus to store references to edges in the cells of a two-dimensional n×narray A.\nSpeciﬁcally, the cell A[i][j]holds a reference to the edge (u,v), if it exists, where u\nis the vertex with index iandvis the vertex with index j. If there is no such edge,\nthen A[i][j]=null. We note that array Ais symmetric if graph Gis undirected, as\nA[i][j]=A[j][i]for all pairs iandj. (See Figure 14.7.)\nThe most signiﬁcant advantage of an adjacency matrix is that any edge (u,v)\ncan be accessed in worst-case O(1)time; recall that the adjacency map supports\nthat operation in O(1)expected time. However, several operation are less efﬁcient\nwith an adjacency matrix. For example, to ﬁnd the edges incident to vertex v, we\nmust presumably examine all nentries in the row associated with v; recall that an\nadjacency list or map can locate those edges in optimal O(deg(v))time. Adding or\nremoving vertices from a graph is problematic, as the matrix must be resized.\nFurthermore, the O(n2)space usage of an adjacency matrix is typically far\nworse than the O(n+m)space required of the other representations. Although,\nin the worst case, the number of edges in a dense graph will be proportional to\nn2, most real-world graphs are sparse . In such cases, use of an adjacency matrix\nis inefﬁcient. However, if a graph is dense, the constants of proportionality of an\nadjacency matrix can be smaller than that of an adjacency list or map. In fact, if\nedges do not have auxiliary data, a boolean adjacency matrix can use one bit per\nedge slot, such that A[i][j]=true if and only if associated (u,v)is an edge.\nhe g\nvu\nw zf h0\n1\n2\n30 1 2 3\nu\nv\nw\nze\neg\ngf\nf h\n(a) (b)\nFigure 14.7: (a) An undirected graph G; (b) a schematic representation of the aux-\niliary adjacency matrix structure for G, in which nvertices are mapped to indices 0\nton−1. Although not diagrammed as such, we presume that there is a unique Edge\ninstance for each edge, and that it maintains references to its endpoint vertices. We\nalso assume that there is a secondary edge list (not pictured), to allow the edges()\nmethod to run in O(m)time, for a graph with medges.\nwww.it-ebooks.info"
  },
  {
    "page": 644,
    "content": "626 Chapter 14. Graph Algorithms\n14.2.5 Java Implementation\nIn this section, we provide an implementation of the Graph ADT, based on the\nadjacency map representation, as described in Section 14.2.3. We use positional\nlists to represent each of the primary lists VandE, as originally described in the\nedge list representation. Additionally, for each vertex v, we use a hash-based map\nto represent the secondary incidence map I(v).\nTo gracefully support both undirected and directed graphs, each vertex main-\ntains two different map references: outgoing andincoming . In the directed case,\nthese are initialized to two distinct map instances, representing Iout(v)andIin(v),\nrespectively. In the case of an undirected graph, we assign both outgoing and\nincoming as aliases to a single map instance.\nOur implementation is organized as follows. We assume deﬁnitions for Vertex ,\nEdge , andGraph interfaces, although for the sake of brevity, we do not include\nthose deﬁnitions in the book (they are available online). We then deﬁne a concrete\nAdjacencyMapGraph class, with nested classes InnerVertex andInnerEdge to im-\nplement the vertex and edge abstractions. These classes use generic parameters V\nandEto designate the element type stored respectively at vertices and edges.\nWe begin in Code Fragment 14.1, with the deﬁnitions of the InnerVertex and\nInnerEdge classes (although in reality, those deﬁnitions should be nested within the\nfollowing AdjacencyMapGraph class). Note well how the InnerVertex constructor\ninitializes the outgoing andincoming instance variables depending on whether the\noverall graph is undirected or directed.\nCode Fragments 14.2 and 14.3 contain the core implementation of the class\nAdjacencyMapGraph . A graph instance maintains a boolean variable that desig-\nnates whether the graph is directed, and it maintains the vertex list and edge list.\nAlthough not shown in these code fragments, our implementation includes pri-\nvatevalidate methods that perform type conversions between the public Vertex\nandEdge interface types to the concrete InnerVertex andInnerEdge classes, while\nalso performing some error checking. This design is similar to the validate method\nof theLinkedPositionalList class (see Code Fragment 7.10 of Section 7.3.3), which\nconverts an outward Position to the underlying Node type for that class.\nThe most complex methods are those that modify the graph. When insertVertex\nis called, we must create a new InnerVertex instance, add that vertex to the list of\nvertices, and record its position within that list (so that we can efﬁciently delete\nit from the list if the vertex is removed from the graph). When inserting an edge\n(u,v), we must also create a new instance, add it to the edge list, and record its\nposition, yet we must also add the new edge to the outgoing adjacency map for\nvertex u, and the incoming map for vertex v. Code Fragment 14.3 contains code\nforremoveVertex as well; the implementation of removeEdge is not included, but\nis available in the online version of the code.\nwww.it-ebooks.info"
  },
  {
    "page": 645,
    "content": "14.2. Data Structures for Graphs 627\n1/∗∗A vertex of an adjacency map graph representation. ∗/\n2private class InnerVertex <V>implements Vertex<V>{\n3privateV element;\n4privatePosition<Vertex<V>>pos;\n5privateMap<Vertex<V>, Edge<E>>outgoing, incoming;\n6/∗∗Constructs a new InnerVertex instance storing the given element. ∗/\n7publicInnerVertex(V elem, boolean graphIsDirected){\n8element = elem;\n9outgoing = newProbeHashMap <>();\n10if(graphIsDirected)\n11 incoming = newProbeHashMap <>();\n12else\n13 incoming = outgoing; // if undirected, alias outgoing map\n14}\n15/∗∗Returns the element associated with the vertex. ∗/\n16publicV getElement(){returnelement;}\n17/∗∗Stores the position of this vertex within the graph 's vertex list. ∗/\n18public void setPosition(Position <Vertex<V>>p){pos = p;}\n19/∗∗Returns the position of this vertex within the graph 's vertex list. ∗/\n20publicPosition<Vertex<V>>getPosition(){returnpos;}\n21/∗∗Returns reference to the underlying map of outgoing edges. ∗/\n22publicMap<Vertex<V>, Edge<E>>getOutgoing(){returnoutgoing;}\n23/∗∗Returns reference to the underlying map of incoming edges. ∗/\n24publicMap<Vertex<V>, Edge<E>>getIncoming(){returnincoming;}\n25}//------------ end of InnerVertex class ------------\n26\n27/∗∗An edge between two vertices. ∗/\n28private class InnerEdge <E>implements Edge<E>{\n29privateE element;\n30privatePosition<Edge<E>>pos;\n31privateVertex<V>[ ] endpoints;\n32/∗∗Constructs InnerEdge instance from u to v, storing the given element. ∗/\n33publicInnerEdge(Vertex <V>u, Vertex <V>v, E elem){\n34element = elem;\n35endpoints = (Vertex <V>[ ])newVertex[ ]{u,v};// array of length 2\n36}\n37/∗∗Returns the element associated with the edge. ∗/\n38publicE getElement(){returnelement;}\n39/∗∗Returns reference to the endpoint array. ∗/\n40publicVertex<V>[ ] getEndpoints() {returnendpoints;}\n41/∗∗Stores the position of this edge within the graph 's vertex list. ∗/\n42public void setPosition(Position <Edge<E>>p){pos = p;}\n43/∗∗Returns the position of this edge within the graph 's vertex list. ∗/\n44publicPosition<Edge<E>>getPosition(){returnpos;}\n45}//------------ end of InnerEdge class ------------\nCode Fragment 14.1: InnerVertex andInnerEdge classes (to be nested within the\nAdjacencyMapGraph class). Interfaces Vertex<V>andEdge<E>are not shown.\nwww.it-ebooks.info"
  },
  {
    "page": 646,
    "content": "628 Chapter 14. Graph Algorithms\n1public class AdjacencyMapGraph <V,E>implements Graph<V,E>{\n2// nested InnerVertex and InnerEdge classes deﬁned here...\n3private boolean isDirected;\n4privatePositionalList <Vertex<V>>vertices = newLinkedPositionalList <>();\n5privatePositionalList <Edge<E>>edges =newLinkedPositionalList <>();\n6/∗∗Constructs an empty graph (either undirected or directed). ∗/\n7publicAdjacencyMapGraph( boolean directed){isDirected = directed; }\n8/∗∗Returns the number of vertices of the graph ∗/\n9public int numVertices(){returnvertices.size();}\n10/∗∗Returns the vertices of the graph as an iterable collection ∗/\n11publicIterable<Vertex<V>>vertices(){returnvertices;}\n12/∗∗Returns the number of edges of the graph ∗/\n13public int numEdges(){returnedges.size();}\n14/∗∗Returns the edges of the graph as an iterable collection ∗/\n15publicIterable<Edge<E>>edges(){returnedges;}\n16/∗∗Returns the number of edges for which vertex v is the origin. ∗/\n17public int outDegree(Vertex <V>v){\n18InnerVertex <V>vert = validate(v);\n19returnvert.getOutgoing().size();\n20}\n21/∗∗Returns an iterable collection of edges for which vertex v is the origin. ∗/\n22publicIterable<Edge<E>>outgoingEdges(Vertex <V>v){\n23InnerVertex <V>vert = validate(v);\n24returnvert.getOutgoing().values(); // edges are the values in the adjacency map\n25}\n26/∗∗Returns the number of edges for which vertex v is the destination. ∗/\n27public int inDegree(Vertex <V>v){\n28InnerVertex <V>vert = validate(v);\n29returnvert.getIncoming().size();\n30}\n31/∗∗Returns an iterable collection of edges for which vertex v is the destination. ∗/\n32publicIterable<Edge<E>>incomingEdges(Vertex <V>v){\n33InnerVertex <V>vert = validate(v);\n34returnvert.getIncoming().values(); // edges are the values in the adjacency map\n35}\n36publicEdge<E>getEdge(Vertex <V>u, Vertex <V>v){\n37/∗∗Returns the edge from u to v, or null if they are not adjacent. ∗/\n38InnerVertex <V>origin = validate(u);\n39returnorigin.getOutgoing().get(v); // will be null if no edge from u to v\n40}\n41/∗∗Returns the vertices of edge e as an array of length two. ∗/\n42publicVertex<V>[ ] endVertices(Edge <E>e){\n43InnerEdge <E>edge = validate(e);\n44returnedge.getEndpoints();\n45}\nCode Fragment 14.2: AdjacencyMapGraph class deﬁnition. (Continues in Code\nFragment 14.3.) The validate(v) andvalidate(e) methods are available online.\nwww.it-ebooks.info"
  },
  {
    "page": 647,
    "content": "14.2. Data Structures for Graphs 629\n46/∗∗Returns the vertex that is opposite vertex v on edge e. ∗/\n47publicVertex<V>opposite(Vertex <V>v, Edge<E>e)\n48 throwsIllegalArgumentException {\n49InnerEdge <E>edge = validate(e);\n50Vertex<V>[ ] endpoints = edge.getEndpoints();\n51if(endpoints[0] == v)\n52 returnendpoints[1];\n53else if(endpoints[1] == v)\n54 returnendpoints[0];\n55else\n56 throw new IllegalArgumentException( \"v is not incident to this edge\" );\n57}\n58/∗∗Inserts and returns a new vertex with the given element. ∗/\n59publicVertex<V>insertVertex(V element) {\n60InnerVertex <V>v =newInnerVertex <>(element, isDirected);\n61v.setPosition(vertices.addLast(v));\n62returnv;\n63}\n64/∗∗Inserts and returns a new edge between u and v, storing given element. ∗/\n65publicEdge<E>insertEdge(Vertex <V>u, Vertex <V>v, E element)\n66 throwsIllegalArgumentException {\n67if(getEdge(u,v) == null){\n68 InnerEdge <E>e =newInnerEdge <>(u, v, element);\n69 e.setPosition(edges.addLast(e));\n70 InnerVertex <V>origin = validate(u);\n71 InnerVertex <V>dest = validate(v);\n72 origin.getOutgoing().put(v, e);\n73 dest.getIncoming().put(u, e);\n74 returne;\n75}else\n76 throw new IllegalArgumentException( \"Edge from u to v exists\" );\n77}\n78/∗∗Removes a vertex and all its incident edges from the graph. ∗/\n79public void removeVertex(Vertex <V>v){\n80InnerVertex <V>vert = validate(v);\n81// remove all incident edges from the graph\n82for(Edge<E>e : vert.getOutgoing().values())\n83 removeEdge(e);\n84for(Edge<E>e : vert.getIncoming().values())\n85 removeEdge(e);\n86// remove this vertex from the list of vertices\n87vertices.remove(vert.getPosition());\n88}\n89}\nCode Fragment 14.3: AdjacencyMapGraph class deﬁnition (continued from Code\nFragment 14.2). We omit the removeEdge method, for brevity.\nwww.it-ebooks.info"
  },
  {
    "page": 648,
    "content": "630 Chapter 14. Graph Algorithms\n14.3 Graph Traversals\nGreek mythology tells of an elaborate labyrinth that was built to house the mon-\nstrous Minotaur, which was part bull and part man. This labyrinth was so complex\nthat neither beast nor human could escape it. No human, that is, until the Greek\nhero, Theseus, with the help of the king’s daughter, Ariadne, decided to implement\nagraph traversal algorithm. Theseus fastened a ball of thread to the door of the\nlabyrinth and unwound it as he traversed the twisting passages in search of the\nmonster. Theseus obviously knew about good algorithm design, for, after ﬁnding\nand defeating the beast, Theseus easily followed the string back out of the labyrinth\nto the loving arms of Ariadne.\nFormally, a traversal is a systematic procedure for exploring a graph by exam-\nining all of its vertices and edges. A traversal is efﬁcient if it visits all the vertices\nand edges in time proportional to their number, that is, in linear time.\nGraph traversal algorithms are key to answering many fundamental questions\nabout graphs involving the notion of reachability , that is, in determining how to\ntravel from one vertex to another while following paths of a graph. Interesting\nproblems that deal with reachability in an undirected graph Ginclude the following:\n•Computing a path from vertex uto vertex v, or reporting that no such path\nexists.\n•Given a start vertex sofG, computing, for every vertex vofG, a path with\nthe minimum number of edges between sandv, or reporting that no such\npath exists.\n•Testing whether Gis connected.\n•Computing a spanning tree of G, ifGis connected.\n•Computing the connected components of G.\n•Identifying a cycle in G, or reporting that Ghas no cycles.\nInteresting problems that deal with reachability in a directed graph /vectorGinclude the\nfollowing:\n•Computing a directed path from vertex uto vertex v, or reporting that no such\npath exists.\n•Finding all the vertices of /vectorGthat are reachable from a given vertex s.\n•Determine whether /vectorGis acyclic.\n•Determine whether /vectorGis strongly connected.\nIn the remainder of this section, we will present two efﬁcient graph traversal\nalgorithms, called depth-ﬁrst search andbreadth-ﬁrst search , respectively.\nwww.it-ebooks.info"
  },
  {
    "page": 649,
    "content": "14.3. Graph Traversals 631\n14.3.1 Depth-First Search\nThe ﬁrst traversal algorithm we consider in this section is depth-ﬁrst search (DFS).\nDepth-ﬁrst search is useful for testing a number of properties of graphs, including\nwhether there is a path from one vertex to another and whether or not a graph is\nconnected.\nDepth-ﬁrst search in a graph Gis analogous to wandering in a labyrinth with\na string and a can of paint without getting lost. We begin at a speciﬁc starting\nvertex sinG, which we initialize by ﬁxing one end of our string to sand painting\nsas “visited.” The vertex sis now our “current” vertex. In general, if we call\nour current vertex u, we traverse Gby considering an arbitrary edge (u,v)incident\nto the current vertex u. If the edge (u,v)leads us to a vertex vthat is already\nvisited (that is, painted), we ignore that edge. If, on the other hand, (u,v)leads\nto an unvisited vertex v, then we unroll our string, and go to v. We then paint\nvas “visited,” and make it the current vertex, repeating the computation above.\nEventually, we will get to a “dead end,” that is, a current vertex vsuch that all the\nedges incident to vlead to vertices already visited. To get out of this impasse, we\nroll our string back up, backtracking along the edge that brought us to v, going back\nto a previously visited vertex u. We then make uour current vertex and repeat the\ncomputation above for any edges incident to uthat we have not yet considered. If\nall of u’s incident edges lead to visited vertices, then we again roll up our string\nand backtrack to the vertex we came from to get to u, and repeat the procedure at\nthat vertex. Thus, we continue to backtrack along the path that we have traced so\nfar until we ﬁnd a vertex that has yet unexplored edges, take one such edge, and\ncontinue the traversal. The process terminates when our backtracking leads us back\nto the start vertex s, and there are no more unexplored edges incident to s.\nThe pseudocode for a depth-ﬁrst search traversal starting at a vertex u(see\nCode Fragment 14.4) follows our analogy with string and paint. We use recursion\nto implement the string analogy, and we assume that we have a mechanism (the\npaint analogy) to determine whether a vertex or edge has been previously explored.\nAlgorithm DFS( G,u):\nInput: A graph Gand a vertex uofG\nOutput: A collection of vertices reachable from u, with their discovery edges\nMark vertex uas visited.\nforeach of u’s outgoing edges, e=(u,v)do\nifvertex vhas not been visited then\nRecord edge eas the discovery edge for vertex v.\nRecursively call DFS(G,v).\nCode Fragment 14.4: TheDFS algorithm.\nwww.it-ebooks.info"
  },
  {
    "page": 650,
    "content": "632 Chapter 14. Graph Algorithms\nClassifying Graph Edges with DFS\nAn execution of depth-ﬁrst search can be used to analyze the structure of a graph,\nbased upon the way in which edges are explored during the traversal. The DFSprocess naturally identiﬁes what is known as the depth-ﬁrst search tree rooted at\na starting vertex s. Whenever an edge e=(u,v)is used to discover a new vertex v\nduring the DFS algorithm of Code Fragment 14.4, that edge is known as a discovery\nedge ortree edge , as oriented from utov. All other edges that are considered during\nthe execution of DFS are known as nontree edges , which take us to a previously\nvisited vertex. In the case of an undirected graph, we will ﬁnd that all nontree edges\nthat are explored connect the current vertex to one that is an ancestor of it in the\nDFS tree. We will call such an edge a back edge . When performing a DFS on a\ndirected graph, there are three possible kinds of nontree edges:\n•back edges , which connect a vertex to an ancestor in the DFS tree\n•forward edges , which connect a vertex to a descendant in the DFS tree\n•cross edges , which connect a vertex to a vertex that is neither its ancestor nor\nits descendant\nAn example application of the DFS algorithm on a directed graph is shown in\nFigure 14.8, demonstrating each type of nontree edge. An example application ofthe DFS algorithm on an undirected graph is shown in Figure 14.9.\nBOS\nJFKORD\nMIASFO\nLAXDFW\n41\n5\n2\n3\n67\nSFO\nMIAJFK\nDFWBOS\nORD\nLAX\n(a) (b)\nFigure 14.8: An example of a DFS in a directed graph, starting at vertex (BOS):\n(a) intermediate step, where, for the ﬁrst time, a considered edge leads to an alreadyvisited vertex (DFW); (b) the completed DFS. The tree edges are shown with thickblue lines, the back edges are shown with dashed blue lines, and the forward and\ncross edges are shown with dotted black lines. The order in which the vertices are\nvisited is indicated by a label next to each vertex. The edge (ORD,DFW) is a backedge, but (DFW,ORD) is a forward edge. Edge (BOS,SFO) is a forward edge, and(SFO,LAX) is a cross edge.\nwww.it-ebooks.info"
  },
  {
    "page": 651,
    "content": "14.3. Graph Traversals 633\nA C D\nE F G H\nI J K L\nM N O PB A C D\nE F G H\nI J K L\nM N O PB\n(a) (b)\nA C D\nE F G H\nI J K L\nM N O PB A C D\nE F G H\nI J K L\nM N O PB\n(c) (d)\nA C D\nE F G H\nI J K L\nM N O PB A C D\nE F G H\nI J K L\nM N O PB\n(e) (f)\nFigure 14.9: Example of depth-ﬁrst search traversal on an undirected graph starting\nat vertex A. We assume that a vertex’s adjacencies are considered in alphabetical\norder. Visited vertices and explored edges are highlighted, with discovery edges\ndrawn as solid lines and nontree (back) edges as dashed lines: (a) input graph;(b) path of tree edges, traced from A until back edge (G,C) is examined; (c) reach-\ning F, which is a dead end; (d) after backtracking to I, resuming with edge (I,M),\nand hitting another dead end at O; (e) after backtracking to G, continuing with edge(G,L), and hitting another dead end at H; (f) ﬁnal result.\nwww.it-ebooks.info"
  },
  {
    "page": 652,
    "content": "634 Chapter 14. Graph Algorithms\nProperties of a Depth-First Search\nThere are a number of observations that we can make about the depth-ﬁrst search\nalgorithm, many of which derive from the way the DFS algorithm partitions the\nedges of a graph Ginto groups. We will begin with the most signiﬁcant property.\nProposition 14.12: LetGbe an undirected graph on which a DFS traversal start-\ning at a vertex shas been performed. Then the traversal visits all vertices in the\nconnected component of s, and the discovery edges form a spanning tree of the\nconnected component of s.\nJustiﬁcation: Suppose there is at least one vertex wins’s connected component\nnot visited, and let vbe the ﬁrst unvisited vertex on some path from stow(we may\nhave v=w). Since vis the ﬁrst unvisited vertex on this path, it has a neighbor u\nthat was visited. But when we visited u, we must have considered the edge (u,v);\nhence, it cannot be correct that vis unvisited. Therefore, there are no unvisited\nvertices in s’s connected component.\nSince we only follow a discovery edge when we go to an unvisited vertex, we\nwill never form a cycle with such edges. Therefore, the discovery edges form a\nconnected subgraph without cycles, hence a tree. Moreover, this is a spanning\ntree because, as we have just seen, the depth-ﬁrst search visits each vertex in the\nconnected component of s.\nProposition 14.13: Let/vectorGbe a directed graph. Depth-ﬁrst search on /vectorGstarting at\na vertex svisits all the vertices of /vectorGthat are reachable from s. Also, the DFS tree\ncontains directed paths from sto every vertex reachable from s.\nJustiﬁcation: LetVsbe the subset of vertices of /vectorGvisited by DFS starting at\nvertex s. We want to show that Vscontains sand every vertex reachable from s\nbelongs to Vs. Suppose now, for the sake of a contradiction, that there is a vertex w\nreachable from sthat is not in Vs. Consider a directed path from stow, and let(u,v)\nbe the ﬁrst edge on such a path taking us out of Vs, that is, uis in Vsbutvis not\ninVs. When DFS reaches u, it explores all the outgoing edges of u, and thus must\nalso reach vertex vvia edge(u,v). Hence, vshould be in Vs, and we have obtained\na contradiction. Therefore, Vsmust contain every vertex reachable from s.\nWe prove the second fact by induction on the steps of the algorithm. We claim\nthat each time a discovery edge (u,v)is identiﬁed, there exists a directed path from\nstovin the DFS tree. Since umust have previously been discovered, there exists\na path from stou, so by appending the edge (u,v)to that path, we have a directed\npath from stov.\nNote that since back edges always connect a vertex vto a previously visited\nvertex u, each back edge implies a cycle in G, consisting of the discovery edges\nfrom utovplus the back edge (u,v).\nwww.it-ebooks.info"
  },
  {
    "page": 653,
    "content": "14.3. Graph Traversals 635\nRunning Time of Depth-First Search\nIn terms of its running time, depth-ﬁrst search is an efﬁcient method for traversing\na graph. Note that DFS is called at most once on each vertex (since it gets markedas visited), and therefore every edge is examined at most twice for an undirected\ngraph, once from each of its end vertices, and at most once in a directed graph,\nfrom its origin vertex. If we let n\ns≤nbe the number of vertices reachable from\na vertex s, and ms≤mbe the number of incident edges to those vertices, a DFS\nstarting at sruns in O(ns+ms)time, provided the following conditions are satisﬁed:\n•The graph is represented by a data structure such that creating and iterating\nthrough the outgoingEdges( v)takes O(deg(v))time, and the opposite( v,e)\nmethod takes O(1)time. The adjacency list structure is one such structure,\nbut the adjacency matrix structure is not.\n•We have a way to “mark” a vertex or edge as explored, and to test if a vertex\nor edge has been explored in O(1)time. We discuss ways of implementing\nDFS to achieve this goal in the next section.\nGiven the assumptions above, we can solve a number of interesting problems.\nProposition 14.14: LetGbe an undirected graph with nvertices and medges. A\nDFS traversal of Gcan be performed in O(n+m)time, and can be used to solve\nthe following problems in O(n+m)time:\n•Computing a path between two given vertices of G, if one exists.\n•Testing whether Gis connected.\n•Computing a spanning tree of G, ifGis connected.\n•Computing the connected components of G.\n•Computing a cycle in G, or reporting that Ghas no cycles.\nProposition 14.15: Let/vectorGbe a directed graph with nvertices and medges. A\nDFS traversal of /vectorGcan be performed in O(n+m)time, and can be used to solve\nthe following problems in O(n+m)time:\n•Computing a directed path between two given vertices of /vectorG, if one exists.\n•Computing the set of vertices of /vectorGthat are reachable from a given vertex s.\n•Testing whether /vectorGis strongly connected.\n•Computing a directed cycle in /vectorG, or reporting that /vectorGis acyclic.\nThe justiﬁcation of Propositions 14.14 and 14.15 is based on algorithms that\nuse slightly modiﬁed versions of the DFS algorithm as subroutines. We will explore\nsome of those extensions in the remainder of this section.\nwww.it-ebooks.info"
  },
  {
    "page": 654,
    "content": "636 Chapter 14. Graph Algorithms\n14.3.2 DFS Implementation and Extensions\nWe will begin by providing a Java implementation of the depth-ﬁrst search al-\ngorithm. We originally described the algorithm with pseudocode in Code Frag-\nment 14.4. In order to implement it, we must have a mechanism for keeping track\nof which vertices have been visited, and for recording the resulting DFS tree edges.\nFor this bookkeeping, we use two auxiliary data structures. First, we maintain a set,\nnamedknown , containing vertices that have already been visited. Second, we keep\na map, named forest , that associates, with a vertex v, the edge eof the graph that is\nused to discover v(if any). Our DFS method is presented in Code Fragment 14.5.\n1/∗∗Performs depth-ﬁrst search of Graph g starting at Vertex u. ∗/\n2public static <V,E>voidDFS(Graph <V,E>g, Vertex <V>u,\n3 Set<Vertex<V>>known, Map <Vertex<V>,Edge<E>>forest){\n4known.add(u); // u has been discovered\n5for(Edge<E>e : g.outgoingEdges(u)) {// for every outgoing edge from u\n6Vertex<V>v = g.opposite(u, e);\n7if(!known.contains(v)) {\n8 forest.put(v, e); // e is the tree edge that discovered v\n9 DFS(g, v, known, forest); // recursively explore from v\n10}\n11}\n12}\nCode Fragment 14.5: Recursive implementation of depth-ﬁrst search on a graph,\nstarting at a designated vertex u. As an outcome of a call, visited vertices are added\nto the known set, and discovery edges are added to the forest.\nOurDFS method does not make any assumption about how the SetorMap\ninstances are implemented; however, the O(n+m)running-time analysis of the\nprevious section does presume that we can “mark” a vertex as explored or test the\nstatus of a vertex in O(1)time. If we use hash-based implementations of the set\nand map structure, then all of their operations run in O(1)expected time, and the\noverall algorithm runs in O(n+m)time with very high probability. In practice, this\nis a compromise we are willing to accept.\nIf vertices can be numbered from 0 ,..., n−1 (a common assumption for graph\nalgorithms), then the set and map can be implemented more directly as a lookup\ntable, with a vertex label used as an index into an array of size n. In that case, the\nnecessary set and map operations run in worst-case O(1)time. Alternatively, we\ncan “decorate” each vertex with the auxiliary information, either by leveraging the\ngeneric type of the element that is stored with each vertex, or by redesigning the\nVertex type to store additional ﬁelds. That would allow marking operations to be\nperformed in O(1)-time, without any assumption about vertices being numbered.\nwww.it-ebooks.info"
  },
  {
    "page": 655,
    "content": "14.3. Graph Traversals 637\nReconstructing a Path from utov\nWe can use the basic DFS method as a tool to identify the (directed) path leading\nfrom vertex utov, ifvis reachable from u. This path can easily be reconstructed\nfrom the information that was recorded in the forest of discovery edges during the\ntraversal. Code Fragment 14.6 provides an implementation of a secondary method\nthat produces an ordered list of vertices on the path from utov, if given the map of\ndiscovery edges that was computed by the original DFS method.\nTo reconstruct the path, we begin at the endof the path, examining the forest\nof discovery edges to determine what edge was used to reach vertex v. We then\ndetermine the opposite vertex of that edge and repeat the process to determine what\nedge was used to discover it. By continuing this process until reaching u, we can\nconstruct the entire path. Assuming constant-time lookup in the forest map, the\npath reconstruction takes time proportional to the length of the path, and therefore,\nit runs in O(n)time (in addition to the time originally spent calling DFS).\n1/∗∗Returns an ordered list of edges comprising the directed path from u to v. ∗/\n2public static <V,E>PositionalList <Edge<E>>\n3constructPath(Graph <V,E>g, Vertex <V>u, Vertex <V>v,\n4 Map<Vertex<V>,Edge<E>>forest){\n5PositionalList <Edge<E>>path =newLinkedPositionalList <>();\n6if(forest.get(v) != null){ // v was discovered during the search\n7Vertex<V>walk = v; // we construct the path from back to front\n8while(walk != u){\n9 Edge<E>edge = forest.get(walk);\n10 path.addFirst(edge); // add edge to *front* of path\n11 walk = g.opposite(walk, edge); // repeat with opposite endpoint\n12}\n13}\n14returnpath;\n15}\nCode Fragment 14.6: Method to reconstruct a directed path from utov, given the\ntrace of discovery from a DFS started at u. The method returns an ordered list of\nvertices on the path.\nTesting for Connectivity\nWe can use the basic DFS method to determine whether a graph is connected. In\nthe case of an undirected graph, we simply start a depth-ﬁrst search at an arbitrary\nvertex and then test whether known.size() equals nat the conclusion. If the graph\nis connected, then by Proposition 14.12, all vertices will have been discovered;conversely, if the graph is not connected, there must be at least one vertex vthat is\nnot reachable from u, and that will not be discovered.\nwww.it-ebooks.info"
  },
  {
    "page": 656,
    "content": "638 Chapter 14. Graph Algorithms\nFor directed graph, /vectorG, we may wish to test whether it is strongly connected , that\nis, whether for every pair of vertices uandv, both ureaches vandvreaches u. If we\nstart an independent call to DFS from each vertex, we could determine whether this\nwas the case, but those ncalls when combined would run in O(n(n+m)). However,\nwe can determine if /vectorGis strongly connected much faster than this, requiring only\ntwo depth-ﬁrst searches.\nWe begin by performing a depth-ﬁrst search of our directed graph /vectorGstarting at\nan arbitrary vertex s. If there is any vertex of /vectorGthat is not visited by this traversal,\nand is not reachable from s, then the graph is not strongly connected. If this ﬁrst\ndepth-ﬁrst search visits each vertex of /vectorG, we need to then check whether sis reach-\nable from all other vertices. Conceptually, we can accomplish this by making a\ncopy of graph /vectorG, but with the orientation of all edges reversed. A depth-ﬁrst search\nstarting at sin the reversed graph will reach every vertex that could reach sin the\noriginal. In practice, a better approach than making a new graph is to reimplementa version of the DFS method that loops through all incoming edges to the current\nvertex, rather than all outgoing edges. Since this algorithm makes just two DFS\ntraversals of /vectorG, it runs in O(n+m)time.\nComputing All Connected Components\nWhen a graph is not connected, the next goal we may have is to identify all of theconnected components of an undirected graph, or the strongly connected compo-\nnents of a directed graph. We will begin by discussing the undirected case.\nIf an initial call to DFS fails to reach all vertices of a graph, we can restart a\nnew call to DFS at one of those unvisited vertices. An implementation of such a\ncomprehensive DFSComplete method is given in Code Fragment 14.7. It returns a\nmap that represents a DFS forest for the entire graph. We say this is a forest rather\nthan a tree, because the graph may not be connected.\nVertices that serve as roots of DFS trees within this forest will not have discov-\nery edges and will not appear as keys in the returned map. Therefore, the number of\nconnected components of the graph gis equal to g.numVertices()−forest.size() .\n1/∗∗Performs DFS for the entire graph and returns the DFS forest as a map. ∗/\n2public static <V,E>Map<Vertex<V>,Edge<E>>DFSComplete(Graph <V,E>g){\n3Set<Vertex<V>>known = newHashSet<>();\n4Map<Vertex<V>,Edge<E>>forest =newProbeHashMap <>();\n5for(Vertex<V>u : g.vertices())\n6if(!known.contains(u))\n7 DFS(g, u, known, forest); // (re)start the DFS process at u\n8returnforest;\n9}\nCode Fragment 14.7: Top-level method that returns a DFS forest for an entire graph.\nwww.it-ebooks.info"
  },
  {
    "page": 657,
    "content": "14.3. Graph Traversals 639\nWe can further determine which vertices are in which component, either by\nexamining the structure of the forest that is returned, or by making a minor modiﬁ-\ncation to the core DFS method to tag each vertex with a component number whenit is ﬁrst discovered. (See Exercise C-14.43.)\nAlthough the DFSComplete method makes multiple calls to the original DFS\nmethod, the total time spent by a call to DFSComplete isO(n+m). For an undi-\nrected graph, recall from our original analysis on page 635 that a single call to DFS\nstarting at vertex sruns in time O(n\ns+ms)where nsis the number of vertices reach-\nable from s, and msis the number of incident edges to those vertices. Because each\ncall to DFS explores a different component, the sum of ns+msterms is n+m.\nThe situation is more complex for ﬁnding strongly connected components of\na directed graph. The O(n+m)total bound for a call to DFSComplete applies to\nthe directed case as well, because when restarting the process, we proceed with theexisting set of known vertices. This ensures that the DFS subroutine is called onceon each vertex, and therefore that each outgoing edge is explored only once duringthe entire process.\nAs an example, consider again the graph of Figure 14.8. If we were to start\nthe original DFS method at vertex ORD, the known set of vertices would become\n{ORD, DFW, SFO, LAX, MIA }. If restarting the DFS method at vertex BOS,\nthe outgoing edges to vertices SFO and MIA would not result in further recursion,because those vertices are marked as known.\nHowever, the forest returned by a single call to DFSComplete does not rep-\nresent the strongly connected components of the graph. There exists an approachfor computing those components in O(n+m)time, making use of two calls to\nDFSComplete , but the details are beyond the scope of this book.\nDetecting Cycles with DFS\nFor both undirected and directed graphs, a cycle exists if and only if a back edge\nexists relative to the DFS traversal of that graph. It is easy to see that if a back edgeexists, a cycle exists by taking the back edge from the descendant to its ancestor\nand then following the tree edges back to the descendant. Conversely, if a cycle\nexists in the graph, there must be a back edge relative to a DFS (although we do notprove this fact here).\nAlgorithmically, detecting a back edge in the undirected case is easy, because\nall edges are either tree edges or back edges. In the case of a directed graph, addi-\ntional modiﬁcations to the core DFS implementation are needed to properly cate-\ngorize a nontree edge as a back edge. When a directed edge is explored leading toa previously visited vertex, we must recognize whether that vertex is an ancestor ofthe current vertex. This can be accomplished, for example, by maintaining anotherset, with all vertices upon which a recursive call to DFS is currently active. We\nleave details as an exercise (C-14.42).\nwww.it-ebooks.info"
  },
  {
    "page": 658,
    "content": "640 Chapter 14. Graph Algorithms\n14.3.3 Breadth-First Search\nThe advancing and backtracking of a depth-ﬁrst search, as described in the previ-\nous section, deﬁnes a traversal that could be physically traced by a single person\nexploring a graph. In this section, we will consider another algorithm for travers-\ning a connected component of a graph, known as a breadth-ﬁrst search (BFS). The\nBFS algorithm is more akin to sending out, in all directions, many explorers who\ncollectively traverse a graph in coordinated fashion.\nA BFS proceeds in rounds and subdivides the vertices into levels . BFS starts\nat vertex s, which is at level 0. In the ﬁrst round, we paint as “visited,” all vertices\nadjacent to the start vertex s; these vertices are one step away from the beginning\nand are placed into level 1. In the second round, we allow all explorers to go\ntwo steps (i.e., edges) away from the starting vertex. These new vertices, which\nare adjacent to level 1 vertices and not previously assigned to a level, are placed\ninto level 2 and marked as “visited.” This process continues in similar fashion,\nterminating when no new vertices are found in a level.\nA Java implementation of BFS is given in Code Fragment 14.8. We follow a\nconvention similar to that of DFS (Code Fragment 14.5), maintaining a known set\nof vertices, and storing the BFS tree edges in a map. We illustrate a BFS traversal\nin Figure 14.10.\n1/∗∗Performs breadth-ﬁrst search of Graph g starting at Vertex u. ∗/\n2public static <V,E>voidBFS(Graph <V,E>g, Vertex <V>s,\n3 Set<Vertex<V>>known, Map <Vertex<V>,Edge<E>>forest){\n4PositionalList <Vertex<V>>level =newLinkedPositionalList <>();\n5known.add(s);\n6level.addLast(s); // ﬁrst level includes only s\n7while(!level.isEmpty()) {\n8PositionalList <Vertex<V>>nextLevel = newLinkedPositionalList <>();\n9for(Vertex<V>u : level)\n10 for(Edge<E>e : g.outgoingEdges(u)) {\n11 Vertex<V>v = g.opposite(u, e);\n12 if(!known.contains(v)) {\n13 known.add(v);\n14 forest.put(v, e); // e is the tree edge that discovered v\n15 nextLevel.addLast(v); // v will be further considered in next pass\n16}\n17}\n18level = nextLevel; // relabel ’next’ level to become the current\n19}\n20}\nCode Fragment 14.8: Implementation of breadth-ﬁrst search on a graph, starting at\na designated vertex s.\nwww.it-ebooks.info"
  },
  {
    "page": 659,
    "content": "14.3. Graph Traversals 641\nF H\nI J K L\nM N O PA B C\nED\nG0\nB\nI J K L\nM N O PC D\nG F EA\nH0 1\n(a) (b)\nA\nJ K L\nM N O PB C\nED\nH G F\nI0 1 2\nK L\nM N O PIH G FA B C D\nE\nJ2 3 0 1\n(c) (d)\nF G H\nI J K L\nM N O PA B C D\nE\n41 2 3 0\nF G H\nI J K L\nM N O PA B C D\nE\n41 2 3 0\n5\n(e) (f)\nFigure 14.10: Example of breadth-ﬁrst search traversal, where the edges incident to\na vertex are considered in alphabetical order of the adjacent vertices. The discovery\nedges are shown with solid lines and the nontree (cross) edges are shown withdashed lines: (a) starting the search at A; (b) discovery of level 1; (c) discovery oflevel 2; (d) discovery of level 3; (e) discovery of level 4; (f) discovery of level 5.\nwww.it-ebooks.info"
  },
  {
    "page": 660,
    "content": "642 Chapter 14. Graph Algorithms\nWhen discussing DFS, we described a classiﬁcation of nontree edges being\neither back edges , which connect a vertex to one of its ancestors, forward edges ,\nwhich connect a vertex to one of its descendants, or cross edges , which connect a\nvertex to another vertex that is neither its ancestor nor its descendant. For BFS on\nan undirected graph, all nontree edges are cross edges (see Exercise C-14.46), andfor BFS on a directed graph, all nontree edges are either back edges or cross edges\n(see Exercise C-14.47).\nThe BFS traversal algorithm has a number of interesting properties, some of\nwhich we explore in the proposition that follows. Most notably, a path in a breadth-\nﬁrst search tree rooted at vertex sto any other vertex vis guaranteed to be the\nshortest such path from stovin terms of the number of edges.\nProposition 14.16:\nLetGbe an undirected or directed graph on which a BFS\ntraversal starting at vertex shas been performed. Then\n•The traversal visits all vertices of Gthat are reachable from s.\n•For each vertex vat level i, the path of the BFS tree Tbetween sandvhasi\nedges, and any other path of Gfrom stovhas at least iedges.\n•If(u,v)is an edge that is not in the BFS tree, then the level number of vcan\nbe at most 1greater than the level number of u.\nWe leave the justiﬁcation of this proposition as Exercise C-14.49.\nThe analysis of the running time of BFS is similar to the one of DFS, with\nthe algorithm running in O(n+m)time, or more speciﬁcally, in O(ns+ms)time\nifnsis the number of vertices reachable from vertex s, and ms≤mis the num-\nber of incident edges to those vertices. To explore the entire graph, the process\ncan be restarted at another vertex, akin to the DFSComplete method of Code Frag-\nment 14.7. The actual path from vertex sto vertex vcan be reconstructed using the\nconstructPath method of Code Fragment 14.6\nProposition 14.17: LetGbe a graph with nvertices and medges represented\nwith the adjacency list structure. A BFS traversal of Gtakes O(n+m)time.\nAlthough our implementation of BFS in Code Fragment 14.8 progresses level\nby level, the BFS algorithm can also be implemented using a single FIFO queueto represent the current fringe of the search. Starting with the source vertex in the\nqueue, we repeatedly remove the vertex from the front of the queue and insert anyof its unvisited neighbors to the back of the queue. (See Exercise C-14.50.)\nIn comparing the capabilities of DFS and BFS, both can be used to efﬁciently\nﬁnd the set of vertices that are reachable from a given source, and to determine pathsto those vertices. However, BFS guarantees that those paths use as few edges aspossible. For an undirected graph, both algorithms can be used to test connectivity,to identify connected components, or to locate a cycle. For directed graphs, theDFS algorithm is better suited for certain tasks, such as ﬁnding a directed cycle in\nthe graph, or in identifying the strongly connected components.\nwww.it-ebooks.info"
  },
  {
    "page": 661,
    "content": "14.4. Transitive Closure 643\n14.4 Transitive Closure\nWe have seen that graph traversals can be used to answer basic questions of reach-\nability in a directed graph. In particular, if we are interested in knowing whether\nthere is a path from vertex uto vertex vin a graph, we can perform a DFS or BFS\ntraversal starting at uand observe whether vis discovered. If representing a graph\nwith an adjacency list or adjacency map, we can answer the question of reachability\nforuandvinO(n+m)time (see Propositions 14.15 and 14.17).\nIn certain applications, we may wish to answer many reachability queries more\nefﬁciently, in which case it may be worthwhile to precompute a more convenient\nrepresentation of a graph. For example, the ﬁrst step for a service that computes\ndriving directions from an origin to a destination might be to assess whether the\ndestination is reachable. Similarly, in an electricity network, we may wish to be\nable to quickly determine whether current ﬂows from one particular vertex to an-\nother. Motivated by such applications, we introduce the following deﬁnition. The\ntransitive closure of a directed graph /vectorGis itself a directed graph /vectorG∗such that the\nvertices of /vectorG∗are the same as the vertices of /vectorG, and/vectorG∗has an edge (u,v), when-\never/vectorGhas a directed path from utov(including the case where (u,v)is an edge of\nthe original /vectorG).\nIf a graph is represented as an adjacency list or adjacency map, we can compute\nits transitive closure in O(n(n+m))time by making use of ngraph traversals, one\nfrom each starting vertex. For example, a DFS starting at vertex ucan be used to\ndetermine all vertices reachable from u, and thus a collection of edges originating\nwith uin the transitive closure.\nIn the remainder of this section, we explore an alternative technique for comput-\ning the transitive closure of a directed graph that is particularly well suited for when\na directed graph is represented by a data structure that supports O(1)-time lookup\nfor thegetEdge( u,v)method (for example, the adjacency-matrix structure). Let /vectorG\nbe a directed graph with nvertices and medges. We compute the transitive closure\nof/vectorGin a series of rounds. We initialize /vectorG0=/vectorG. We also arbitrarily number the\nvertices of /vectorGasv1,v2,..., vn. We then begin the computation of the rounds, begin-\nning with round 1. In a generic round k, we construct directed graph /vectorGkstarting\nwith/vectorGk=/vectorGk−1and adding to /vectorGkthe directed edge (vi,vj)if directed graph /vectorGk−1\ncontains both the edges (vi,vk)and(vk,vj). In this way, we will enforce a simple\nrule embodied in the proposition that follows.\nProposition 14.18: Fori=1,..., n, directed graph /vectorGkhas an edge (vi,vj)if and\nonly if directed graph /vectorGhas a directed path from vitovj, whose intermediate\nvertices (if any) are in the set {v1,..., vk}. In particular, /vectorGnis equal to /vectorG∗, the\ntransitive closure of /vectorG.\nwww.it-ebooks.info"
  },
  {
    "page": 662,
    "content": "644 Chapter 14. Graph Algorithms\nProposition 14.18 suggests a simple algorithm for computing the transitive clo-\nsure of/vectorGthat is based on the series of rounds to compute each /vectorGk. This algorithm\nis known as the Floyd-Warshall algorithm , and its pseudocode is given in Code\nFragment 14.9. We illustrate an example run of the Floyd-Warshall algorithm in\nFigure 14.11.\nAlgorithm FloydWarshall( /vectorG):\nInput: A directed graph /vectorGwith nvertices\nOutput: The transitive closure /vectorG∗of/vectorG\nletv1,v2,..., vnbe an arbitrary numbering of the vertices of /vectorG\n/vectorG0=/vectorG\nfork=1 tondo\n/vectorGk=/vectorGk−1\nfor all i,jin{1,..., n}with i/n⌉}ationslash=jandi,j/n⌉}ationslash=kdo\nifboth edges (vi,vk)and(vk,vj)are in/vectorGk−1then\nadd edge (vi,vj)to/vectorGk(if it is not already present)\nreturn/vectorGn\nCode Fragment 14.9: Pseudocode for the Floyd-Warshall algorithm. This algorithm\ncomputes the transitive closure /vectorG∗ofGby incrementally computing a series of\ndirected graphs /vectorG0,/vectorG1,...,/vectorGn, for k=1,..., n.\nFrom this pseudocode, we can easily analyze the running time of the Floyd-\nWarshall algorithm assuming that the data structure representing Gsupports meth-\nodsgetEdge andinsertEdge inO(1)time. The main loop is executed ntimes and\nthe inner loop considers each of O(n2)pairs of vertices, performing a constant-time\ncomputation for each one. Thus, the total running time of the Floyd-Warshall al-\ngorithm is O(n3). From the description and analysis above we may immediately\nderive the following proposition.\nProposition 14.19: Let/vectorGbe a directed graph with nvertices, and let /vectorGbe repre-\nsented by a data structure that supports lookup and update of adjacency information\ninO(1)time. Then the Floyd-Warshall algorithm computes the transitive closure\n/vectorG∗of/vectorGinO(n3)time.\nPerformance of the Floyd-Warshall Algorithm\nAsymptotically, the O(n3)running time of the Floyd-Warshall algorithm is no bet-\nter than that achieved by repeatedly running DFS, once from each vertex, to com-\npute the reachability. However, the Floyd-Warshall algorithm matches the asymp-totic bounds of the repeated DFS when a graph is dense, or when a graph is sparse\nbut represented as an adjacency matrix. (See Exercise R-14.13.)\nwww.it-ebooks.info"
  },
  {
    "page": 663,
    "content": "14.4. Transitive Closure 645\nv7\nv2v6v4\nv1\nv5v3DFW\nMIASFOORD\nJFKBOS\nLAXv7\nv2v6v4\nv1\nv5v3\nMIASFOORD\nJFK\nLAXBOS\nDFW\n(a) (b)\nv7\nv2v6v4\nv1\nv5v3LAX\nMIASFOORD\nJFKBOS\nDFWv7\nv2v6v4\nv5v3\nv1JFK\nLAXDFWBOS\nSFOORD\nMIA\n(c) (d)\nv4\nv1\nv5v7\nv3v2v6\nSFOORD\nMIALAXBOS\nJFK\nDFWv4\nv1\nv5v7\nv3v2v6\nLAXBOS\nJFK\nDFWSFOORD\nMIA\n(e) (f)\nFigure 14.11: Sequence of directed graphs computed by the Floyd-Warshall algo-\nrithm: (a) initial directed graph /vectorG=/vectorG0and numbering of the vertices; (b) directed\ngraph/vectorG1; (c)/vectorG2; (d)/vectorG3; (e)/vectorG4; (f)/vectorG5. Note that /vectorG5=/vectorG6=/vectorG7. If directed\ngraph/vectorGk−1has the edges (vi,vk)and(vk,vj), but not the edge (vi,vj), in the draw-\ning of directed graph /vectorGk, we show edges (vi,vk)and(vk,vj)with dashed lines, and\nedge(vi,vj)with a thick line. For example, in (b) existing edges (MIA,LAX) and\n(LAX,ORD) result in new edge (MIA,ORD).\nwww.it-ebooks.info"
  },
  {
    "page": 664,
    "content": "646 Chapter 14. Graph Algorithms\nThe importance of the Floyd-Warshall algorithm is that it is much easier to im-\nplement than repeated DFS, and much faster in practice because there are relatively\nfew low-level operations hidden within the asymptotic notation. The algorithm isparticularly well suited for the use of an adjacency matrix, as a single bit can beused to designate the reachability modeled as an edge (u,v)in the transitive closure.\nHowever, note that repeated calls to DFS results in better asymptotic perfor-\nmance when the graph is sparse and represented using an adjacency list or adja-cency map. In that case, a single DFS runs in O(n+m)time, and so the transitive\nclosure can be computed in O(n\n2+nm)time, which is preferable to O(n3).\nJava Implementation\nWe will conclude with a Java implementation of the Floyd-Warshall algorithm,as presented in Code Fragment 14.10. Although the pseudocode for the algorithmdescribes a series of directed graphs /vectorG\n0,/vectorG1,...,/vectorGn, we directly modify the original\ngraph, repeatedly adding new edges to the closure as we progress through rounds\nof the Floyd-Warshall algorithm.\nAlso, the pseudocode for the algorithm describes the loops based on vertices\nbeing indexed from 0 to n−1. With our graph ADT, we prefer to use Java’s for-\neach loop syntax directly on the vertices of the graph. Therefore, in Code Frag-\nment 14.10, variables i,j, and kare references to vertices, not integer indices into\nthe sequence of vertices.\nFinally, we make one additional optimization in the Java implementation, rel-\native to the pseudocode, by not bothering to iterate through values of junless we\nhave veriﬁed that edge (i,k)exists in the current version of the closure.\n1/∗∗Converts graph g into its transitive closure. ∗/\n2public static <V,E>voidtransitiveClosure(Graph <V,E>g){\n3for(Vertex<V>k : g.vertices())\n4for(Vertex<V>i : g.vertices())\n5 // verify that edge (i,k) exists in the partial closure\n6 if(i != k && g.getEdge(i,k) != null)\n7 for(Vertex<V>j : g.vertices())\n8 // verify that edge (k,j) exists in the partial closure\n9 if(i != j && j != k && g.getEdge(k,j) != null)\n10 // if (i,j) not yet included, add it to the closure\n11 if(g.getEdge(i,j) == null)\n12 g.insertEdge(i, j, null);\n13}\nCode Fragment 14.10: Java implementation of the Floyd-Warshall algorithm.\nwww.it-ebooks.info"
  },
  {
    "page": 665,
    "content": "14.5. Directed Acyclic Graphs 647\n14.5 Directed Acyclic Graphs\nDirected graphs without directed cycles are encountered in many applications.\nSuch a directed graph is often referred to as a directed acyclic graph , orDAG ,\nfor short. Applications of such graphs include the following:\n•Prerequisites between courses of an academic program.\n•Inheritance between classes of an object-oriented program.\n•Scheduling constraints between the tasks of a project.\nWe will explore this latter application further in the following example:\nExample 14.20: In order to manage a large project, it is convenient to break it up\ninto a collection of smaller tasks. The tasks, however, are rarely independent, be-\ncause scheduling constraints exist between them. (For example, in a house building\nproject, the task of ordering nails obviously precedes the task of nailing shingles\nto the roof deck.) Clearly, scheduling constraints cannot have circularities, because\nthey would make the project impossible. (For example, in order to get a job you\nneed to have work experience, but in order to get work experience you need to have\na job.) The scheduling constraints impose restrictions on the order in which the\ntasks can be executed. Namely, if a constraint says that task amust be completed\nbefore task bis started, then amust precede bin the order of execution of the tasks.\nThus, if we model a feasible set of tasks as vertices of a directed graph, and we\nplace a directed edge from utovwhenever the task for umust be executed before\nthe task for v, then we deﬁne a directed acyclic graph.\n14.5.1 Topological Ordering\nThe example above motivates the following deﬁnition. Let /vectorGbe a directed graph\nwith nvertices. A topological ordering of/vectorGis an ordering v1,...,vnof the vertices\nof/vectorGsuch that for every edge (vi,vj)of/vectorG, it is the case that i<j. That is, a topo-\nlogical ordering is an ordering such that any directed path in /vectorGtraverses vertices in\nincreasing order. Note that a directed graph may have more than one topological\nordering. (See Figure 14.12.)\nProposition 14.21: /vectorGhas a topological ordering if and only if it is acyclic.\nJustiﬁcation: The necessity (the “only if” part of the statement) is easy to\ndemonstrate. Suppose /vectorGis topologically ordered. Assume, for the sake of a con-\ntradiction, that /vectorGhas a cycle consisting of edges (vi0,vi1),(vi1,vi2),...,(vik−1,vi0).\nBecause of the topological ordering, we must have i0<i1<···<ik−1<i0, which\nis clearly impossible. Thus, /vectorGmust be acyclic.\nwww.it-ebooks.info"
  },
  {
    "page": 666,
    "content": "648 Chapter 14. Graph Algorithms\n1\n832\n7654\nFC\nD\nGB\nHEA2\n863\n7541\nFC\nD\nGB\nHEA\n(a) (b)\nFigure 14.12: Two topological orderings of the same acyclic directed graph.\nWe now argue the sufﬁciency of the condition (the “if” part). Suppose /vectorGis\nacyclic. We will give an algorithmic description of how to build a topological\nordering for /vectorG. Since/vectorGis acyclic, /vectorGmust have a vertex with no incoming edges\n(that is, with in-degree 0). Let v1be such a vertex. Indeed, if v1did not exist,\nthen in tracing a directed path from an arbitrary start vertex, we would eventually\nencounter a previously visited vertex, thus contradicting the acyclicity of /vectorG. If we\nremove v1from/vectorG, together with its outgoing edges, the resulting directed graph is\nstill acyclic. Hence, the resulting directed graph also has a vertex with no incoming\nedges, and we let v2be such a vertex. By repeating this process until the directed\ngraph becomes empty, we obtain an ordering v1,...,vnof the vertices of /vectorG. Because\nof the construction above, if (vi,vj)is an edge of /vectorG, then vimust be deleted before\nvjcan be deleted, and thus, i<j. Therefore, v1,..., vnis a topological ordering.\nProposition 14.21’s justiﬁcation suggests an algorithm for computing a topo-\nlogical ordering of a directed graph, which we call topological sorting . We present\na Java implementation of the technique in Code Fragment 14.11, and an example\nexecution of the algorithm in Figure 14.13. Our implementation uses a map, named\ninCount , to map each vertex vto a counter that represents the current number of\nincoming edges to v, excluding those coming from vertices that have previously\nbeen added to the topological order. As was the case with our graph traversals, a\nhash-based map only provides O(1)expected time access to its entries, rather than\nworst-case time. This could easily be converted to worst-case time if vertices could\nbe indexed from 0 to n−1, or if we store the count as a ﬁeld of the vertex instance.\nAs a side effect, the topological sorting algorithm of Code Fragment 14.11\nalso tests whether the given directed graph /vectorGis acyclic. Indeed, if the algorithm\nterminates without ordering all the vertices, then the subgraph of the vertices that\nhave not been ordered must contain a directed cycle.\nwww.it-ebooks.info"
  },
  {
    "page": 667,
    "content": "14.5. Directed Acyclic Graphs 649\n1/∗∗Returns a list of verticies of directed acyclic graph g in topological order. ∗/\n2public static <V,E>PositionalList <Vertex<V>>topologicalSort(Graph <V,E>g){\n3// list of vertices placed in topological order\n4PositionalList <Vertex<V>>topo =newLinkedPositionalList <>();\n5// container of vertices that have no remaining constraints\n6Stack<Vertex<V>>ready =newLinkedStack <>();\n7// map keeping track of remaining in-degree for each vertex\n8Map<Vertex<V>, Integer>inCount = newProbeHashMap <>();\n9for(Vertex<V>u : g.vertices()){\n10inCount.put(u, g.inDegree(u)); // initialize with actual in-degree\n11if(inCount.get(u) == 0) // if u has no incoming edges,\n12 ready.push(u); // it is free of constraints\n13}\n14while(!ready.isEmpty()) {\n15Vertex<V>u = ready.pop();\n16topo.addLast(u);\n17for(Edge<E>e : g.outgoingEdges(u)) {// consider all outgoing neighbors of u\n18 Vertex<V>v = g.opposite(u, e);\n19 inCount.put(v, inCount.get(v) −1);// v has one less constraint without u\n20 if(inCount.get(v) == 0)\n21 ready.push(v);\n22}\n23}\n24returntopo;\n25}\nCode Fragment 14.11: Java implementation for the topological sorting algorithm.\n(We show an example execution of this algorithm in Figure 14.13.)\nProposition 14.22: Let/vectorGbe a directed graph with nvertices and medges, using\nan adjacency list representation. The topological sorting algorithm runs in O(n+m)\ntime using O(n)auxiliary space, and either computes a topological ordering of /vectorG\nor fails to include some vertices, which indicates that /vectorGhas a directed cycle.\nJustiﬁcation: The initial recording of the nin-degrees uses O(n)time based\non theinDegree method. Say that a vertex uisvisited by the topological sorting\nalgorithm when uis removed from the ready list. A vertex ucan be visited only\nwheninCount.get( u)is 0, which implies that all its predecessors (vertices with\noutgoing edges into u) were previously visited. As a consequence, any vertex that\nis on a directed cycle will never be visited, and any other vertex will be visited\nexactly once. The algorithm traverses all the outgoing edges of each visited vertex\nonce, so its running time is proportional to the number of outgoing edges of the\nvisited vertices. In accordance with Proposition 14.9, the running time is (n+m).\nRegarding the space usage, observe that containers topo ,ready , andinCount have\nat most one entry per vertex, and therefore use O(n)space.\nwww.it-ebooks.info"
  },
  {
    "page": 668,
    "content": "650 Chapter 14. Graph Algorithms\n10\n323\n10\n2\nGC\nHD\nFB\nEA\n00\n321 21\n2 E FCA B\nD\nG\nH0\n220 221\n1 E FCA B\nD\nG\nH\n(a) (b) (c)\n0 1\n213 22\n1 E FCA B\nD\nG\nH 213 104\n21\nFC\nDA B\nG\nHE\n213 04\n2\n51\nCB\nD\nFA\nG\nHE\n(d) (e) (f)\n1034\n61\n2\n5 E F\nHGDCA B\n7\n03 62\n51 4\nC\nG\nHB A\nD\nF E\n84\n73 61\n2\n5 E F\nHGDCA B\n(g) (h) (i)\nFigure 14.13: Example of a run of algorithm topologicalSort (Code Frag-\nment 14.11). The label near a vertex shows its current inCount value, and its\neventual rank in the resulting topological order. The highlighted vertex is one with\ninCount equal to zero that will become the next vertex in the topological order.\nDashed lines denote edges that have already been examined, which are no longer\nreﬂected in the inCount values.\nwww.it-ebooks.info"
  },
  {
    "page": 669,
    "content": "14.6. Shortest Paths 651\n14.6 Shortest Paths\nAs we saw in Section 14.3.3, the breadth-ﬁrst search strategy can be used to ﬁnd a\npath with as few edges as possible from some starting vertex to every other vertex in\na connected graph. This approach makes sense in cases where each edge is as good\nas any other, but there are many situations where this approach is not appropriate.\nFor example, we might want to use a graph to represent the roads between\ncities, and we might be interested in ﬁnding the fastest way to travel cross-country.\nIn this case, it is probably not appropriate for all the edges to be equal to each other,\nfor some inter-city distances will likely be much larger than others. Likewise, we\nmight be using a graph to represent a computer network (such as the Internet), and\nwe might be interested in ﬁnding the fastest way to route a data packet between\ntwo computers. In this case, it again may not be appropriate for all the edges to\nbe equal to each other, for some connections in a computer network are typically\nmuch faster than others (for example, some edges might represent low-bandwidth\nconnections, while others might represent high-speed, ﬁber-optic connections). It\nis natural, therefore, to consider graphs whose edges are not weighted equally.\n14.6.1 Weighted Graphs\nAweighted graph is a graph that has a numeric (for example, integer) label w(e)\nassociated with each edge e, called the weight of edge e. For e= (u,v), we let\nnotation w(u,v)=w(e). We show an example of a weighted graph in Figure 14.14.\nBOS\nJFK\nMIAORD\nDFWSFO\nLAX2704\n1846867\n740\n125810908021464\n337\n23421235\n1121187\nFigure 14.14: A weighted graph whose vertices represent major U.S. airports and\nwhose edge weights represent distances in miles. This graph has a path from JFK to\nLAX of total weight 2,777 (going through ORD and DFW). This is the minimum-\nweight path in the graph from JFK to LAX.\nwww.it-ebooks.info"
  },
  {
    "page": 670,
    "content": "652 Chapter 14. Graph Algorithms\nDeﬁning Shortest Paths in a Weighted Graph\nLetGbe a weighted graph. The length (or weight) of a path is the sum of the\nweights of the edges of P. That is, if P=((v0,v1),(v1,v2),...,(vk−1,vk)), then the\nlength of P, denoted w(P), is deﬁned as\nw(P)=k−1\n∑\ni=0w(vi,vi+1).\nThedistance from a vertex uto a vertex vinG, denoted d(u,v), is the length of a\nminimum-length path (also called shortest path ) from utov, if such a path exists.\nPeople often use the convention that d(u,v) =∞if there is no path at all from\nutovinG. Even if there is a path from utovinG, however, if there is a cycle\ninGwhose total weight is negative, the distance from utovmay not be deﬁned.\nFor example, suppose vertices in Grepresent cities, and the weights of edges in\nGrepresent how much money it costs to go from one city to another. If someone\nwere willing to actually pay us to go from say JFK to ORD, then the “cost” of the\nedge (JFK,ORD) would be negative. If someone else were willing to pay us to gofrom ORD to JFK, then there would be a negative-weight cycle in Gand distances\nwould no longer be deﬁned. That is, anyone could now build a path (with cycles)inGfrom any city Ato another city Bthat ﬁrst goes to JFK and then cycles as\nmany times as he or she likes from JFK to ORD and back, before going on to B.\nThe existence of such paths would allow us to build arbitrarily low negative-costpaths (and, in this case, make a fortune in the process). But distances cannot bearbitrarily low negative numbers. Thus, any time we use edge weights to representdistances, we must be careful not to introduce any negative-weight cycles.\nSuppose we are given a weighted graph G, and we are asked to ﬁnd a shortest\npath from some vertex sto each other vertex in G, viewing the weights on the edges\nas distances. In this section, we explore efﬁcient ways of ﬁnding all such shortestpaths, if they exist. The ﬁrst algorithm we discuss is for the simple, yet common,case when all the edge weights in Gare nonnegative (that is, w(e)≥0 for each edge\neofG); hence, we know in advance that there are no negative-weight cycles in G.\nRecall that the special case of computing a shortest path when all weights are equal\nto one was solved with the BFS traversal algorithm presented in Section 14.3.3.\nThere is an interesting approach for solving this single-source problem based\non the greedy-method design pattern (Section 13.4.2). Recall that in this pattern we\nsolve the problem at hand by repeatedly selecting the best choice from among those\navailable in each iteration. This paradigm can often be used in situations where weare trying to optimize some cost function over a collection of objects. We can addobjects to our collection, one at a time, always picking the next one that optimizes\nthe function from among those yet to be chosen.\nwww.it-ebooks.info"
  },
  {
    "page": 671,
    "content": "14.6. Shortest Paths 653\n14.6.2 Dijkstra’s Algorithm\nThe main idea in applying the greedy-method pattern to the single-source shortest-\npath problem is to perform a “weighted” breadth-ﬁrst search starting at the source\nvertex s. In particular, we can use the greedy method to develop an algorithm that\niteratively grows a “cloud” of vertices out of s, with the vertices entering the cloud\nin order of their distances from s. Thus, in each iteration, the next vertex chosen\nis the vertex outside the cloud that is closest to s. The algorithm terminates when\nno more vertices are outside the cloud (or when those outside the cloud are not\nconnected to those within the cloud), at which point we have a shortest path from\nsto every vertex of Gthat is reachable from s. This approach is a simple, but\nnevertheless powerful, example of the greedy-method design pattern. Applying the\ngreedy method to the single-source, shortest-path problem, results in an algorithm\nknown as Dijkstra’s algorithm .\nEdge Relaxation\nLet us deﬁne a label D[v]for each vertex vinV, which we use to approximate the\ndistance in Gfrom stov. The meaning of these labels is that D[v]will always store\nthe length of the best path we have found so far from stov. Initially, D[s]=0 and\nD[v]=∞for each v/n⌉}ationslash=s, and we deﬁne the set C, which is our “ cloud ” of vertices,\nto initially be the empty set. At each iteration of the algorithm, we select a vertex\nunot in Cwith smallest D[u]label, and we pull uintoC. (In general, we will use\na priority queue to select among the vertices outside the cloud.) In the very ﬁrst\niteration we will, of course, pull sintoC. Once a new vertex uis pulled into C,\nwe update the label D[v]of each vertex vthat is adjacent to uand is outside of C,\nto reﬂect the fact that there may be a new and better way to get to vviau. This\nupdate operation is known as a relaxation procedure, for it takes an old estimate\nand checks if it can be improved to get closer to its true value. The speciﬁc edge\nrelaxation operation is as follows:\nEdge Relaxation :\nifD[u]+w(u,v)<D[v]then\nD[v] =D[u]+w(u,v)\nAlgorithm Description and Example\nWe give the pseudocode for Dijkstra’s algorithm in Code Fragment 14.12, and il-\nlustrate several iterations of Dijkstra’s algorithm in Figures 14.15 through 14.17.\nwww.it-ebooks.info"
  },
  {
    "page": 672,
    "content": "654 Chapter 14. Graph Algorithms\nAlgorithm ShortestPath( G,s):\nInput: A directed or undirected graph Gwith nonnegative edge weights, and a\ndistinguished vertex sofG.\nOutput: The length of a shortest path from stovfor each vertex vofG.\nInitialize D[s] =0 and D[v] =∞for each vertex v/n⌉}ationslash=s.\nLet a priority queue Qcontain all the vertices of Gusing the Dlabels as keys.\nwhile Qis not empty do\n{pull a new vertex uinto the cloud}\nu=value returned by Q.removeMin ()\nforeach edge (u,v)such that vis in Qdo\n{perform the relaxation procedure on edge (u,v)}\nifD[u]+w(u,v)<D[v]then\nD[v] =D[u]+w(u,v)\nChange the key of vertex vinQtoD[v].\nreturn the label D[v]of each vertex v\nCode Fragment 14.12: Pseudocode for Dijkstra’s algorithm, solving the single-\nsource shortest-path problem for an undirected or directed graph.\n3371846187849\n1258\n1090867\n144\n9466212704\n184\n23421235740\n1391\n1121PVD\n1464802\nBWI\nDFW\nLAXORD\nMIASFOBOS\nJFK∞∞\n∞∞\n0∞\n∞\n∞∞1464621740\n1391\n11219461842704\n23421235802\n3371846187849\nPVD\n1258\n1090867\n144\nDFWJFK\nMIAORD\nBWI\nLAXBOS\nSFO∞\n946621\n184\n0\n∞∞\n∞\n∞\n(a) (b)\nFigure 14.15: An example execution of Dijkstra’s shortest-path algorithm on a\nweighted graph. The start vertex is BWI. A box next to each vertex vstores the\nlabel D[v]. The edges of the shortest-path tree are drawn as thick arrows, and for\neach vertex uoutside the “cloud” we show the current best edge for pulling in u\nwith a thick line. (Continues in Figure 14.16.)\nwww.it-ebooks.info"
  },
  {
    "page": 673,
    "content": "14.6. Shortest Paths 655\n3371846187\n9461842704\n2342PVD\n1235802849\n1258\n1090867\n144\n621740\n1391\n11211464\nDFW\nLAXORD\nMIASFOBOS\nJFK\nBWI\n9461575184621\n∞\n0371\n328\n∞802\n3371846144\n9461842704\n23421235187849\n1258\n1090867\n621740\n1391\n11211464PVD\nDFW\nLAXORD\nMIASFOBOS\nJFK\nBWI\n9461575184621\n∞\n0371\n328\n∞\n(c) (d)\n144\n9461842704\n23421235802\n3371846187849\n1258\n1090867\n621740\n1391\n1121PVD\n1464\nDFW\nLAXORD\nMIASFOBOS\nJFK\nBWI\n∞328\n3075\n946371\n1575184\n0621\n144PVD\n1464\n11211391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946SFO\nLAXORD\nMIABOS\nJFK\nBWI\nDFW0621328\n1423371\n2467\n∞184\n946\n(e) (f)\n23421235802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n11211464144\n9462704\n184\nDFW\nLAXORDBOS\nJFK\nBWISFO\nMIA\n946371\n2467621\n1423\n32880328\n184\n802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n1121144\n94614641842704\n1235\n2342\nLAXORDBOS\nJFK\nBWISFO\nMIADFW\n946371\n1423621\n2467\n26580328\n184\n(g) (h)\nFigure 14.16: An example execution of Dijkstra’s shortest-path algorithm on a\nweighted graph. (Continued from Figure 14.15; continues in Figure 14.17.)\nwww.it-ebooks.info"
  },
  {
    "page": 674,
    "content": "656 Chapter 14. Graph Algorithms\n23421235802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n1121144\n94614641842704\nLAXORDBOS\nJFK\nBWI\nMIADFWSFO\n946371\n2658621\n14232467\n0328\n184\n802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n1121144\n94614641842704\n23421235ORDBOS\nJFK\nBWI\nMIADFWSFO\nLAX\n946371\n2467621\n1423\n26580328\n184\n(i) (j)\nFigure 14.17: An example execution of Dijkstra’s shortest-path algorithm on a\nweighted graph. (Continued from Figure 14.16.)\nWhy It Works\nThe interesting aspect of the Dijkstra algorithm is that, at the moment a vertex u\nis pulled into C, its label D[u]stores the correct length of a shortest path from v\ntou. Thus, when the algorithm terminates, it will have computed the shortest-path\ndistance from sto every vertex of G. That is, it will have solved the single-source\nshortest-path problem.\nIt is probably not immediately clear why Dijkstra’s algorithm correctly ﬁnds the\nshortest path from the start vertex sto each other vertex uin the graph. Why is it\nthat the distance from stouis equal to the value of the label D[u]at the time vertex\nuis removed from the priority queue Qand added to the cloud C? The answer\nto this question depends on there being no negative-weight edges in the graph, for\nit allows the greedy method to work correctly, as we show in the proposition thatfollows.\nProposition 14.23:\nIn Dijkstra’s algorithm, whenever a vertex vis pulled into the\ncloud, the label D[v]is equal to d(s,v), the length of a shortest path from stov.\nJustiﬁcation: Suppose that D[v]>d(s,v)for some vertex vinV, and let z\nbe the ﬁrst vertex the algorithm pulled into the cloud C(that is, removed from\nQ) such that D[z]>d(s,z). There is a shortest path Pfrom stoz(for otherwise\nd(s,z) =∞=D[z]). Let us therefore consider the moment when zis pulled into\nC, and let ybe the ﬁrst vertex of P(when going from stoz) that is not in Cat this\nmoment. Let xbe the predecessor of yin path P(note that we could have x=s).\n(See Figure 14.18.) We know, by our choice of y, that xis already in Cat this point.\nwww.it-ebooks.info"
  },
  {
    "page": 675,
    "content": "14.6. Shortest Paths 657\nthe ﬁrst “wrong” vertex picked\nzpicked implies\nthatD[z]≤D[y]\nPD[z]>d(s,z)C\nD[y]=d(s,y)yz\ns\nxD[x]=d(s,x)\nFigure 14.18: A schematic illustration for the justiﬁcation of Proposition 14.23.\nMoreover, D[x] =d(s,x), since zis the ﬁrst incorrect vertex. When xwas pulled\nintoC, we tested (and possibly updated) D[y]so that we had at that point\nD[y]≤D[x]+w(x,y)=d(s,x)+w(x,y).\nBut since yis the next vertex on the shortest path from stoz, this implies that\nD[y]=d(s,y).\nBut we are now at the moment when we are picking z, not y, to join C; hence,\nD[z]≤D[y].\nIt should be clear that a subpath of a shortest path is itself a shortest path. Hence,\nsince yis on the shortest path from stoz,\nd(s,y)+d(y,z) =d(s,z).\nMoreover, d(y,z)≥0 because there are no negative-weight edges. Therefore,\nD[z]≤D[y]=d(s,y)≤d(s,y)+d(y,z)=d(s,z).\nBut this contradicts the deﬁnition of z; hence, there can be no such vertex z.\nThe Running Time of Dijkstra’s Algorithm\nIn this section, we analyze the time complexity of Dijkstra’s algorithm. We denote\nwith nandmthe number of vertices and edges of the input graph G, respectively.\nWe assume that the edge weights can be added and compared in constant time.\nBecause of the high level of the description we gave for Dijkstra’s algorithm in\nCode Fragment 14.12, analyzing its running time requires that we give more details\non its implementation. Speciﬁcally, we should indicate the data structures used and\nhow they are implemented.\nwww.it-ebooks.info"
  },
  {
    "page": 676,
    "content": "658 Chapter 14. Graph Algorithms\nLet us ﬁrst assume that we are representing the graph Gusing an adjacency\nlist or adjacency map structure. This data structure allows us to step through the\nvertices adjacent to uduring the relaxation step in time proportional to their number.\nTherefore, the time spent in the management of the nested forloop, and the number\nof iterations of that loop, is\n∑\nuinVGoutdeg(u),\nwhich is O(m)by Proposition 14.9. The outer while loop executes O(n)times,\nsince a new vertex is added to the cloud during each iteration. This still does notsettle all the details for the algorithm analysis, however, for we must say more about\nhow to implement the other principal data structure in the algorithm—the priority\nqueue Q.\nReferring back to Code Fragment 14.12 in search of priority queue operations,\nwe ﬁnd that nvertices are originally inserted into the priority queue; since these are\nthe only insertions, the maximum size of the queue is n. In each of niterations of\nthewhile loop, a call to removeMin is made to extract the vertex uwith smallest\nDlabel from Q. Then, for each neighbor vofu, we perform an edge relaxation,\nand may potentially update the key of vin the queue. Thus, we actually need an\nimplementation of an adaptable priority queue (Section 9.5), in which case the\nkey of a vertex vis changed using the method replaceKey( e,k), where eis the\npriority queue entry associated with vertex v. In the worst case, there could be one\nsuch update for each edge of the graph. Overall, the running time of Dijkstra’salgorithm is bounded by the sum of the following:\n•ninsertions into Q.\n•ncalls to the removeMin method on Q.\n•mcalls to the replaceKey method on Q.\nIfQis an adaptable priority queue implemented as a heap, then each of the\nabove operations run in O(logn), and so the overall running time for Dijkstra’s\nalgorithm is O((n+m)logn). Note that if we wish to express the running time as a\nfunction of nonly, then it is O(n\n2logn)in the worst case.\nLet us now consider an alternative implementation for the adaptable priority\nqueue Qusing an unsorted sequence. (See Exercise P-9.52.) This, of course, re-\nquires that we spend O(n)time to extract the minimum element, but it affords\nvery fast key updates, provided Qsupports location-aware entries (Section 9.5.1).\nSpeciﬁcally, we can implement each key update done in a relaxation step in O(1)\ntime—we simply change the key value once we locate the entry in Qto update.\nHence, this implementation results in a running time that is O(n2+m), which can\nbe simpliﬁed to O(n2)since Gis simple.\nwww.it-ebooks.info"
  },
  {
    "page": 677,
    "content": "14.6. Shortest Paths 659\nComparing the Two Implementations\nWe have two choices for implementing the adaptable priority queue with location-\naware entries in Dijkstra’s algorithm: a heap implementation, which yields a run-\nning time of O((n+m)logn), and an unsorted sequence implementation, which\nyields a running time of O(n2). Since both implementations would be fairly simple\nto code, they are about equal in terms of the programming sophistication needed.\nThese two implementations are also about equal in terms of the constant factors in\ntheir worst-case running times. Looking only at these worst-case times, we prefer\nthe heap implementation when the number of edges in the graph is small (that is,\nwhen m<n2/logn), and we prefer the sequence implementation when the number\nof edges is large (that is, when m>n2/logn).\nProposition 14.24: Given a weighted graph Gwith nvertices and medges, such\nthat the weight of each edge is nonnegative, and a vertex sofG, Dijkstra’s algorithm\ncan compute the distance from sto all other vertices of Gin the better of O(n2)or\nO((n+m)logn)time.\nWe note that an advanced priority queue implementation, known as a Fibonacci\nheap , can be used to implement Dijkstra’s algorithm in O(m+nlogn)time.\nProgramming Dijkstra’s Algorithm in Java\nHaving given a pseudocode description of Dijkstra’s algorithm, let us now present\nJava code for performing Dijkstra’s algorithm, assuming we are given a graph\nwhose edge elements are nonnegative integer weights. Our implementation of the\nalgorithm is in the form of a method, shortestPathLengths , that takes a graph and\na designated source vertex as parameters. (See Code Fragment 14.13.) It returns\na map, named cloud , storing the shortest-path distance d(s,v)for each vertex v\nthat is reachable from the source. We rely on our HeapAdaptablePriorityQueue\ndeveloped in Section 9.5.2 as an adaptable priority queue.\nAs we have done with other algorithms in this chapter, we rely on hash-based\nmaps to store auxiliary data (in this case, mapping vto its distance bound D[v]and\nits adaptable priority queue entry). The expected O(1)-time access to elements of\nthese dictionaries could be converted to worst-case bounds, either by numbering\nvertices from 0 to n−1 to use as indices into an array, or by storing the information\nwithin each vertex’s element.\nThe pseudocode for Dijkstra’s algorithm begins by assigning D[v]=∞for each\nvother than the source; we rely on the special value Integer.MAX VALUE in Java to\nprovide a sufﬁcient numeric value to model inﬁnity. However, we avoid including\nvertices with this “inﬁnite” distance in the resulting cloud that is returned by the\nmethod. The use of this numeric limit could be avoided altogether by waiting to\nadd a vertex to the priority queue until after an edge that reaches it is relaxed. (See\nExercise C-14.62.)\nwww.it-ebooks.info"
  },
  {
    "page": 678,
    "content": "660 Chapter 14. Graph Algorithms\n1/∗∗Computes shortest-path distances from src vertex to all reachable vertices of g. ∗/\n2public static <V>Map<Vertex<V>, Integer>\n3shortestPathLengths(Graph <V,Integer >g, Vertex <V>src){\n4// d.get(v) is upper bound on distance from src to v\n5Map<Vertex<V>, Integer>d =newProbeHashMap <>();\n6// map reachable v to its d value\n7Map<Vertex<V>, Integer>cloud =newProbeHashMap <>();\n8// pq will have vertices as elements, with d.get(v) as key\n9AdaptablePriorityQueue <Integer, Vertex <V>>pq;\n10pq =newHeapAdaptablePriorityQueue <>();\n11// maps from vertex to its pq locator\n12Map<Vertex<V>, Entry<Integer,Vertex <V>>>pqTokens;\n13pqTokens = newProbeHashMap <>();\n14\n15// for each vertex v of the graph, add an entry to the priority queue, with\n16// the source having distance 0 and all others having inﬁnite distance\n17for(Vertex<V>v : g.vertices()){\n18if(v == src)\n19 d.put(v,0);\n20else\n21 d.put(v, Integer.MAX VALUE);\n22pqTokens.put(v, pq.insert(d.get(v), v)); // save entry for future updates\n23}\n24// now begin adding reachable vertices to the cloud\n25while(!pq.isEmpty()){\n26Entry<Integer, Vertex <V>>entry = pq.removeMin();\n27intkey = entry.getKey();\n28Vertex<V>u = entry.getValue();\n29cloud.put(u, key); // this is actual distance to u\n30pqTokens.remove(u); // u is no longer in pq\n31for(Edge<Integer>e : g.outgoingEdges(u)) {\n32 Vertex<V>v = g.opposite(u,e);\n33 if(cloud.get(v) == null){\n34 // perform relaxation step on edge (u,v)\n35 intwgt = e.getElement();\n36 if(d.get(u) + wgt <d.get(v)){ // better path to v?\n37 d.put(v, d.get(u) + wgt); // update the distance\n38 pq.replaceKey(pqTokens.get(v), d.get(v)); // update the pq entry\n39}\n40}\n41}\n42}\n43returncloud; // this only includes reachable vertices\n44}\nCode Fragment 14.13: Java implementation of Dijkstra’s algorithm for computing\nthe shortest-path distances from a single source. We assume that e.getElement()\nfor edge erepresents the weight of that edge.\nwww.it-ebooks.info"
  },
  {
    "page": 679,
    "content": "14.6. Shortest Paths 661\nReconstructing a Shortest-Path Tree\nOur pseudocode description of Dijkstra’s algorithm in Code Fragment 14.12 and\nour implementation in Code Fragment 14.13 compute the value D[v], for each ver-\ntexv, that is the length of a shortest path from the source vertex stov. However,\nthose forms of the algorithm do not explicitly compute the actual paths that achievethose distances. Fortunately, it is possible to represent shortest paths from sourcesto every reachable vertex in a graph using a compact data structure known as a\nshortest-path tree . This is possible because if a shortest path from stovpasses\nthrough an intermediate vertex u, it must begin with a shortest path from stou.\nWe next demonstrate that a shortest-path tree rooted at source scan be recon-\nstructed in O(n+m)time, given the D[v]values produced by Dijkstra’s algorithm\nusing sas the source. As we did when representing the DFS and BFS trees, we\nwill map each vertex v/n⌉}ationslash=sto a parent u(possibly, u=s), such that uis the vertex\nimmediately before von a shortest path from stov. Ifuis the vertex just before v\non a shortest path from stov, it must be that\nD[u]+w(u,v)=D[v].\nConversely, if the above equation is satisﬁed, then a shortest path from stoufol-\nlowed by the edge (u,v)is a shortest path to v.\nOur implementation in Code Fragment 14.14 reconstructs a tree based on this\nlogic, testing all incoming edges to each vertex v, looking for a (u,v)that satisﬁes\nthe key equation. The running time is O(n+m), as we consider each vertex and all\nincoming edges to those vertices. (See Proposition 14.9.)\n1/∗∗\n2 ∗Reconstructs a shortest-path tree rooted at vertex s, given distance map d.\n3 ∗The tree is represented as a map from each reachable vertex v (other than s)\n4 ∗to the edge e = (u,v) that is used to reach v from its parent u in the tree.\n5 ∗/\n6public static <V>Map<Vertex<V>,Edge<Integer>>\n7spTree(Graph <V,Integer >g, Vertex <V>s, Map<Vertex<V>,Integer>d){\n8Map<Vertex<V>, Edge<Integer>>tree =newProbeHashMap <>();\n9for(Vertex<V>v : d.keySet())\n10 if(v != s)\n11 for(Edge<Integer>e : g.incomingEdges(v)) {// consider INCOMING edges\n12 Vertex<V>u = g.opposite(v, e);\n13 intwgt = e.getElement();\n14 if(d.get(v) == d.get(u) + wgt)\n15 tree.put(v, e); // edge is is used to reach v\n16}\n17returntree;\n18}\nCode Fragment 14.14: Java method that reconstructs a single-source shortest-path\ntree, based on knowledge of the shortest-path distances.\nwww.it-ebooks.info"
  },
  {
    "page": 680,
    "content": "662 Chapter 14. Graph Algorithms\n14.7 Minimum Spanning Trees\nSuppose we wish to connect all the computers in a new ofﬁce building using the\nleast amount of cable. We can model this problem using an undirected, weighted\ngraph Gwhose vertices represent the computers, and whose edges represent all\nthe possible pairs (u,v)of computers, where the weight w(u,v)of edge(u,v)is\nequal to the amount of cable needed to connect computer uto computer v. Rather\nthan computing a shortest-path tree from some particular vertex v, we are interested\ninstead in ﬁnding a tree Tthat contains all the vertices of Gand has the minimum\ntotal weight over all such trees. Algorithms for ﬁnding such a tree are the focus of\nthis section.\nProblem Deﬁnition\nGiven an undirected, weighted graph G, we are interested in ﬁnding a tree Tthat\ncontains all the vertices in Gand minimizes the sum\nw(T)= ∑\n(u,v)inTw(u,v).\nA tree, such as this, that contains every vertex of a connected graph Gis said to\nbe a spanning tree , and the problem of computing a spanning tree Twith smallest\ntotal weight is known as the minimum spanning tree (orMST ) problem.\nThe development of efﬁcient algorithms for the minimum spanning tree prob-\nlem predates the modern notion of computer science itself. In this section, we\ndiscuss two classic algorithms for solving the MST problem. These algorithms\nare both applications of the greedy method , which, as was discussed brieﬂy in the\nprevious section, is based on choosing objects to join a growing collection by it-\neratively picking an object that minimizes some cost function. The ﬁrst algorithm\nwe discuss is the Prim-Jarn ´ ık algorithm, which grows the MST from a single root\nvertex, much in the same way as Dijkstra’s shortest-path algorithm. The second\nalgorithm we discuss is Kruskal’s algorithm, which “grows” the MST in clusters\nby considering edges in nondecreasing order of their weights.\nIn order to simplify the description of the algorithms, we assume, in the follow-\ning, that the input graph Gis undirected (that is, all its edges are undirected) and\nsimple (that is, it has no self-loops and no parallel edges). Hence, we denote the\nedges of Gas unordered vertex pairs (u,v).\nBefore we discuss the details of these algorithms, however, let us give a crucial\nfact about minimum spanning trees that forms the basis of the algorithms.\nwww.it-ebooks.info"
  },
  {
    "page": 681,
    "content": "14.7. Minimum Spanning Trees 663\nA Crucial Fact about Minimum Spanning Trees\nThe two MST algorithms we discuss are based on the greedy method, which in this\ncase depends crucially on the following fact. (See Figure 14.19.)\nV1 V2e\nmin-weight\n“bridge” edgeeBelongs to a Minimum Spanning Tree\nFigure 14.19: An illustration of the crucial fact about minimum spanning trees.\nProposition 14.25: LetGbe a weighted connected graph, and let V1andV2be a\npartition of the vertices of Ginto two disjoint nonempty sets. Furthermore, let ebe\nan edge in Gwith minimum weight from among those with one endpoint in V1and\nthe other in V2. There is a minimum spanning tree Tthat has eas one of its edges.\nJustiﬁcation: LetTbe a minimum spanning tree of G. IfTdoes not contain\nedge e, the addition of etoTmust create a cycle. Therefore, there is some edge\nf/n⌉}ationslash=eof this cycle that has one endpoint in V1and the other in V2. Moreover, by\nthe choice of e,w(e)≤w(f). If we remove ffrom T∪{e}, we obtain a spanning\ntree whose total weight is no more than before. Since Twas a minimum spanning\ntree, this new tree must also be a minimum spanning tree.\nIn fact, if the weights in Gare distinct, then the minimum spanning tree is\nunique; we leave the justiﬁcation of this less crucial fact as an exercise (C-14.64).\nIn addition, note that Proposition 14.25 remains valid even if the graph Gcon-\ntains negative-weight edges or negative-weight cycles, unlike the algorithms we\npresented for shortest paths.\nwww.it-ebooks.info"
  },
  {
    "page": 682,
    "content": "664 Chapter 14. Graph Algorithms\n14.7.1 Prim-Jarn ´ ık Algorithm\nIn the Prim-Jarn ´ ık algorithm, we grow a minimum spanning tree from a single\ncluster starting from some “root” vertex s. The main idea is similar to that of\nDijkstra’s algorithm. We will begin with some vertex s, deﬁning the initial “cloud”\nof vertices C. Then, in each iteration, we choose a minimum-weight edge e=(u,v),\nconnecting a vertex uin the cloud Cto a vertex voutside of C. The vertex vis\nthen brought into the cloud Cand the process is repeated until a spanning tree is\nformed. Again, the crucial fact about minimum spanning trees comes into play,\nfor by always choosing the smallest-weight edge joining a vertex inside Cto one\noutside C, we are assured of always adding a valid edge to the MST.\nTo efﬁciently implement this approach, we can take another cue from Dijkstra’s\nalgorithm. We maintain a label D[v]for each vertex voutside the cloud C, so that\nD[v]stores the weight of the minimum observed edge for joining vto the cloud\nC. (In Dijkstra’s algorithm, this label measured the full path length from starting\nvertex stov, including an edge (u,v).) These labels serve as keys in a priority\nqueue used to decide which vertex is next in line to join the cloud. We give the\npseudocode in Code Fragment 14.15.\nAlgorithm PrimJarnik( G):\nInput: An undirected, weighted, connected graph Gwith nvertices and medges\nOutput: A minimum spanning tree TforG\nPick any vertex sofG\nD[s] =0\nforeach vertex v/n⌉}ationslash=sdo\nD[v] =∞\nInitialize T=∅.\nInitialize a priority queue Qwith an entry (D[v],v)for each vertex v.\nFor each vertex v, maintain connect(v)as the edge achieving D[v](if any).\nwhile Qis not empty do\nLetube the value of the entry returned by Q.removeMin ().\nConnect vertex utoTusing edge connect(e).\nforeach edge e′=(u,v)such that vis in Qdo\n{check if edge (u,v)better connects vtoT}\nifw(u,v)<D[v]then\nD[v] =w(u,v)\nconnect(v) =e′.\nChange the key of vertex vinQtoD[v].\nreturn the tree T\nCode Fragment 14.15: The Prim-Jarn ´ ık algorithm for the MST problem.\nwww.it-ebooks.info"
  },
  {
    "page": 683,
    "content": "14.7. Minimum Spanning Trees 665\nAnalyzing the Prim-Jarn ´ ık Algorithm\nThe implementation issues for the Prim-Jarn ´ ık algorithm are similar to those for\nDijkstra’s algorithm, relying on an adaptable priority queue Q(Section 9.5.1).\nWe initially perform ninsertions into Q, later perform nextract-min operations,\nand may update a total of mpriorities as part of the algorithm. Those steps are\nthe primary contributions to the overall running time. With a heap-based priority\nqueue, each operation runs in O(logn)time, and the overall time for the algorithm\nisO((n+m)logn), which is O(mlogn)for a connected graph. Alternatively, we\ncan achieve O(n2)running time by using an unsorted list as a priority queue.\nIllustrating the Prim-Jarn ´ ık Algorithm\nWe illustrate the Prim-Jarn ´ ık algorithm in Figures 14.20 and 14.21.\n187\n1391740\n621867\n1090849\n1846\n802\n12352704\n184144\n337\n23421258\n946PVD\n1464\n1121JFK\nBWIORD\nMIALAXDFWSFOBOS\n144\n1391740\n621867\n1090849\n1846\n802\n12352704\n184\n337\n23421258\n946187PVD\n1464\n1121SFOBOS\nJFK\nBWIORD\nMIALAXDFW\n(a) (b)\n1090621867\n849\n187\n1846\n802\n12352704\n184144\n337\n23421258\n946740\n1391PVD\n1464\n1121SFOBOS\nJFK\nBWIORD\nMIALAXDFW187740867\n1090849\n1846\n802\n12352704\n184144\n337\n23421258\n1391621\n946PVD\n1464\n1121SFOBOS\nJFK\nBWIORD\nMIALAXDFW\n(c) (d)\nFigure 14.20: An illustration of the Prim-Jarn ´ ık MST algorithm, starting with vertex\nPVD. (Continues in Figure 14.21.)\nwww.it-ebooks.info"
  },
  {
    "page": 684,
    "content": "666 Chapter 14. Graph Algorithms\n9461391740867\n10901258849\n187\n1846\n802\n12352704\n184144\n337\n2342621PVD\n1464\n1121DFWBOS\nJFK\nBWIORD\nMIASFO\nLAX1464621867\n10901258849\n187\n1846\n802\n12352704\n184\n946144\n337\n23421121PVD\n1391740\nDFWBOS\nJFK\nBWIORD\nMIASFO\nLAX\n(e) (f)\n12351391740\n621867\n10901258849\n187\n1846\n8022704\n184\n946144\n337\n2342PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO\n12351391740\n621867\n10901258849\n187\n1846\n802\n23422704\n184\n946144\n337PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO\n(g) (h)\n3371391740\n621867\n10901258849\n187\n1846\n802\n1235\n23422704\n184\n946144PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO144\n1391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO\n(i) (j)\nFigure 14.21: An illustration of the Prim-Jarn ´ ık MST algorithm. (Continued from\nFigure 14.20.)\nwww.it-ebooks.info"
  },
  {
    "page": 685,
    "content": "14.7. Minimum Spanning Trees 667\n14.7.2 Kruskal’s Algorithm\nIn this section, we will introduce Kruskal’s algorithm for constructing a minimum\nspanning tree. While the Prim-Jarn ´ ık algorithm builds the MST by growing a single\ntree until it spans the graph, Kruskal’s algorithm maintains many smaller trees in a\nforest , repeatedly merging pairs of trees until a single tree spans the graph.\nInitially, each vertex is in its own cluster. The algorithm then considers each\nedge in turn, ordered by increasing weight. If an edge econnects vertices in two\ndifferent clusters, then eis added to the set of edges of the minimum spanning\ntree, and the two trees are merged with the addition of e. If, on the other hand, e\nconnects two vertices in the same cluster, then eis discarded. Once the algorithm\nhas added enough edges to form a spanning tree, it terminates and outputs this tree\nas the minimum spanning tree.\nWe give pseudocode for Kruskal’s MST algorithm in Code Fragment 14.16 and\nwe show an example of this algorithm in Figures 14.22, 14.23, and 14.24.\nAlgorithm Kruskal( G):\nInput: A simple connected weighted graph Gwith nvertices and medges\nOutput: A minimum spanning tree TforG\nforeach vertex vinGdo\nDeﬁne an elementary cluster C(v) ={v}.\nInitialize a priority queue Qto contain all edges in G, using the weights as keys.\nT=∅ {Twill ultimately contain the edges of an MST }\nwhile Thas fewer than n−1 edges do\n(u,v) =value returned by Q.removeMin ()\nLetC(u)be the cluster containing u, and let C(v)be the cluster containing v.\nifC(u)/n⌉}ationslash=C(v)then\nAdd edge (u,v)toT.\nMerge C(u)andC(v)into one cluster.\nreturn treeT\nCode Fragment 14.16: Kruskal’s algorithm for the MST problem.\nAs was the case with the Prim-Jarn ´ ık algorithm, the correctness of Kruskal’s al-\ngorithm is based upon the crucial fact about minimum spanning trees from Propo-\nsition 14.25. Each time Kruskal’s algorithm adds an edge (u,v)to the minimum\nspanning tree T, we can deﬁne a partitioning of the set of vertices V(as in the\nproposition) by letting V1be the cluster containing vand letting V2contain the rest\nof the vertices in V. This clearly deﬁnes a disjoint partitioning of the vertices of\nVand, more importantly, since we are extracting edges from Qin order by their\nweights, emust be a minimum-weight edge with one vertex in V1and the other in\nV2. Thus, Kruskal’s algorithm always adds a valid minimum spanning tree edge.\nwww.it-ebooks.info"
  },
  {
    "page": 686,
    "content": "668 Chapter 14. Graph Algorithms\n144\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n946621\n184PVD\n1464\n1121BWI\nLAXBOS\nJFK\nMIAORD\nSFO\nDFW184PVD\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n144\n946621\n1464\n1121LAXJFKBOS\nSFO\nMIAORD\nBWI\nDFW\n(a) (b)\n187PVD\n1391740867\n10901258849\n1846\n337802\n1235\n23422704\n184144\n946621\n1464\n1121DFW\nLAXJFKBOS\nMIAORD\nBWISFO621PVD\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144\n9461464\n1121\nMIADFWSFO\nLAXBOS\nJFK\nBWIORD\n(c) (d)\n621\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144\n946PVD\n1464\n1121ORD\nMIADFWSFO\nLAXJFKBOS\nBWI\n9461391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144PVD\n1464\n1121BOS\nBWI\nMIADFWSFO\nLAXJFKORD\n(e) (f)\nFigure 14.22: Example of an execution of Kruskal’s MST algorithm on a graph with\ninteger weights. We show the clusters as shaded regions and we highlight the edge\nbeing considered in each iteration. (Continues in Figure 14.23.)\nwww.it-ebooks.info"
  }
]